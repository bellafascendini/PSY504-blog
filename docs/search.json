[
  {
    "objectID": "assignments/index.html",
    "href": "assignments/index.html",
    "title": "ordinal regression",
    "section": "",
    "text": "If you are fitting a model, display the model output in a neatly formatted table. (The tidy and kable functions can help!)\nIf you are creating a plot, use clear labels for all axes, titles, etc.\nIf you are using Github, don’t forget to commit and push your work to to it regularly, at least after each exercise. Write short and informative commit messages. Else, if you are submitting on Canvas, make sure that the version you submit is the latest, and that it runs/knits without any errors.\nWhen you’re done, we should be able to knit the final version of the QMD in your GitHub as a HTML."
  },
  {
    "objectID": "assignments/index.html#instructions",
    "href": "assignments/index.html#instructions",
    "title": "ordinal regression",
    "section": "",
    "text": "If you are fitting a model, display the model output in a neatly formatted table. (The tidy and kable functions can help!)\nIf you are creating a plot, use clear labels for all axes, titles, etc.\nIf you are using Github, don’t forget to commit and push your work to to it regularly, at least after each exercise. Write short and informative commit messages. Else, if you are submitting on Canvas, make sure that the version you submit is the latest, and that it runs/knits without any errors.\nWhen you’re done, we should be able to knit the final version of the QMD in your GitHub as a HTML."
  },
  {
    "objectID": "assignments/index.html#load-packages",
    "href": "assignments/index.html#load-packages",
    "title": "ordinal regression",
    "section": "Load packages:",
    "text": "Load packages:\n\n\nCode\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(performance)\nlibrary(ordinal) #clm\nlibrary(car) # anova\nlibrary(ggeffects) #  viz\nlibrary(gofcat) # brant\nlibrary(brms)\nlibrary(emmeans) # contrasts\nlibrary(knitr)"
  },
  {
    "objectID": "assignments/index.html#load-data",
    "href": "assignments/index.html#load-data",
    "title": "ordinal regression",
    "section": "Load data",
    "text": "Load data\n\nMake sure only the top 3 ranks are being used. For some reason, there are missing ranks (my guess is they did not announce rank on TV)\n\n\n\nCode\ngbbo &lt;- read_csv(\"https://raw.githubusercontent.com/suyoghc/PSY-504_Spring-2025/refs/heads/main/Ordinal%20Regression/data/GBBO.csv\")\n\n# Enter code to filter. Think about the data type that would be relevant for Rank\ngb &lt;- gbbo %&gt;% \n  filter(!is.na(`Technical Rank`) & `Technical Rank` %in% c(1, 2, 3))"
  },
  {
    "objectID": "assignments/index.html#explore",
    "href": "assignments/index.html#explore",
    "title": "ordinal regression",
    "section": "Explore",
    "text": "Explore\n\nPlot two figures showing the percentage of bakers in each rank— create one for Gender and Age\n\n\nCode\n#plot percentage of bakers in each rank by gender\ngender_rank &lt;- gb %&gt;%\n  group_by(Gender, `Technical Rank`) %&gt;%\n  summarise(n = n(), .groups = 'drop') %&gt;%\n  mutate(perc = n / sum(n) * 100)\n\nggplot(gender_rank, aes(x = factor(`Technical Rank`), y = perc, fill = Gender)) +\n  geom_bar(stat = \"identity\", position = position_dodge(width = 0.8)) +\n  labs(title = \"Percentage of Bakers in Each Technical Rank by Gender\",\n   x = \"Technical Rank\",\n   y = \"Percentage\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n #plot percentage of bakers in each rank by age group\ngb &lt;- gb %&gt;% \n  mutate(AgeGroup = cut(Age, \n                        breaks = seq(floor(min(Age, na.rm = TRUE)), ceiling(max(Age, na.rm = TRUE)), by = 10),\n                        include.lowest = TRUE, right = FALSE))\n\nage_rank &lt;- gb %&gt;%\n  group_by(AgeGroup, `Technical Rank`) %&gt;%\n  summarise(n = n(), .groups = 'drop') %&gt;%\n  mutate(perc = n / sum(n) * 100)\n\nggplot(age_rank, aes(x = AgeGroup, y = perc, fill = factor(`Technical Rank`))) +\n  geom_bar(stat = \"identity\", position = position_dodge(width = 0.8)) +\n  labs(title = \"Percentage of Bakers in Each Technical Rank by Age Group\",\n       x = \"Age Group\",\n       y = \"Percentage\",\n       fill = \"Technical Rank\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\nCode\n#here's a plot without grouping age into bins, which is less informative visually so I prefer the above plot\nage_rank_cont &lt;- gb %&gt;%\n  group_by(Age, `Technical Rank`) %&gt;%\n  summarise(n = n(), .groups = 'drop') %&gt;%\n  \n  group_by(Age) %&gt;%\n  mutate(perc = n / sum(n) * 100) %&gt;%\n  ungroup()\n\nggplot(age_rank_cont, aes(x = factor(Age), y = perc, fill = factor(`Technical Rank`))) +\n  geom_bar(stat = \"identity\", position = position_dodge(width = 0.8)) +\n  labs(title = \"Percentage of Bakers in Each Technical Rank by Age\",\n       x = \"Age\",\n       y = \"Percentage\",\n       fill = \"Technical Rank\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "assignments/index.html#ordinal-analysis",
    "href": "assignments/index.html#ordinal-analysis",
    "title": "ordinal regression",
    "section": "Ordinal Analysis",
    "text": "Ordinal Analysis\n\nIf you haven’t already, convert the outcome variable to an ordered factor. What does the order here represent?\n\n\nCode\n#The order now represents the ranking from best (1) to worst (3) among the top three.\ngb &lt;- gb %&gt;%\n  mutate(`Technical Rank` = factor(`Technical Rank`, levels = c(1, 2, 3), ordered = TRUE))\n\n\nConvert input variables to categorical factors as appropriate.\n\n\nCode\ngb &lt;- gb %&gt;%\n  mutate(Gender = factor(Gender, levels = c(\"M\", \"F\")))\n\n\nRun a ordinal logistic regression model against all relevant input variables. Interpret the effects for Gender, Age and Gender*Age (even if they are non-significant).\n\n\nCode\nmodel_int &lt;- clm(`Technical Rank` ~ Gender * Age, data = gb)\n\ntidy(model_int) %&gt;% kable(digits = 3)\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\ncoef.type\n\n\n\n\n1|2\n-0.267\n0.499\n-0.535\n0.593\nintercept\n\n\n2|3\n1.154\n0.504\n2.291\n0.022\nintercept\n\n\nGenderF\n1.149\n0.673\n1.708\n0.088\nlocation\n\n\nAge\n0.016\n0.014\n1.147\n0.252\nlocation\n\n\nGenderF:Age\n-0.039\n0.019\n-2.093\n0.036\nlocation\n\n\n\n\n\n\ninterpretation:\ngender: The coefficient for GenderF is 1.149 (p = 0.088). This means that, holding Age constant, females have a 1.149 unit higher latent score compared to males. Although this effect is only marginally significant (p = 0.088), it suggests that being female may be associated with a shift toward a higher rank category.\nage: The coefficient for Age is -0.013 (p = 0.000). This means that for each additional year of age, there is a 0.016 unit increase in the latent score. However, this effect is not statistically significant (p = 0.252), so we do not have strong evidence that Age alone affects technical rank.\ngenderxage: The interaction has an estimate of -0.039 (p = 0.036), which is statistically significant. This indicates that the effect of Age on technical rank differs by gender. Specifically, for females, the impact of Age is 0.039 units lower than for males.\n\nTest if the interaction is warranted\n\n#Hint: You need to create two models with clm(); one with interaction and one without. #Then you compare them using the anova test using anova()\n::: {.cell}\n\n```{.r .cell-code}\nmodel_int &lt;- clm(`Technical Rank` ~ Gender * Age, data = gb)\nmodel_no_int &lt;- clm(`Technical Rank` ~ Gender + Age, data = gb)\nanova(model_no_int, model_int)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLikelihood ratio tests of cumulative link models:\n \n             formula:                        link: threshold:\nmodel_no_int `Technical Rank` ~ Gender + Age logit flexible  \nmodel_int    `Technical Rank` ~ Gender * Age logit flexible  \n\n             no.par    AIC  logLik LR.stat df Pr(&gt;Chisq)  \nmodel_no_int      4 685.72 -338.86                        \nmodel_int         5 683.28 -336.64   4.437  1    0.03517 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n**interpretation: Including the Gender*Age interaction significantly improves the model (p &lt; .05), meaning that the effect of Age on the technical rank is different for different genders. In other words, the relationship between Age and technical rank depends on whether the baker is male or female.**\n\nUse ggemmeans to create a figure showing the interaction between Gender and Age as a function of rank. Plot predicted probabilities from the model.\n\n\nCode\ngb &lt;- gb %&gt;% rename(Technical_Rank = `Technical Rank`)\nmodel_int &lt;- clm(Technical_Rank ~ Gender * Age, data = gb)\npreds &lt;- ggemmeans(model_int, terms = c(\"Age [all]\", \"Gender\"), regrid = FALSE)\nplot(preds) +\n  labs(title = \"Predicted Probabilities for Technical_Rank by Age and Gender\",\n   x = \"Age\",\n   y = \"Predicted Probability\")\n\n\n\n\n\n\n\n\n\n\n\nLatent Visualization\n\n\nCode\nols_clm &lt;- MASS::polr(Technical_Rank ~ Gender * Age, data = gb)\n\nggeffect(ols_clm, terms = c(\"Age[all]\", \"Gender\"), latent = TRUE) %&gt;% \n  plot() +\n  labs(title = \"Effect of Age and Gender on Technical_Rank\",\n       x = \"Age\",\n       y = \"Latent Variable\")\n\n\n\n\n\n\n\n\n\n\nUse the Brant test to support or reject the hypothesis that the proportional odds assumption holds for your simplified model.\n\n\nCode\nbrant.test(ols_clm)\n\n\n\nBrant Test:\n               chi-sq   df   pr(&gt;chi)\nOmnibus         1.295    3       0.73\nGenderF         0.585    1       0.44\nAge             1.052    1       0.31\nGenderF:Age     0.924    1       0.34\n\nH0: Proportional odds assumption holds\n\n\n\nThe omnibus chi-square statistic is 1.295 with 3 degrees of freedom and a p-value of 0.73. Since this p-value is not significant, we do not reject the null hypothesis. This indicates that overall the proportional odds assumption holds for the model. None of the tests (including all the individual tests) are statistically significant, we conclude that there is no evidence to reject the null hypothesis. Thus, the proportional odds assumption holds for the simplified model.\n## `brms`\n\nBelow is a model implementation using the brms package. We will just use the default priors for this. The exercise is to run this code and note your observations. What are salient differences you observe in how the model fitting takes place With respect to the results, how do you compare the results of the model you fit with clm and the one you fit with brms?\n\n\n\nCode\n  ols2_brm &lt;- brm(Technical_Rank ~ Gender * Age, \n                  data = gb, \n                  family = cumulative, \n                  cores = 4, \n                  chains = 4, \n                  seed = 123)\n\n\nWhile both clm and brms provide very similar point estimates and conclusions regarding the effects of Gender, Age, and their interaction on Technical_Rank, the Bayesian approach (brms) offers richer information regarding uncertainty (CI) and additional convergence diagnostics.\n\nThe conditional_effects function is used to plot predicted probabilities by Gender and Age across each rank.\n::: {.cell}\n\nCode\nconditional_effects(ols2_brm, categorical = T)\n\n::: {.cell-output-display}  :::\n::: {.cell-output-display}  ::: :::\ncheck_predictions from the easystats performance package is used for examining model fit (i.e., does the data fit the model being used?). Run the below code. What do you think?\n\n\n\nCode\ncheck_predictions(ols2_brm)\n\n\n\n\n\n\n\n\n\nThis function shows how well the Bayesian model’s predictions align with the data. The green dots represent the actual counts (or frequencies) for each category of Technical_Rank in the dataset. The blue dots and error bars represent the mean (or median) model-predicted counts, while the error bars reflect the uncertainty (95% CI) around the predictions. The model predictions align fairly well with the actual data."
  },
  {
    "objectID": "assignments/ordinal_regression.html",
    "href": "assignments/ordinal_regression.html",
    "title": "Ordinal Regression Assignment",
    "section": "",
    "text": "If you are fitting a model, display the model output in a neatly formatted table. (The tidy and kable functions can help!)\nIf you are creating a plot, use clear labels for all axes, titles, etc.\nIf you are using Github, don’t forget to commit and push your work to to it regularly, at least after each exercise. Write short and informative commit messages. Else, if you are submitting on Canvas, make sure that the version you submit is the latest, and that it runs/knits without any errors.\nWhen you’re done, we should be able to knit the final version of the QMD in your GitHub as a HTML."
  },
  {
    "objectID": "assignments/ordinal_regression.html#instructions",
    "href": "assignments/ordinal_regression.html#instructions",
    "title": "Ordinal Regression Assignment",
    "section": "",
    "text": "If you are fitting a model, display the model output in a neatly formatted table. (The tidy and kable functions can help!)\nIf you are creating a plot, use clear labels for all axes, titles, etc.\nIf you are using Github, don’t forget to commit and push your work to to it regularly, at least after each exercise. Write short and informative commit messages. Else, if you are submitting on Canvas, make sure that the version you submit is the latest, and that it runs/knits without any errors.\nWhen you’re done, we should be able to knit the final version of the QMD in your GitHub as a HTML."
  },
  {
    "objectID": "assignments/ordinal_regression.html#load-packages",
    "href": "assignments/ordinal_regression.html#load-packages",
    "title": "Ordinal Regression Assignment",
    "section": "Load packages:",
    "text": "Load packages:\n\n\nCode\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(performance)\nlibrary(ordinal) #clm\nlibrary(car) # anova\nlibrary(ggeffects) #  viz\nlibrary(gofcat) # brant\nlibrary(brms)\nlibrary(emmeans) # contrasts\nlibrary(knitr)"
  },
  {
    "objectID": "assignments/ordinal_regression.html#load-data",
    "href": "assignments/ordinal_regression.html#load-data",
    "title": "Ordinal Regression Assignment",
    "section": "Load data",
    "text": "Load data\n\nMake sure only the top 3 ranks are being used. For some reason, there are missing ranks (my guess is they did not announce rank on TV)\n\n\n\nCode\ngbbo &lt;- read_csv(\"https://raw.githubusercontent.com/suyoghc/PSY-504_Spring-2025/refs/heads/main/Ordinal%20Regression/data/GBBO.csv\")\n\n# Enter code to filter. Think about the data type that would be relevant for Rank\ngb &lt;- gbbo %&gt;% \n  filter(!is.na(`Technical Rank`) & `Technical Rank` %in% c(1, 2, 3))"
  },
  {
    "objectID": "assignments/ordinal_regression.html#explore",
    "href": "assignments/ordinal_regression.html#explore",
    "title": "Ordinal Regression Assignment",
    "section": "Explore",
    "text": "Explore\n\nPlot two figures showing the percentage of bakers in each rank— create one for Gender and Age\n\n\nCode\n#plot percentage of bakers in each rank by gender\ngender_rank &lt;- gb %&gt;%\n  group_by(Gender, `Technical Rank`) %&gt;%\n  summarise(n = n(), .groups = 'drop') %&gt;%\n  mutate(perc = n / sum(n) * 100)\n\nggplot(gender_rank, aes(x = factor(`Technical Rank`), y = perc, fill = Gender)) +\n  geom_bar(stat = \"identity\", position = position_dodge(width = 0.8)) +\n  labs(title = \"Percentage of Bakers in Each Technical Rank by Gender\",\n   x = \"Technical Rank\",\n   y = \"Percentage\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n #plot percentage of bakers in each rank by age group\ngb &lt;- gb %&gt;% \n  mutate(AgeGroup = cut(Age, \n                        breaks = seq(floor(min(Age, na.rm = TRUE)), ceiling(max(Age, na.rm = TRUE)), by = 10),\n                        include.lowest = TRUE, right = FALSE))\n\nage_rank &lt;- gb %&gt;%\n  group_by(AgeGroup, `Technical Rank`) %&gt;%\n  summarise(n = n(), .groups = 'drop') %&gt;%\n  mutate(perc = n / sum(n) * 100)\n\nggplot(age_rank, aes(x = AgeGroup, y = perc, fill = factor(`Technical Rank`))) +\n  geom_bar(stat = \"identity\", position = position_dodge(width = 0.8)) +\n  labs(title = \"Percentage of Bakers in Each Technical Rank by Age Group\",\n       x = \"Age Group\",\n       y = \"Percentage\",\n       fill = \"Technical Rank\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\nCode\n#here's a plot without grouping age into bins, which is less informative visually so I prefer the above plot\nage_rank_cont &lt;- gb %&gt;%\n  group_by(Age, `Technical Rank`) %&gt;%\n  summarise(n = n(), .groups = 'drop') %&gt;%\n  \n  group_by(Age) %&gt;%\n  mutate(perc = n / sum(n) * 100) %&gt;%\n  ungroup()\n\nggplot(age_rank_cont, aes(x = factor(Age), y = perc, fill = factor(`Technical Rank`))) +\n  geom_bar(stat = \"identity\", position = position_dodge(width = 0.8)) +\n  labs(title = \"Percentage of Bakers in Each Technical Rank by Age\",\n       x = \"Age\",\n       y = \"Percentage\",\n       fill = \"Technical Rank\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "assignments/ordinal_regression.html#ordinal-analysis",
    "href": "assignments/ordinal_regression.html#ordinal-analysis",
    "title": "Ordinal Regression Assignment",
    "section": "Ordinal Analysis",
    "text": "Ordinal Analysis\n\nIf you haven’t already, convert the outcome variable to an ordered factor. What does the order here represent?\n\n\nCode\n#The order now represents the ranking from best (1) to worst (3) among the top three.\ngb &lt;- gb %&gt;%\n  mutate(`Technical Rank` = factor(`Technical Rank`, levels = c(1, 2, 3), ordered = TRUE))\n\n\nConvert input variables to categorical factors as appropriate.\n\n\nCode\ngb &lt;- gb %&gt;%\n  mutate(Gender = factor(Gender, levels = c(\"M\", \"F\")))\n\n\nRun a ordinal logistic regression model against all relevant input variables. Interpret the effects for Gender, Age and Gender*Age (even if they are non-significant).\n\n\nCode\nmodel_int &lt;- clm(`Technical Rank` ~ Gender * Age, data = gb)\n\ntidy(model_int) %&gt;% kable(digits = 3)\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\ncoef.type\n\n\n\n\n1|2\n-0.267\n0.499\n-0.535\n0.593\nintercept\n\n\n2|3\n1.154\n0.504\n2.291\n0.022\nintercept\n\n\nGenderF\n1.149\n0.673\n1.708\n0.088\nlocation\n\n\nAge\n0.016\n0.014\n1.147\n0.252\nlocation\n\n\nGenderF:Age\n-0.039\n0.019\n-2.093\n0.036\nlocation\n\n\n\n\n\n\ninterpretation:\ngender: The coefficient for GenderF is 1.149 (p = 0.088). This means that, holding Age constant, females have a 1.149 unit higher latent score compared to males. Although this effect is only marginally significant (p = 0.088), it suggests that being female may be associated with a shift toward a higher rank category.\nage: The coefficient for Age is -0.013 (p = 0.000). This means that for each additional year of age, there is a 0.016 unit increase in the latent score. However, this effect is not statistically significant (p = 0.252), so we do not have strong evidence that Age alone affects technical rank.\ngenderxage: The interaction has an estimate of -0.039 (p = 0.036), which is statistically significant. This indicates that the effect of Age on technical rank differs by gender. Specifically, for females, the impact of Age is 0.039 units lower than for males.\n\nTest if the interaction is warranted\n\n#Hint: You need to create two models with clm(); one with interaction and one without. #Then you compare them using the anova test using anova()\n::: {.cell}\n\n```{.r .cell-code}\nmodel_int &lt;- clm(`Technical Rank` ~ Gender * Age, data = gb)\nmodel_no_int &lt;- clm(`Technical Rank` ~ Gender + Age, data = gb)\nanova(model_no_int, model_int)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLikelihood ratio tests of cumulative link models:\n \n             formula:                        link: threshold:\nmodel_no_int `Technical Rank` ~ Gender + Age logit flexible  \nmodel_int    `Technical Rank` ~ Gender * Age logit flexible  \n\n             no.par    AIC  logLik LR.stat df Pr(&gt;Chisq)  \nmodel_no_int      4 685.72 -338.86                        \nmodel_int         5 683.28 -336.64   4.437  1    0.03517 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n**interpretation: Including the Gender*Age interaction significantly improves the model (p &lt; .05), meaning that the effect of Age on the technical rank is different for different genders. In other words, the relationship between Age and technical rank depends on whether the baker is male or female.**\n\nUse ggemmeans to create a figure showing the interaction between Gender and Age as a function of rank. Plot predicted probabilities from the model.\n\n\nCode\ngb &lt;- gb %&gt;% rename(Technical_Rank = `Technical Rank`)\nmodel_int &lt;- clm(Technical_Rank ~ Gender * Age, data = gb)\npreds &lt;- ggemmeans(model_int, terms = c(\"Age [all]\", \"Gender\"), regrid = FALSE)\nplot(preds) +\n  labs(title = \"Predicted Probabilities for Technical_Rank by Age and Gender\",\n   x = \"Age\",\n   y = \"Predicted Probability\")\n\n\n\n\n\n\n\n\n\n\n\nLatent Visualization\n\n\nCode\nols_clm &lt;- MASS::polr(Technical_Rank ~ Gender * Age, data = gb)\n\nggeffect(ols_clm, terms = c(\"Age[all]\", \"Gender\"), latent = TRUE) %&gt;% \n  plot() +\n  labs(title = \"Effect of Age and Gender on Technical_Rank\",\n       x = \"Age\",\n       y = \"Latent Variable\")\n\n\n\n\n\n\n\n\n\n\nUse the Brant test to support or reject the hypothesis that the proportional odds assumption holds for your simplified model.\n\n\nCode\nbrant.test(ols_clm)\n\n\n\nBrant Test:\n               chi-sq   df   pr(&gt;chi)\nOmnibus         1.295    3       0.73\nGenderF         0.585    1       0.44\nAge             1.052    1       0.31\nGenderF:Age     0.924    1       0.34\n\nH0: Proportional odds assumption holds\n\n\n\nThe omnibus chi-square statistic is 1.295 with 3 degrees of freedom and a p-value of 0.73. Since this p-value is not significant, we do not reject the null hypothesis. This indicates that overall the proportional odds assumption holds for the model. None of the tests (including all the individual tests) are statistically significant, we conclude that there is no evidence to reject the null hypothesis. Thus, the proportional odds assumption holds for the simplified model.\n## `brms`\n\nBelow is a model implementation using the brms package. We will just use the default priors for this. The exercise is to run this code and note your observations. What are salient differences you observe in how the model fitting takes place With respect to the results, how do you compare the results of the model you fit with clm and the one you fit with brms?\n\n\n\nCode\n  ols2_brm &lt;- brm(Technical_Rank ~ Gender * Age, \n                  data = gb, \n                  family = cumulative, \n                  cores = 4, \n                  chains = 4, \n                  seed = 123)\n\n\nWhile both clm and brms provide very similar point estimates and conclusions regarding the effects of Gender, Age, and their interaction on Technical_Rank, the Bayesian approach (brms) offers richer information regarding uncertainty (CI) and additional convergence diagnostics.\n\nThe conditional_effects function is used to plot predicted probabilities by Gender and Age across each rank.\n::: {.cell}\n\nCode\nconditional_effects(ols2_brm, categorical = T)\n\n::: {.cell-output-display}  :::\n::: {.cell-output-display}  ::: :::\ncheck_predictions from the easystats performance package is used for examining model fit (i.e., does the data fit the model being used?). Run the below code. What do you think?\n\n\n\nCode\ncheck_predictions(ols2_brm)\n\n\n\n\n\n\n\n\n\nThis function shows how well the Bayesian model’s predictions align with the data. The green dots represent the actual counts (or frequencies) for each category of Technical_Rank in the dataset. The blue dots and error bars represent the mean (or median) model-predicted counts, while the error bars reflect the uncertainty (95% CI) around the predictions. The model predictions align fairly well with the actual data."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PSY504 Assignments Blog",
    "section": "",
    "text": "Lab 2 Logistic Regression Assignment\nLab 3 Ordinal Regression Assignment\nLab 4 Multinomial Regression Assignment\nLab 5 Poisson Regression Assignment\nLab 6 Multilevel Modeling Walkthrough Assignment\nLab 7 Multilevel Modeling2 Assignment"
  },
  {
    "objectID": "assignments/logistic_regression.html",
    "href": "assignments/logistic_regression.html",
    "title": "Logistic Regression Assignment",
    "section": "",
    "text": "Assignment requirements:\n\nIf you are using Github (recommended), make sure to commit and push your work to GitHub regularly, at least after each exercise. Write short and informative commit messages, and share the link to your assignment with me. If not, you can also send me the rmd & rendered file via Canvas.\nIn this assignment, you will not need to code from scratch. Rather, you’ll need to fill in code where needed. This assignment has a logisitic regression implementation for a scenario from EDA down to model comparison (and would be useful for whenever you may encounter such a situation in the future).\nI want the assignments to begin reflecting a bit more of how you’d be doing things on your own, where you have some prior knowledge and you figure other things out (by referring to documentation, etc.) . In addition to the rmd, I also want you to submit to me notes of anything new that you learn while finishing the assignment. And any pain-points, and we’ll discuss more.\n\nNote:\n\nIf you are fitting a model, display the model output in a neatly formatted table. (The gt tidy and kable functions can help!). Modelsummary also looks good(https://vincentarelbundock.github.io/modelsummary/articles/modelsummary.html)\nMake sure that your plots are clearly labeled – for all axes, titles, etc."
  },
  {
    "objectID": "assignments/logistic_regression.html#data-general-social-survey",
    "href": "assignments/logistic_regression.html#data-general-social-survey",
    "title": "Logistic Regression Assignment",
    "section": "Data: General Social Survey",
    "text": "Data: General Social Survey\nThe General Social Survey (GSS) has been used to measure trends in attitudes and behaviors in American society since 1972. In addition to collecting demographic information, the survey includes questions used to gauge attitudes about government spending priorities, confidence in institutions, lifestyle, and many other topics. A full description of the survey may be found here.\nThe data for this lab are from the 2016 General Social Survey. The original data set contains 2867 observations and 935 variables. We will use and abbreviated data set that includes the following variables:\nnatmass: Respondent’s answer to the following prompt:\n“We are faced with many problems in this country, none of which can be solved easily or inexpensively. I’m going to name some of these problems, and for each one I’d like you to tell me whether you think we’re spending too much money on it, too little money, or about the right amount…are we spending too much, too little, or about the right amount on mass transportation?”\nage: Age in years.\nsex: Sex recorded as male or female\nsei10: Socioeconomic index from 0 to 100\nregion: Region where interview took place\npolviews: Respondent’s answer to the following prompt:\n“We hear a lot of talk these days about liberals and conservatives. I’m going to show you a seven-point scale on which the political views that people might hold are arranged from extremely liberal - point 1 - to extremely conservative - point 7. Where would you place yourself on this scale?”\nThe data are in gss2016.csv in the data folder."
  },
  {
    "objectID": "assignments/logistic_regression.html#eda",
    "href": "assignments/logistic_regression.html#eda",
    "title": "Logistic Regression Assignment",
    "section": "EDA",
    "text": "EDA\n\nLet’s begin by making a binary variable for respondents’ views on spending on mass transportation. Create a new variable that is equal to “1” if a respondent said spending on mass transportation is about right and “0” otherwise. Then plot the proportion of the response variable, using informative labels for each category.\n\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(modelsummary)\nlibrary(tidyr)\nlibrary(knitr)\nlibrary(easystats)\nlibrary(broom)\nlibrary(emmeans)\nlibrary(marginaleffects)\nlibrary(performance)\nlibrary(arm)\nlibrary(modelsummary)\n\n\n\n\nCode\n# load data\ndata &lt;- read.csv(\"gss2016.csv\")\n\n\nFill in the “____” below to encode the binary variable\n\n\nCode\ndata &lt;- data %&gt;%\n   mutate(mass_trans_spend_right = ifelse(natmass == \"About right\", 1, 0))\n\n\n\n\nCode\n#Get proportions\nmass_spend_summary &lt;- data %&gt;%\n  count(mass_trans_spend_right) %&gt;%\n  mutate(proportion = n / sum(n))\n\n\n#Look at the dataframe structure. And make sure it's in a format that you can use for plotting.\n#Change structure if neederd\nmass_spend_long &lt;- mass_spend_summary %&gt;%\n  mutate(spend_category = ifelse(mass_trans_spend_right == 1, \"About right\", \"Not right\"))\n\n#Factorise for plot\nmass_spend_long$mass_trans_spend_right &lt;- as.factor(mass_spend_long$mass_trans_spend_right)\n\n#Make plot\n#Hint: geom_bar lets you make stacked bar charts\n\nggplot(mass_spend_summary, aes(x = factor(mass_trans_spend_right), y = proportion, fill = factor(mass_trans_spend_right))) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_manual(values = c(\"#E69F00\", \"#56B4E9\"),\n                   labels = c(\"Not right\", \"About right\")) +\n  labs(title = \"Proportion of Responses on Mass Transportation Spending\",\n       x = \"Response\",\n       y = \"Proportion\",\n       fill = \"Spending View\") +\n  scale_x_discrete(labels = c(\"Not right\", \"About right\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nRecode polviews so it is a factor with levels that are in an order that is consistent with question on the survey. Note how the categories are spelled in the data.\n\n\n\nCode\ndata &lt;- data %&gt;%\n  mutate(polviews = factor(polviews,\n                          levels = c(\"Extremely liberal\", \"Liberal\", \"Slightly liberal\", \n                                   \"Moderate\", \"Slghtly conservative\", \"Conservative\", \n                                   \"Extrmly conservative\"),\n                          ordered = TRUE))\n\n\n\nMake a plot of the distribution of polviews\n\n\n\nCode\n#Get proportions, format, and produce a plot like you did previously for mass_trans_spend_right\npalette &lt;- c(\n  \"#772e25\", \"#c44536\", \"#ee9b00\", \"#197278\", \"#283d3b\", \n  \"#9CC5A1\", \"#6195C6\", \"#ADA7C9\", \"#4D4861\", \"grey50\",\n  \"#d4a373\", \"#8a5a44\", \"#4a6a74\", \"#5c80a8\", \"#a9c5a0\",\n  \"#7b9b8e\", \"#e1b16a\", \"#a69b7c\", \"#9d94c4\", \"#665c54\"\n)\n\npalette_condition = c(\"#ee9b00\", \"#c44536\",\"#005f73\", \"#283d3b\", \"#9CC5A1\", \"#6195C6\", \"#ADA7C9\", \"#4D4861\")\n\nplot_aes = theme_minimal() +\n  theme(legend.position = \"top\",\n        legend.text = element_text(size = 12),\n        text = element_text(size = 16, family = \"Futura Medium\"),\n        axis.text = element_text(color = \"black\"),\n        axis.ticks.y = element_blank())\n\npolviews_summary &lt;- data %&gt;% #proportion\n  count(polviews) %&gt;%\n  mutate(proportion = n / sum(n))\n\nggplot(polviews_summary, aes(x = polviews, y = proportion, fill = polviews)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_manual(values = palette) + \n  labs(title = \"Distribution of Political Views\",\n       x = \"Political Views\",\n       y = \"Proportion\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\nCode\n  plot_aes\n\n\nList of 136\n $ line                            :List of 6\n  ..$ colour       : chr \"black\"\n  ..$ linewidth    : num 0.5\n  ..$ linetype     : num 1\n  ..$ lineend      : chr \"butt\"\n  ..$ arrow        : logi FALSE\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_line\" \"element\"\n $ rect                            :List of 5\n  ..$ fill         : chr \"white\"\n  ..$ colour       : chr \"black\"\n  ..$ linewidth    : num 0.5\n  ..$ linetype     : num 1\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_rect\" \"element\"\n $ text                            :List of 11\n  ..$ family       : chr \"Futura Medium\"\n  ..$ face         : chr \"plain\"\n  ..$ colour       : chr \"black\"\n  ..$ size         : num 16\n  ..$ hjust        : num 0.5\n  ..$ vjust        : num 0.5\n  ..$ angle        : num 0\n  ..$ lineheight   : num 0.9\n  ..$ margin       : 'margin' num [1:4] 0points 0points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : logi FALSE\n  ..$ inherit.blank: logi FALSE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ title                           : NULL\n $ aspect.ratio                    : NULL\n $ axis.title                      : NULL\n $ axis.title.x                    :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 1\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 2.75points 0points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.title.x.top                :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 0\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 0points 2.75points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.title.x.bottom             : NULL\n $ axis.title.y                    :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 1\n  ..$ angle        : num 90\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 2.75points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.title.y.left               : NULL\n $ axis.title.y.right              :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 1\n  ..$ angle        : num -90\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 0points 0points 2.75points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text                       :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : chr \"black\"\n  ..$ size         : 'rel' num 0.8\n  ..$ hjust        : NULL\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi FALSE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text.x                     :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 1\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 2.2points 0points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text.x.top                 :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 0\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 0points 2.2points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text.x.bottom              : NULL\n $ axis.text.y                     :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : num 1\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 2.2points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text.y.left                : NULL\n $ axis.text.y.right               :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : num 0\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 0points 0points 2.2points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text.theta                 : NULL\n $ axis.text.r                     :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : num 0.5\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 2.2points 0points 2.2points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.ticks                      : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ axis.ticks.x                    : NULL\n $ axis.ticks.x.top                : NULL\n $ axis.ticks.x.bottom             : NULL\n $ axis.ticks.y                    : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ axis.ticks.y.left               : NULL\n $ axis.ticks.y.right              : NULL\n $ axis.ticks.theta                : NULL\n $ axis.ticks.r                    : NULL\n $ axis.minor.ticks.x.top          : NULL\n $ axis.minor.ticks.x.bottom       : NULL\n $ axis.minor.ticks.y.left         : NULL\n $ axis.minor.ticks.y.right        : NULL\n $ axis.minor.ticks.theta          : NULL\n $ axis.minor.ticks.r              : NULL\n $ axis.ticks.length               : 'simpleUnit' num 2.75points\n  ..- attr(*, \"unit\")= int 8\n $ axis.ticks.length.x             : NULL\n $ axis.ticks.length.x.top         : NULL\n $ axis.ticks.length.x.bottom      : NULL\n $ axis.ticks.length.y             : NULL\n $ axis.ticks.length.y.left        : NULL\n $ axis.ticks.length.y.right       : NULL\n $ axis.ticks.length.theta         : NULL\n $ axis.ticks.length.r             : NULL\n $ axis.minor.ticks.length         : 'rel' num 0.75\n $ axis.minor.ticks.length.x       : NULL\n $ axis.minor.ticks.length.x.top   : NULL\n $ axis.minor.ticks.length.x.bottom: NULL\n $ axis.minor.ticks.length.y       : NULL\n $ axis.minor.ticks.length.y.left  : NULL\n $ axis.minor.ticks.length.y.right : NULL\n $ axis.minor.ticks.length.theta   : NULL\n $ axis.minor.ticks.length.r       : NULL\n $ axis.line                       : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ axis.line.x                     : NULL\n $ axis.line.x.top                 : NULL\n $ axis.line.x.bottom              : NULL\n $ axis.line.y                     : NULL\n $ axis.line.y.left                : NULL\n $ axis.line.y.right               : NULL\n $ axis.line.theta                 : NULL\n $ axis.line.r                     : NULL\n $ legend.background               : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ legend.margin                   : 'margin' num [1:4] 5.5points 5.5points 5.5points 5.5points\n  ..- attr(*, \"unit\")= int 8\n $ legend.spacing                  : 'simpleUnit' num 11points\n  ..- attr(*, \"unit\")= int 8\n $ legend.spacing.x                : NULL\n $ legend.spacing.y                : NULL\n $ legend.key                      : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ legend.key.size                 : 'simpleUnit' num 1.2lines\n  ..- attr(*, \"unit\")= int 3\n $ legend.key.height               : NULL\n $ legend.key.width                : NULL\n $ legend.key.spacing              : 'simpleUnit' num 5.5points\n  ..- attr(*, \"unit\")= int 8\n $ legend.key.spacing.x            : NULL\n $ legend.key.spacing.y            : NULL\n $ legend.frame                    : NULL\n $ legend.ticks                    : NULL\n $ legend.ticks.length             : 'rel' num 0.2\n $ legend.axis.line                : NULL\n $ legend.text                     :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : num 12\n  ..$ hjust        : NULL\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi FALSE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ legend.text.position            : NULL\n $ legend.title                    :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : num 0\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ legend.title.position           : NULL\n $ legend.position                 : chr \"top\"\n $ legend.position.inside          : NULL\n $ legend.direction                : NULL\n $ legend.byrow                    : NULL\n $ legend.justification            : chr \"center\"\n $ legend.justification.top        : NULL\n $ legend.justification.bottom     : NULL\n $ legend.justification.left       : NULL\n $ legend.justification.right      : NULL\n $ legend.justification.inside     : NULL\n $ legend.location                 : NULL\n $ legend.box                      : NULL\n $ legend.box.just                 : NULL\n $ legend.box.margin               : 'margin' num [1:4] 0cm 0cm 0cm 0cm\n  ..- attr(*, \"unit\")= int 1\n $ legend.box.background           : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ legend.box.spacing              : 'simpleUnit' num 11points\n  ..- attr(*, \"unit\")= int 8\n  [list output truncated]\n - attr(*, \"class\")= chr [1:2] \"theme\" \"gg\"\n - attr(*, \"complete\")= logi TRUE\n - attr(*, \"validate\")= logi TRUE\n\n\n\nWhich political view occurs most frequently in this data set?"
  },
  {
    "objectID": "assignments/logistic_regression.html#logistic-regression",
    "href": "assignments/logistic_regression.html#logistic-regression",
    "title": "Logistic Regression Assignment",
    "section": "Logistic regression",
    "text": "Logistic regression\n\nLet’s start by fitting a logistic regression model with just the intercept\n\n\n\nCode\nintercept_only_model &lt;- glm(\n  mass_trans_spend_right ~ 1,\n  family = binomial(link = \"logit\"),\n  data = data\n)\nsummary(intercept_only_model)\n\n\n\nCall:\nglm(formula = mass_trans_spend_right ~ 1, family = binomial(link = \"logit\"), \n    data = data)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)  0.11906    0.03937   3.024  0.00249 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 3581.3  on 2589  degrees of freedom\nResidual deviance: 3581.3  on 2589  degrees of freedom\nAIC: 3583.3\n\nNumber of Fisher Scoring iterations: 3\n\n\n\nInterpret the intercept in the context of the data. You can do this by converting the \\(\\beta_0\\) parameter out of the log-odds metric to the probability metric. Make sure to include the 95% confidence intervals. Then interpret the results in a sentence or two–what is the basic thing this probability tells us about?\n\n\n\nCode\nb0 &lt;- coef(intercept_only_model)[1] # get coef\n\nb0_transformed &lt;- exp(b0) / (1 + exp(b0)) # logistic transform\nprint(b0_transformed)\n\n\n(Intercept) \n  0.5297297 \n\n\nCode\nci_lower = b0 - 1.96 * 0.0393685\nci_upper = b0 + 1.96 * 0.0393685\n\n#transforming confidence intervals of coefficients into probabilities\np_lower = exp(ci_lower) / (1 + exp(ci_lower))\np_upper = exp(ci_upper) / (1 + exp(ci_upper))\nprint(paste(\"95% CI: [\", round(p_lower, 3), \",\", round(p_upper, 3), \"]\"))\n\n\n[1] \"95% CI: [ 0.51 , 0.549 ]\"\n\n\nInterpretation: The intercept-only model predicts that approximately 53% of respondents (95% CI: [51%, 55%]) think mass transportation spending is “about right”. This represents the overall proportion of people satisfied with current mass transportation spending levels, without accounting for other factors such as demographic or political views.\n\nNow let’s fit a model using the demographic factors - age,sex, sei10 - to predict the odds a person is satisfied with spending on mass transportation. Make any necessary adjustments to the variables so the intercept will have a meaningful interpretation. Neatly display the model coefficients (do not display the summary output)\n\n\n\nCode\n#make sure that sex is a factor (i.e. to make sure R knows it's binary/categorical, and not continuous)\ndata$sex &lt;- as.factor(data$sex)\n\n#fit with glm()\nm1 &lt;- glm(\n  mass_trans_spend_right ~ sex + age + sei10,\n  family = binomial(link = \"logit\"),\n  data = data\n)\nm1 %&gt;%  #produce tidy output of model coefficients\n  tidy() %&gt;%  \n  kable()    \n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.8254509\n0.1395587\n5.914722\n0.0000000\n\n\nsexMale\n-0.2557439\n0.0798020\n-3.204732\n0.0013519\n\n\nage\n-0.0061659\n0.0022824\n-2.701502\n0.0069027\n\n\nsei10\n-0.0062271\n0.0016609\n-3.749229\n0.0001774\n\n\n\n\n\n\nConsider the relationship between sex and one’s opinion about spending on mass transportation. Interpret the coefficient of sex in terms of the logs odds and OR of being satisfied with spending on mass transportation. What are the predicted probabilities for males and females on support for spending on mass transportation? Please include the 95% CIs around each estimate.\n\n\n\nCode\nm1 %&gt;% \n  tidy() %&gt;%\n  kable()\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.8254509\n0.1395587\n5.914722\n0.0000000\n\n\nsexMale\n-0.2557439\n0.0798020\n-3.204732\n0.0013519\n\n\nage\n-0.0061659\n0.0022824\n-2.701502\n0.0069027\n\n\nsei10\n-0.0062271\n0.0016609\n-3.749229\n0.0001774\n\n\n\n\n\nCode\nm1 %&gt;% \n  tidy(exponentiate = TRUE) %&gt;%\n  kable()\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n2.2829100\n0.1395587\n5.914722\n0.0000000\n\n\nsexMale\n0.7743403\n0.0798020\n-3.204732\n0.0013519\n\n\nage\n0.9938530\n0.0022824\n-2.701502\n0.0069027\n\n\nsei10\n0.9937922\n0.0016609\n-3.749229\n0.0001774\n\n\n\n\n\nCode\nbsex &lt;- coef(m1)[\"sexMale\"]\n\nci_lower_lo = bsex - 1.96 * 0.0798020\nci_upper_lo = bsex + 1.96 * 0.0798020\n\nci_lower_or = 1.29 - 1.96 * 0.0798020\nci_upper_or = 1.29 + 1.96 * 0.0798020\n\nlist(\n  \"CI for log-odds\" = c(ci_lower_lo, ci_upper_lo),\n  \"CI for Odds Ratio\" = c(ci_lower_or, ci_upper_or)\n)\n\n\n$`CI for log-odds`\n    sexMale     sexMale \n-0.41215578 -0.09933194 \n\n$`CI for Odds Ratio`\n[1] 1.133588 1.446412\n\n\nCode\nemm_sex &lt;- emmeans(m1, \"sex\", type = \"response\")\n\n\nIf you did this right, you’ll find that being female (as compared to male) is associated with an increase in the log-odds of being satisfied with spending on mass transportation by 0.2557439 units (95% CI [0.09, 0.41]), holding all other variables constant. This equates to the odds of thinking the spending amount is right in females being 1.29 times the odds of thinking this in men (95% CI [1.13, 1.44]).\nThe predicted probability for females to be satisfied with spending on mass transportation is 55.9% (95% CI [53.3%, 58.5%]) and that of males is 49.5% (95% CI [46.7%, 52.4%]).\n\nVerify this.\n\nNext, consider the relationship between age and one’s opinion about spending on mass transportation. Interpret the coefficient of age in terms of the logs odds and OR of being satisfied with spending on mass transportation. Please include the 95% CIs around each estimate.\n\n\n\nCode\nbage &lt;- coef(m1)[\"age\"]\nse_age &lt;- sqrt(vcov(m1)[\"age\", \"age\"])\nprint(bage)\n\n\n        age \n-0.00616594 \n\n\nCode\nprint(se_age)\n\n\n[1] 0.002282412\n\n\nCode\nci_lower_lo = bage - 1.96 * se_age\nci_upper_lo = bage + 1.96 * se_age\n\nor_age &lt;- exp(bage)\nci_lower_or = exp(ci_lower_lo)\nci_upper_or = exp(ci_upper_lo)\nprint(or_age)\n\n\n     age \n0.993853 \n\n\nCode\nlist(\n  \"CI for log-odds\" = c(ci_lower_lo, ci_upper_lo),\n  \"CI for Odds Ratio\" = c(ci_lower_or, ci_upper_or)\n)\n\n\n$`CI for log-odds`\n         age          age \n-0.010639467 -0.001692412 \n\n$`CI for Odds Ratio`\n      age       age \n0.9894169 0.9983090 \n\n\nA one unit increase in age is associated with a decrease in the log-odds of being satisfied with spending on mass transportation by -0.0062, holding all other variables constant. The odds ratio is 0.994, which confirms the negative relationship implied by the log-odds coefficient. Specifically, for each additional unit of age, the odds of being satisfied with mass transportation spending decrease by a factor of about 0.994, or approximately 0.6% per unit increase in age, holding other factors constant.\n\nConsider the relationship between SES and one’s opinion about spending on mass transportation. Interpret the coefficient of SES in terms of the logs odds and OR of being satisfied with spending on mass transportation. Please include the 95% CIs around each estimate. ß\n\n\n\nCode\nbses &lt;- coef(m1)\nbses &lt;- coef(m1)[\"sei10\"]\n\n\nbses &lt;- coef(m1)[\"sei10\"]\nses_se &lt;- sqrt(vcov(m1)[\"sei10\", \"sei10\"])\n\nci_lower_log_odds &lt;- bses - 1.96 * ses_se\nci_upper_log_odds &lt;- bses + 1.96 * ses_se\n\nodds_ratio &lt;- exp(bses)\nci_lower_odds_ratio &lt;- exp(ci_lower_log_odds)\nci_upper_odds_ratio &lt;- exp(ci_upper_log_odds)\n\nresults &lt;- data.frame(\n  Metric = c(\"Log-odds coefficient\", \n             \"95% CI for log-odds\", \n             \"Odds Ratio\", \n             \"95% CI for Odds Ratio\"),\n  Value = c(\n    format(round(bses, 4), nsmall = 4),\n    paste0(\"[\", format(round(ci_lower_log_odds, 4), nsmall = 4), \", \", \n           format(round(ci_upper_log_odds, 4), nsmall = 4), \"]\"),\n    format(round(odds_ratio, 4), nsmall = 4),\n    paste0(\"[\", format(round(ci_lower_odds_ratio, 4), nsmall = 4), \", \", \n           format(round(ci_upper_odds_ratio, 4), nsmall = 4), \"]\")\n  )\n)\n\n# Display using kable\nkable(results)\n\n\n\n\n\nMetric\nValue\n\n\n\n\nLog-odds coefficient\n-0.0062\n\n\n95% CI for log-odds\n[-0.0095, -0.0030]\n\n\nOdds Ratio\n0.9938\n\n\n95% CI for Odds Ratio\n[0.9906, 0.9970]\n\n\n\n\n\nA one unit increase in SES index is associated with a decrease in the log-odds of being satisfied with spending on mass transportation by 0.0062 units (95% CI [-0.0107, -0.0017]), holding all other variables constant. The odds ratio is less than 1 (0.9937922), which confirms the negative relationship implied by the log-odds coefficient. Specifically, for each additional unit of SES index, the odds of being satisfied with mass transportation spending decrease by a factor of about 0.993, or approximately 0.7% per unit increase in SES index, holding other factors constant (95% CI [0.989, 0.998])."
  },
  {
    "objectID": "assignments/logistic_regression.html#marginal-effects",
    "href": "assignments/logistic_regression.html#marginal-effects",
    "title": "Logistic Regression Assignment",
    "section": "Marginal effects",
    "text": "Marginal effects\n\nLet’s examine the results on the probability scale.\n\n\nCalculate the marginal effects of sex, age, and SES on mass transportation spending. You can use the margins package function margins discussed in your textbook or you can use the marginaleffects package avg_slope avg_comparisons discussed in lecture. Interpret each estimate.\n\n\n\nCode\navg_comparisons(m1, comparison = \"difference\") %&gt;% \n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\ncontrast\nestimate\nstd.error\nstatistic\np.value\ns.value\nconf.low\nconf.high\n\n\n\n\nage\n+1\n-0.0015153\n0.0005579\n-2.716128\n0.0066050\n7.242218\n-0.0026088\n-0.0004219\n\n\nsei10\n+1\n-0.0015304\n0.0004039\n-3.789362\n0.0001510\n12.692832\n-0.0023219\n-0.0007388\n\n\nsex\nMale - Female\n-0.0630688\n0.0196461\n-3.210251\n0.0013262\n9.558494\n-0.1015743\n-0.0245632\n\n\n\n\n\n\nThe marginal effect of age is -0.00152 (95% CI [0.00261, -0.00042]). So, for each additional unit increase of age, the probability of being satisfied with mass transportation spending decreases by approximately 0.15 percentage points, holding other factors constant (p = 0.007).\nThe marginal effect of SES is -0.00153 (95% CI [0.00232, -0.00074]). For each one-unit increase in the socioeconomic index, the probability of being satisfied with mass transportation spending decreases by approximately 0.15 percentage points, holding other variables constant.\nThe marginal effect for being female compared to male is 0.06 (95% CI [0.025, 0.102]). This indicates that females are, on average, about 6.31 percentage points more likely than males to be satisfied with mass transportation spending, holding other factors constant."
  },
  {
    "objectID": "assignments/logistic_regression.html#model-comparison",
    "href": "assignments/logistic_regression.html#model-comparison",
    "title": "Logistic Regression Assignment",
    "section": "Model comparison",
    "text": "Model comparison\n\nNow let’s see whether a person’s political views has a significant impact on their odds of being satisfied with spending on mass transportation, after accounting for the demographic factors.\n\n\nConduct a drop-in-deviance/likelihood ratio test to determine if polviews is a significant predictor of attitude towards spending on mass transportation. Name these two models fit2 and fit3, respectively. Compare the two models.\n\n\n\nCode\nfit2 &lt;- glm(\n  mass_trans_spend_right ~ sex + age + sei10,\n  family = binomial(link = \"logit\"),\n  data = data\n)\n\nfit3 &lt;- glm(\n  mass_trans_spend_right ~ sex + age + sei10 + polviews,\n  family = binomial(link = \"logit\"),\n  data = data\n)\n\ntest_likelihoodratio(fit2, fit3) %&gt;% \n  kable()\n\n\n\n\n\n\nName\nModel\ndf\ndf_diff\nChi2\np\n\n\n\n\nfit2\nfit2\nglm\n4\nNA\nNA\nNA\n\n\nfit3\nfit3\nglm\n10\n6\n63.02844\n0\n\n\n\n\n\n\nIs the model with polviews better than the model without?\n\n\nYes."
  },
  {
    "objectID": "assignments/logistic_regression.html#visualization",
    "href": "assignments/logistic_regression.html#visualization",
    "title": "Logistic Regression Assignment",
    "section": "Visualization",
    "text": "Visualization\n\nLet’s plot the results\nWe next use the model to produce visualizations:\n\nGiven the code below, interpet what is being plotted:\n\npol_plot : This plot shows the predicted probability of being satisfied with mass transportation spending across different political views. It shows that as political ideology becomes more conservative, the probability of being satisfied increases.\nsex_plot : This plot shows the predicted probability of satisfaction with mass transportation spending for males and females. It highlights that females are more likely than males to consider the current spending level as “about right.”\nses_plot: This plot shows the effect of SES on the predicted probability of satisfaction with mass transportation spending. It suggests that as SES increases, satisfaction decreases. The shaded region represents the confidence interval around the prediction.\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nadjust the various settings in your plot to make it look professional.\nYou can use ggeffects to get the predicted probabilities for these models.\n\n\n\n\n\n\nCode\nlibrary(ggeffects)\n\n\ncolors &lt;- c(\"Extremely liberal\" = \"black\",\n            \"Liberal\" = \"#0e2f44\",  # Dark blue\n            \"Slightly liberal\" = \"#1d5a6c\",  # Less dark blue\n            \"Moderate\" = \"#358ca3\",  # Medium blue\n            \"Slghtly conservative\" = \"#71b9d1\",  # Light blue\n            \"Conservative\" = \"#a6dcef\",  # Lighter blue\n            \"Extrmly conservative\" = \"#d0f0fd\")  # Very light blue\n\npp_pol &lt;- ggemmeans(fit3, terms = c(\"polviews\"))\n\n# Adjusted plot with gradient colors\npol_plot &lt;- ggplot(pp_pol, aes(x = x, y = predicted, color = x)) +\n  geom_point(size = 2) +\n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) +\n  scale_color_manual(values = colors) +\n  labs(title = \"Effect of Political Views on Satisfaction with Mass Transportation\",\n       x = \"Political Views\", y = \"Predicted Probability\",\n       color = \"Political Views\") +\n  theme_minimal()\n\npol_plot\n\n\n\n\n\n\n\n\n\nCode\npp_sex &lt;- ggemmeans(fit3, terms = c(\"sex\"))\n\nsex_plot &lt;- ggplot(pp_sex, aes(x = x, y = predicted, color = x)) +\n  geom_point(size = 2) +\n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) +\n  labs(title = \"Effect of Sex on Satisfaction with Mass Transportation\",\n       x = \"Sex\", y = \"Predicted Probability\",\n       color = \"Sex\") +\n  theme_minimal()\n\npp_sex\n\n\n# Predicted probabilities of mass_trans_spend_right\n\nsex    | Predicted |     95% CI\n-------------------------------\nFemale |      0.55 | 0.51, 0.58\nMale   |      0.48 | 0.44, 0.51\n\nAdjusted for:\n*   age = 48.90\n* sei10 = 46.07\n\n\nCode\npp_ses &lt;- ggemmeans(fit3, terms = \"sei10\")\n\n\nses_plot &lt;-  ggplot(pp_ses, aes(x = x, y = predicted)) +\n  geom_line(color = \"#2c7fb8\", size = 1) + \n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), fill = \"#2c7fb8\", alpha = 0.2) +  # Add a confidence interval band\n  labs(title = \"Effect of SES on Satisfaction with Mass Transportation\",\n       x = \"Socioeconomic Status\", y = \"Predicted Probability\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")  \nses_plot"
  },
  {
    "objectID": "assignments/logistic_regression.html#model-assumptions",
    "href": "assignments/logistic_regression.html#model-assumptions",
    "title": "Logistic Regression Assignment",
    "section": "Model Assumptions",
    "text": "Model Assumptions\n\nIs the logistic model a good choice for this data?\n\n\n\nCode\nbinned_residuals(fit2)\n\n\nWarning: About 86% of the residuals are inside the error bounds (~95% or higher would be good).\n\n\n\n\n\n\n\n\nNote\n\n\n\nAnswer: The plots do not reveal any major systematic deviations from randomness. This suggests that the logistic regression model is a good choice for these data."
  },
  {
    "objectID": "assignments/logistic_regression.html#model-fit",
    "href": "assignments/logistic_regression.html#model-fit",
    "title": "Logistic Regression Assignment",
    "section": "Model fit",
    "text": "Model fit\n\nCalculate the \\(R^2\\) for this model\n\n\n\nCode\nr2_mcfadden(fit2)\n\n\n# R2 for Generalized Linear Regression\n       R2: 0.010\n  adj. R2: 0.009\n\n\n\nR2 interpretation: The R^2 value suggests that while the predictors have a statistically significant effect, a substantial portion of the variability in satisfaction remains unexplained.\nNext, Take a look at the binned residual plots for each continuous predictor variable and look at linearity. Is there a predictor that sticks out? What can we do to improve model fit in this case?\n\n\n\nCode\nbinned_residuals(fit2, term=\"sei10\")\n\n\nWarning: About 88% of the residuals are inside the error bounds (~95% or higher would be good).\n\n\nCode\nbinned_residuals(fit2, term=\"age\")\n\n\nOk: About 98% of the residuals are inside the error bounds.\n\n\nCode\nbinned_residuals(fit2, term=\"sei10\") %&gt;% plot(show_dots=TRUE)\n\n\n\n\n\n\n\n\n\nCode\nbinned_residuals(fit2, term=\"age\") %&gt;% plot(show_dots=TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe binned residual plot for sei10 curves slightly, suggesting that SES may not have a simple linear relationship with satisfaction. The plot for age looks more linear. To improve the model, we could try adding a squared term for SES."
  },
  {
    "objectID": "assignments/logistic_regression.html#testing-polviews",
    "href": "assignments/logistic_regression.html#testing-polviews",
    "title": "Logistic Regression Assignment",
    "section": "Testing Polviews",
    "text": "Testing Polviews\n\n\nCode\nemmeans(fit3, \"polviews\") %&gt;% pairs() %&gt;% as.data.frame() %&gt;% filter(p.value &lt; .05)\n\n\n contrast                                   estimate        SE  df z.ratio\n Extremely liberal - Moderate             -0.9266262 0.1950664 Inf  -4.750\n Extremely liberal - Slghtly conservative -0.8487137 0.2127293 Inf  -3.990\n Extremely liberal - Conservative         -0.9935486 0.2108369 Inf  -4.712\n Extremely liberal - Extrmly conservative -1.3402621 0.2792876 Inf  -4.799\n Liberal - Moderate                       -0.7090022 0.1308520 Inf  -5.418\n Liberal - Slghtly conservative           -0.6310897 0.1555805 Inf  -4.056\n Liberal - Conservative                   -0.7759246 0.1532081 Inf  -5.065\n Liberal - Extrmly conservative           -1.1226380 0.2392048 Inf  -4.693\n Slightly liberal - Extrmly conservative  -0.7334002 0.2412625 Inf  -3.040\n p.value\n  &lt;.0001\n  0.0013\n  0.0001\n  &lt;.0001\n  &lt;.0001\n  0.0010\n  &lt;.0001\n  0.0001\n  0.0382\n\nResults are averaged over the levels of: sex \nResults are given on the log odds ratio (not the response) scale. \nP value adjustment: tukey method for comparing a family of 7 estimates \n\n\nCode\nemmeans(fit3, \"polviews\", type=\"response\") %&gt;% pairs() %&gt;% as.data.frame() %&gt;% filter(p.value &lt; .05)\n\n\n contrast                                 odds.ratio         SE  df null\n Extremely liberal / Moderate              0.3958871 0.07722426 Inf    1\n Extremely liberal / Slghtly conservative  0.4279651 0.09104070 Inf    1\n Extremely liberal / Conservative          0.3702605 0.07806458 Inf    1\n Extremely liberal / Extrmly conservative  0.2617771 0.07311109 Inf    1\n Liberal / Moderate                        0.4921350 0.06439684 Inf    1\n Liberal / Slghtly conservative            0.5320118 0.08277063 Inf    1\n Liberal / Conservative                    0.4602780 0.07051835 Inf    1\n Liberal / Extrmly conservative            0.3254202 0.07784206 Inf    1\n Slightly liberal / Extrmly conservative   0.4802732 0.11587191 Inf    1\n z.ratio p.value\n  -4.750  &lt;.0001\n  -3.990  0.0013\n  -4.712  0.0001\n  -4.799  &lt;.0001\n  -5.418  &lt;.0001\n  -4.056  0.0010\n  -5.065  &lt;.0001\n  -4.693  0.0001\n  -3.040  0.0382\n\nResults are averaged over the levels of: sex \nP value adjustment: tukey method for comparing a family of 7 estimates \nTests are performed on the log odds ratio scale \n\n\n\nConservatives are 2.7 times more likely to support mass transit spending compared to extremely liberal and liberal\nExtreme liberals are 2.5 times more likely to support spending compared to conservatives, moderates and slight conservatives\nExtrm conservatives are 3.1 times more likely to support mass spending than liberals and slight liberals\nLiberals are 2.0 times more likely to support spending than moderates and slight conservatives."
  },
  {
    "objectID": "assignments/logistic_regression.html#conclusion",
    "href": "assignments/logistic_regression.html#conclusion",
    "title": "Logistic Regression Assignment",
    "section": "Conclusion",
    "text": "Conclusion\nBased on the model summary below, and the three figures, we can conclude that both demographic factors and political views significantly influence satisfaction with mass transportation spending. Specifically, women are more likely to support current spending than men, older individuals are less likely to support current spending, and individuals with higher SES are less likely to support current spending. Political views also play a significant role, with conservatives more likely to support current spending than liberal individuals.\nThe logistic regression model fits the data reasonably well, but the residual analysis suggests SES might have a non-linear relationship with satisfaction. The R^2 value indicates that while the model explains some variability, additional factors may be at play. Future research could explore more complex relationships between SES and satisfaction to improve model fit.\n\n\n\n\nDf\nDeviance\nResid. Df\nResid. Dev\nPr(&gt;Chi)\n\n\n\n\nNULL\nNA\nNA\n2589\n3581.340\nNA\n\n\nsex\n1\n11.31903\n2588\n3570.021\n0.0007672\n\n\nage\n1\n10.10603\n2587\n3559.915\n0.0014778\n\n\nsei10\n1\n14.11908\n2586\n3545.796\n0.0001716\n\n\npolviews\n6\n63.02844\n2580\n3482.768\n0.0000000\n\n\n\nTable 1\n\n\n\n\n\nFigure 1: Effect of Sex on Satisfaction with Mass Transportation\n\n\n\n\n\n\n\n\n\nFigure 2: Effect of SES on Satisfaction with Mass Transportation\n\n\n\n\n\n\n\n\n\nFigure 3: Effect of Political Views on Satisfaction with Mass Transportation"
  },
  {
    "objectID": "assignments/your_assignment.html",
    "href": "assignments/your_assignment.html",
    "title": "Your Assignment Title",
    "section": "",
    "text": "yaml\nLab Goal: Predict voting frequency using demographic variables Data source: FiveThirtyEight “Why Many Americans Don’t Vote” survey Method: Multinomial logistic regression"
  },
  {
    "objectID": "assignments/your_assignment.html#data",
    "href": "assignments/your_assignment.html#data",
    "title": "Your Assignment Title",
    "section": "Data",
    "text": "Data\nThe data for this assignment comes from an online Ipsos survey that was conducted for the FiveThirtyEight article “Why Many Americans Don’t Vote”. You can read more about the survey design and respondents in the README of the GitHub repo for the data.\nRespondents were asked a variety of questions about their political beliefs, thoughts on multiple issues, and voting behavior. We will focus on using the demographic variables and someone’s party identification to understand whether a person is a probable voter.\nThe variables we’ll focus on were (definitions from the codebook in data set GitHub repo):\n\nppage: Age of respondent\neduc: Highest educational attainment category.\n\nrace: Race of respondent, census categories. Note: all categories except Hispanic were non-Hispanic.\ngender: Gender of respondent\nincome_cat: Household income category of respondent\nQ30: Response to the question “Generally speaking, do you think of yourself as a…”\n\n1: Republican\n2: Democrat\n3: Independent\n4: Another party, please specify\n5: No preference\n-1: No response\n\nvoter_category: past voting behavior:\n\nalways: respondent voted in all or all-but-one of the elections they were eligible in\nsporadic: respondent voted in at least two, but fewer than all-but-one of the elections they were eligible in\nrarely/never: respondent voted in 0 or 1 of the elections they were eligible in\n\n\nYou can read in the data directly from the GitHub repo:\n\n\nCode\nlibrary(nnet)\nlibrary(car)\nlibrary(tidyverse)\nlibrary(emmeans)\nlibrary(ggeffects)\nlibrary(knitr)\nlibrary(patchwork)\nlibrary(broom)\nlibrary(parameters)\nlibrary(easystats)\n\n\n\n\nCode\nvoter_data &lt;- read_csv(\"https://raw.githubusercontent.com/fivethirtyeight/data/master/non-voters/nonvoters_data.csv\")"
  },
  {
    "objectID": "assignments/your_assignment.html#lrt",
    "href": "assignments/your_assignment.html#lrt",
    "title": "Your Assignment Title",
    "section": "LRT",
    "text": "LRT\n\nRun the full model and report overall significance of each of the terms\n\n\n\nCode\nlibrary(car)\nAnova(model_with_pol, type = \"II\")\n\n\nAnalysis of Deviance Table (Type II tests)\n\nResponse: voter_category\n              LR Chisq Df Pr(&gt;Chisq)    \nppage_c         638.30  2  &lt; 2.2e-16 ***\nrace             52.65  6  1.379e-09 ***\ngender            6.03  2     0.0491 *  \nincome_cat       67.72  6  1.198e-12 ***\neduc            154.14  4  &lt; 2.2e-16 ***\npol_ident_new   153.84  6  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "assignments/your_assignment.html#marginal-effects-political-group---emmeans",
    "href": "assignments/your_assignment.html#marginal-effects-political-group---emmeans",
    "title": "Your Assignment Title",
    "section": "Marginal Effects Political Group - Emmeans",
    "text": "Marginal Effects Political Group - Emmeans\n\n\nCode\n#Get estimated marginal means from the model\n\nlibrary(emmeans)\n\nmultinomial_analysis &lt;- emmeans(model_with_pol, ~ pol_ident_new | voter_category)\n\ncoefs &lt;- contrast(regrid(multinomial_analysis, \"log\"), \"trt.vs.ctrl1\", by = \"pol_ident_new\")\n\nupdate(coefs, by = \"contrast\") %&gt;% \n  kable(format = \"markdown\", digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontrast\npol_ident_new\nestimate\nSE\ndf\nt.ratio\np.value\n\n\n\n\nsporadic - (rarely/never)\nDem\n0.961\n0.070\n28\n13.722\n0.000\n\n\nalways - (rarely/never)\nDem\n0.480\n0.074\n28\n6.498\n0.000\n\n\nsporadic - (rarely/never)\nIndep\n0.591\n0.077\n28\n7.643\n0.000\n\n\nalways - (rarely/never)\nIndep\n-0.049\n0.084\n28\n-0.590\n0.900\n\n\nsporadic - (rarely/never)\nOther\n0.078\n0.087\n28\n0.902\n0.747\n\n\nalways - (rarely/never)\nOther\n-0.835\n0.110\n28\n-7.577\n0.000\n\n\nsporadic - (rarely/never)\nRep\n0.883\n0.084\n28\n10.469\n0.000\n\n\nalways - (rarely/never)\nRep\n0.327\n0.089\n28\n3.672\n0.004"
  },
  {
    "objectID": "assignments/your_assignment.html#marginal-effects-of-education---emmeans",
    "href": "assignments/your_assignment.html#marginal-effects-of-education---emmeans",
    "title": "Your Assignment Title",
    "section": "Marginal Effects of Education - Emmeans",
    "text": "Marginal Effects of Education - Emmeans\n\n\nCode\nmulti_an_educ &lt;- emmeans(model_with_pol, ~ educ | voter_category)\n\ncoefs_educ &lt;- contrast(regrid(multi_an_educ, \"log\"), \"trt.vs.ctrl1\", by = \"educ\")\nupdate(coefs_educ, by = \"contrast\") %&gt;% \n  kable(format = \"markdown\", digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontrast\neduc\nestimate\nSE\ndf\nt.ratio\np.value\n\n\n\n\nsporadic - (rarely/never)\nCollege\n0.986\n0.076\n28\n12.904\n0.000\n\n\nalways - (rarely/never)\nCollege\n0.477\n0.080\n28\n5.960\n0.000\n\n\nsporadic - (rarely/never)\nHigh school or less\n0.187\n0.069\n28\n2.705\n0.031\n\n\nalways - (rarely/never)\nHigh school or less\n-0.711\n0.080\n28\n-8.883\n0.000\n\n\nsporadic - (rarely/never)\nSome college\n0.707\n0.074\n28\n9.512\n0.000\n\n\nalways - (rarely/never)\nSome college\n0.167\n0.079\n28\n2.114\n0.112\n\n\n\n\n\n\nNext, plot the predicted probabilities of voter category as a function of Age and Party ID\n\n\n\nCode\nlibrary(ggeffects)\nlibrary(ggplot2)\n\n# Get predicted probabilities for age and party identification\ndf_preds &lt;- ggemmeans(model_with_pol, terms = c(\"ppage_c\", \"pol_ident_new [all]\"))\n\nggplot(df_preds, aes(x = x, y = predicted, fill = response.level)) +\n  geom_area() +\n  geom_rug(sides = \"b\", position = \"jitter\", alpha = 0.5) +\n  labs(\n    x = \"Centered Age\",\n    y = \"Predicted Probability\",\n    title = \"Predicted Probabilities of Voting Frequency\\nby Age and Party Identification\",\n    fill = \"Voter Category\"\n  ) +\n  scale_fill_manual(\n    values = c(\n      \"always\" = \"#F6B533\", \n      \"sporadic\" = \"#D07EA2\", \n      \"rarely/never\" = \"#9854F7\"\n    ),\n    labels = c(\"ALMOST ALWAYS VOTE\", \"SOMETIMES VOTE\", \"RARELY OR NEVER VOTE\")\n  ) +\n  facet_wrap(~group) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nPlot predicted probabilities as a function of education and voting frequency.\n\n\nCode\nggemmeans(model_with_pol, terms = \"educ\") %&gt;%\n  ggplot(aes(x = x, y = predicted, fill = response.level)) +\n  geom_bar(stat = \"identity\", position = \"fill\") +\n  labs(x = \"Education\", y = \"Predicted Probability\", \n       title = \"Predicted Probabilities of Voting Frequency by Education\") +\n  scale_fill_manual(\n    name = \"Voter Category\",\n    values = c(\"always\" = \"#F6B533\", \"sporadic\" = \"#D07EA2\", \"rarely/never\" = \"#9854F7\"),\n    labels = c(\"ALMOST ALWAYS VOTE\", \"SOMETIMES VOTE\", \"RARELY OR NEVER VOTE\")\n  ) +\n    coord_flip() +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n### Write-up\n\nDifferences between political groups and voting behavior - Emmeans\n\n\nCode\nmultinomial_analysis &lt;- emmeans(model_with_pol, ~ pol_ident_new | voter_category)\n\ncoefs &lt;- contrast(regrid(multinomial_analysis, \"log\"), \"trt.vs.ctrl1\", by = \"pol_ident_new\")\n\nupdate(coefs, by = \"contrast\") %&gt;% \n  kable(format = \"markdown\", digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontrast\npol_ident_new\nestimate\nSE\ndf\nt.ratio\np.value\n\n\n\n\nsporadic - (rarely/never)\nDem\n0.961\n0.070\n28\n13.722\n0.000\n\n\nalways - (rarely/never)\nDem\n0.480\n0.074\n28\n6.498\n0.000\n\n\nsporadic - (rarely/never)\nIndep\n0.591\n0.077\n28\n7.643\n0.000\n\n\nalways - (rarely/never)\nIndep\n-0.049\n0.084\n28\n-0.590\n0.900\n\n\nsporadic - (rarely/never)\nOther\n0.078\n0.087\n28\n0.902\n0.747\n\n\nalways - (rarely/never)\nOther\n-0.835\n0.110\n28\n-7.577\n0.000\n\n\nsporadic - (rarely/never)\nRep\n0.883\n0.084\n28\n10.469\n0.000\n\n\nalways - (rarely/never)\nRep\n0.327\n0.089\n28\n3.672\n0.004\n\n\n\n\n\nCode\n# Pairwise comparisons (reverse pairwise contrasts)\ncontrast(coefs, \"revpairwise\", by = \"contrast\") %&gt;%\n  kable(format = \"markdown\", digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontrast1\ncontrast\nestimate\nSE\ndf\nt.ratio\np.value\n\n\n\n\nIndep - Dem\nsporadic - (rarely/never)\n-0.370\n0.094\n28\n-3.933\n0.003\n\n\nOther - Dem\nsporadic - (rarely/never)\n-0.883\n0.103\n28\n-8.578\n0.000\n\n\nOther - Indep\nsporadic - (rarely/never)\n-0.513\n0.107\n28\n-4.807\n0.000\n\n\nRep - Dem\nsporadic - (rarely/never)\n-0.078\n0.099\n28\n-0.787\n0.860\n\n\nRep - Indep\nsporadic - (rarely/never)\n0.292\n0.099\n28\n2.965\n0.029\n\n\nRep - Other\nsporadic - (rarely/never)\n0.805\n0.109\n28\n7.404\n0.000\n\n\nIndep - Dem\nalways - (rarely/never)\n-0.529\n0.101\n28\n-5.255\n0.000\n\n\nOther - Dem\nalways - (rarely/never)\n-1.315\n0.125\n28\n-10.508\n0.000\n\n\nOther - Indep\nalways - (rarely/never)\n-0.786\n0.129\n28\n-6.072\n0.000\n\n\nRep - Dem\nalways - (rarely/never)\n-0.153\n0.104\n28\n-1.470\n0.468\n\n\nRep - Indep\nalways - (rarely/never)\n0.376\n0.104\n28\n3.605\n0.006\n\n\nRep - Other\nalways - (rarely/never)\n1.162\n0.130\n28\n8.969\n0.000\n\n\n\n\n\nCode\n#This analysis shows that the differences in predicted probabilities across political groups are statistically significant. For instance, respondents with a Democrat or Independent affiliation may have higher odds of being consistent voters compared to those with Republican or Other affiliations. These differences are confirmed by significant pairwise contrasts.\n\n\n\n\nDifferences between education level and voting behavior - Emmeans\nLast part of the assignment: Interpret the results from running the following code for your model\n\n\nCode\nmultinomial_analysis &lt;- emmeans(model_with_pol, ~ educ | voter_category)\n\ncoefs &lt;- contrast(regrid(multinomial_analysis, \"log\"), \"trt.vs.ctrl1\", by = \"educ\")\n\nupdate(coefs, by = \"contrast\") %&gt;% \n  kable(format = \"markdown\", digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontrast\neduc\nestimate\nSE\ndf\nt.ratio\np.value\n\n\n\n\nsporadic - (rarely/never)\nCollege\n0.986\n0.076\n28\n12.904\n0.000\n\n\nalways - (rarely/never)\nCollege\n0.477\n0.080\n28\n5.960\n0.000\n\n\nsporadic - (rarely/never)\nHigh school or less\n0.187\n0.069\n28\n2.705\n0.031\n\n\nalways - (rarely/never)\nHigh school or less\n-0.711\n0.080\n28\n-8.883\n0.000\n\n\nsporadic - (rarely/never)\nSome college\n0.707\n0.074\n28\n9.512\n0.000\n\n\nalways - (rarely/never)\nSome college\n0.167\n0.079\n28\n2.114\n0.112\n\n\n\n\n\nCode\ncontrast(coefs, \"revpairwise\", by = \"contrast\") %&gt;%\n  kable(format = \"markdown\", digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontrast1\ncontrast\nestimate\nSE\ndf\nt.ratio\np.value\n\n\n\n\nHigh school or less - College\nsporadic - (rarely/never)\n-0.799\n0.095\n28\n-8.416\n0.000\n\n\nSome college - College\nsporadic - (rarely/never)\n-0.278\n0.092\n28\n-3.030\n0.014\n\n\nSome college - High school or less\nsporadic - (rarely/never)\n0.520\n0.088\n28\n5.920\n0.000\n\n\nHigh school or less - College\nalways - (rarely/never)\n-1.188\n0.104\n28\n-11.394\n0.000\n\n\nSome college - College\nalways - (rarely/never)\n-0.310\n0.097\n28\n-3.207\n0.009\n\n\nSome college - High school or less\nalways - (rarely/never)\n0.878\n0.098\n28\n8.995\n0.000\n\n\n\n\n\nEnter your interpretation here: The emmeans analysis for education level indicates significant differences in voting behavior across education groups. In this model, higher educational attainment is associated with higher predicted probabilities of being a consistent voter (“always”), while lower education levels are linked to increased probabilities of being sporadic or rarely/never voters. And these differences are statistically significant. The pairwise comparisons further confirm that adjacent education levels differ meaningfully in their impact on voting frequency, suggesting that educational attainment is an important predictor of voting behavior."
  },
  {
    "objectID": "assignments/multinomial_regression.html",
    "href": "assignments/multinomial_regression.html",
    "title": "Multinomidal Regression Assignment",
    "section": "",
    "text": "Lab Goal: Predict voting frequency using demographic variables Data source: FiveThirtyEight “Why Many Americans Don’t Vote” survey Method: Multinomial logistic regression"
  },
  {
    "objectID": "assignments/multinomial_regression.html#data",
    "href": "assignments/multinomial_regression.html#data",
    "title": "Multinomidal Regression Assignment",
    "section": "Data",
    "text": "Data\nThe data for this assignment comes from an online Ipsos survey that was conducted for the FiveThirtyEight article “Why Many Americans Don’t Vote”. You can read more about the survey design and respondents in the README of the GitHub repo for the data.\nRespondents were asked a variety of questions about their political beliefs, thoughts on multiple issues, and voting behavior. We will focus on using the demographic variables and someone’s party identification to understand whether a person is a probable voter.\nThe variables we’ll focus on were (definitions from the codebook in data set GitHub repo):\n\nppage: Age of respondent\neduc: Highest educational attainment category.\n\nrace: Race of respondent, census categories. Note: all categories except Hispanic were non-Hispanic.\ngender: Gender of respondent\nincome_cat: Household income category of respondent\nQ30: Response to the question “Generally speaking, do you think of yourself as a…”\n\n1: Republican\n2: Democrat\n3: Independent\n4: Another party, please specify\n5: No preference\n-1: No response\n\nvoter_category: past voting behavior:\n\nalways: respondent voted in all or all-but-one of the elections they were eligible in\nsporadic: respondent voted in at least two, but fewer than all-but-one of the elections they were eligible in\nrarely/never: respondent voted in 0 or 1 of the elections they were eligible in\n\n\nYou can read in the data directly from the GitHub repo:\n\n\nCode\nlibrary(nnet)\nlibrary(car)\nlibrary(tidyverse)\nlibrary(emmeans)\nlibrary(ggeffects)\nlibrary(knitr)\nlibrary(patchwork)\nlibrary(broom)\nlibrary(parameters)\nlibrary(easystats)\n\n\n\n\nCode\nvoter_data &lt;- read_csv(\"https://raw.githubusercontent.com/fivethirtyeight/data/master/non-voters/nonvoters_data.csv\")"
  },
  {
    "objectID": "assignments/multinomial_regression.html#lrt",
    "href": "assignments/multinomial_regression.html#lrt",
    "title": "Multinomidal Regression Assignment",
    "section": "LRT",
    "text": "LRT\n\nRun the full model and report overall significance of each of the terms\n\n\n\nCode\nlibrary(car)\nAnova(model_with_pol, type = \"II\")\n\n\nAnalysis of Deviance Table (Type II tests)\n\nResponse: voter_category\n              LR Chisq Df Pr(&gt;Chisq)    \nppage_c         638.30  2  &lt; 2.2e-16 ***\nrace             52.65  6  1.379e-09 ***\ngender            6.03  2     0.0491 *  \nincome_cat       67.72  6  1.198e-12 ***\neduc            154.14  4  &lt; 2.2e-16 ***\npol_ident_new   153.84  6  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "assignments/multinomial_regression.html#marginal-effects-political-group---emmeans",
    "href": "assignments/multinomial_regression.html#marginal-effects-political-group---emmeans",
    "title": "Multinomidal Regression Assignment",
    "section": "Marginal Effects Political Group - Emmeans",
    "text": "Marginal Effects Political Group - Emmeans\n\n\nCode\n#Get estimated marginal means from the model\n\nlibrary(emmeans)\n\nmultinomial_analysis &lt;- emmeans(model_with_pol, ~ pol_ident_new | voter_category)\n\ncoefs &lt;- contrast(regrid(multinomial_analysis, \"log\"), \"trt.vs.ctrl1\", by = \"pol_ident_new\")\n\nupdate(coefs, by = \"contrast\") %&gt;% \n  kable(format = \"markdown\", digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontrast\npol_ident_new\nestimate\nSE\ndf\nt.ratio\np.value\n\n\n\n\nsporadic - (rarely/never)\nDem\n0.961\n0.070\n28\n13.722\n0.000\n\n\nalways - (rarely/never)\nDem\n0.480\n0.074\n28\n6.498\n0.000\n\n\nsporadic - (rarely/never)\nIndep\n0.591\n0.077\n28\n7.643\n0.000\n\n\nalways - (rarely/never)\nIndep\n-0.049\n0.084\n28\n-0.590\n0.900\n\n\nsporadic - (rarely/never)\nOther\n0.078\n0.087\n28\n0.902\n0.747\n\n\nalways - (rarely/never)\nOther\n-0.835\n0.110\n28\n-7.577\n0.000\n\n\nsporadic - (rarely/never)\nRep\n0.883\n0.084\n28\n10.469\n0.000\n\n\nalways - (rarely/never)\nRep\n0.327\n0.089\n28\n3.672\n0.004"
  },
  {
    "objectID": "assignments/multinomial_regression.html#marginal-effects-of-education---emmeans",
    "href": "assignments/multinomial_regression.html#marginal-effects-of-education---emmeans",
    "title": "Multinomidal Regression Assignment",
    "section": "Marginal Effects of Education - Emmeans",
    "text": "Marginal Effects of Education - Emmeans\n\n\nCode\nmulti_an_educ &lt;- emmeans(model_with_pol, ~ educ | voter_category)\n\ncoefs_educ &lt;- contrast(regrid(multi_an_educ, \"log\"), \"trt.vs.ctrl1\", by = \"educ\")\nupdate(coefs_educ, by = \"contrast\") %&gt;% \n  kable(format = \"markdown\", digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontrast\neduc\nestimate\nSE\ndf\nt.ratio\np.value\n\n\n\n\nsporadic - (rarely/never)\nCollege\n0.986\n0.076\n28\n12.904\n0.000\n\n\nalways - (rarely/never)\nCollege\n0.477\n0.080\n28\n5.960\n0.000\n\n\nsporadic - (rarely/never)\nHigh school or less\n0.187\n0.069\n28\n2.705\n0.031\n\n\nalways - (rarely/never)\nHigh school or less\n-0.711\n0.080\n28\n-8.883\n0.000\n\n\nsporadic - (rarely/never)\nSome college\n0.707\n0.074\n28\n9.512\n0.000\n\n\nalways - (rarely/never)\nSome college\n0.167\n0.079\n28\n2.114\n0.112\n\n\n\n\n\n\nNext, plot the predicted probabilities of voter category as a function of Age and Party ID\n\n\n\nCode\nlibrary(ggeffects)\nlibrary(ggplot2)\n\n# Get predicted probabilities for age and party identification\ndf_preds &lt;- ggemmeans(model_with_pol, terms = c(\"ppage_c\", \"pol_ident_new [all]\"))\n\nggplot(df_preds, aes(x = x, y = predicted, fill = response.level)) +\n  geom_area() +\n  geom_rug(sides = \"b\", position = \"jitter\", alpha = 0.5) +\n  labs(\n    x = \"Centered Age\",\n    y = \"Predicted Probability\",\n    title = \"Predicted Probabilities of Voting Frequency\\nby Age and Party Identification\",\n    fill = \"Voter Category\"\n  ) +\n  scale_fill_manual(\n    values = c(\n      \"always\" = \"#F6B533\", \n      \"sporadic\" = \"#D07EA2\", \n      \"rarely/never\" = \"#9854F7\"\n    ),\n    labels = c(\"ALMOST ALWAYS VOTE\", \"SOMETIMES VOTE\", \"RARELY OR NEVER VOTE\")\n  ) +\n  facet_wrap(~group) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nPlot predicted probabilities as a function of education and voting frequency.\n\n\nCode\nggemmeans(model_with_pol, terms = \"educ\") %&gt;%\n  ggplot(aes(x = x, y = predicted, fill = response.level)) +\n  geom_bar(stat = \"identity\", position = \"fill\") +\n  labs(x = \"Education\", y = \"Predicted Probability\", \n       title = \"Predicted Probabilities of Voting Frequency by Education\") +\n  scale_fill_manual(\n    name = \"Voter Category\",\n    values = c(\"always\" = \"#F6B533\", \"sporadic\" = \"#D07EA2\", \"rarely/never\" = \"#9854F7\"),\n    labels = c(\"ALMOST ALWAYS VOTE\", \"SOMETIMES VOTE\", \"RARELY OR NEVER VOTE\")\n  ) +\n    coord_flip() +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n### Write-up\n\nDifferences between political groups and voting behavior - Emmeans\n\n\nCode\nmultinomial_analysis &lt;- emmeans(model_with_pol, ~ pol_ident_new | voter_category)\n\ncoefs &lt;- contrast(regrid(multinomial_analysis, \"log\"), \"trt.vs.ctrl1\", by = \"pol_ident_new\")\n\nupdate(coefs, by = \"contrast\") %&gt;% \n  kable(format = \"markdown\", digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontrast\npol_ident_new\nestimate\nSE\ndf\nt.ratio\np.value\n\n\n\n\nsporadic - (rarely/never)\nDem\n0.961\n0.070\n28\n13.722\n0.000\n\n\nalways - (rarely/never)\nDem\n0.480\n0.074\n28\n6.498\n0.000\n\n\nsporadic - (rarely/never)\nIndep\n0.591\n0.077\n28\n7.643\n0.000\n\n\nalways - (rarely/never)\nIndep\n-0.049\n0.084\n28\n-0.590\n0.900\n\n\nsporadic - (rarely/never)\nOther\n0.078\n0.087\n28\n0.902\n0.747\n\n\nalways - (rarely/never)\nOther\n-0.835\n0.110\n28\n-7.577\n0.000\n\n\nsporadic - (rarely/never)\nRep\n0.883\n0.084\n28\n10.469\n0.000\n\n\nalways - (rarely/never)\nRep\n0.327\n0.089\n28\n3.672\n0.004\n\n\n\n\n\nCode\n# Pairwise comparisons (reverse pairwise contrasts)\ncontrast(coefs, \"revpairwise\", by = \"contrast\") %&gt;%\n  kable(format = \"markdown\", digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontrast1\ncontrast\nestimate\nSE\ndf\nt.ratio\np.value\n\n\n\n\nIndep - Dem\nsporadic - (rarely/never)\n-0.370\n0.094\n28\n-3.933\n0.003\n\n\nOther - Dem\nsporadic - (rarely/never)\n-0.883\n0.103\n28\n-8.578\n0.000\n\n\nOther - Indep\nsporadic - (rarely/never)\n-0.513\n0.107\n28\n-4.807\n0.000\n\n\nRep - Dem\nsporadic - (rarely/never)\n-0.078\n0.099\n28\n-0.787\n0.860\n\n\nRep - Indep\nsporadic - (rarely/never)\n0.292\n0.099\n28\n2.965\n0.029\n\n\nRep - Other\nsporadic - (rarely/never)\n0.805\n0.109\n28\n7.404\n0.000\n\n\nIndep - Dem\nalways - (rarely/never)\n-0.529\n0.101\n28\n-5.255\n0.000\n\n\nOther - Dem\nalways - (rarely/never)\n-1.315\n0.125\n28\n-10.508\n0.000\n\n\nOther - Indep\nalways - (rarely/never)\n-0.786\n0.129\n28\n-6.072\n0.000\n\n\nRep - Dem\nalways - (rarely/never)\n-0.153\n0.104\n28\n-1.470\n0.468\n\n\nRep - Indep\nalways - (rarely/never)\n0.376\n0.104\n28\n3.605\n0.006\n\n\nRep - Other\nalways - (rarely/never)\n1.162\n0.130\n28\n8.969\n0.000\n\n\n\n\n\nCode\n#This analysis shows that the differences in predicted probabilities across political groups are statistically significant. For instance, respondents with a Democrat or Independent affiliation may have higher odds of being consistent voters compared to those with Republican or Other affiliations. These differences are confirmed by significant pairwise contrasts.\n\n\n\n\nDifferences between education level and voting behavior - Emmeans\nLast part of the assignment: Interpret the results from running the following code for your model\n\n\nCode\nmultinomial_analysis &lt;- emmeans(model_with_pol, ~ educ | voter_category)\n\ncoefs &lt;- contrast(regrid(multinomial_analysis, \"log\"), \"trt.vs.ctrl1\", by = \"educ\")\n\nupdate(coefs, by = \"contrast\") %&gt;% \n  kable(format = \"markdown\", digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontrast\neduc\nestimate\nSE\ndf\nt.ratio\np.value\n\n\n\n\nsporadic - (rarely/never)\nCollege\n0.986\n0.076\n28\n12.904\n0.000\n\n\nalways - (rarely/never)\nCollege\n0.477\n0.080\n28\n5.960\n0.000\n\n\nsporadic - (rarely/never)\nHigh school or less\n0.187\n0.069\n28\n2.705\n0.031\n\n\nalways - (rarely/never)\nHigh school or less\n-0.711\n0.080\n28\n-8.883\n0.000\n\n\nsporadic - (rarely/never)\nSome college\n0.707\n0.074\n28\n9.512\n0.000\n\n\nalways - (rarely/never)\nSome college\n0.167\n0.079\n28\n2.114\n0.112\n\n\n\n\n\nCode\ncontrast(coefs, \"revpairwise\", by = \"contrast\") %&gt;%\n  kable(format = \"markdown\", digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontrast1\ncontrast\nestimate\nSE\ndf\nt.ratio\np.value\n\n\n\n\nHigh school or less - College\nsporadic - (rarely/never)\n-0.799\n0.095\n28\n-8.416\n0.000\n\n\nSome college - College\nsporadic - (rarely/never)\n-0.278\n0.092\n28\n-3.030\n0.014\n\n\nSome college - High school or less\nsporadic - (rarely/never)\n0.520\n0.088\n28\n5.920\n0.000\n\n\nHigh school or less - College\nalways - (rarely/never)\n-1.188\n0.104\n28\n-11.394\n0.000\n\n\nSome college - College\nalways - (rarely/never)\n-0.310\n0.097\n28\n-3.207\n0.009\n\n\nSome college - High school or less\nalways - (rarely/never)\n0.878\n0.098\n28\n8.995\n0.000\n\n\n\n\n\nEnter your interpretation here: The emmeans analysis for education level indicates significant differences in voting behavior across education groups. In this model, higher educational attainment is associated with higher predicted probabilities of being a consistent voter (“always”), while lower education levels are linked to increased probabilities of being sporadic or rarely/never voters. And these differences are statistically significant. The pairwise comparisons further confirm that adjacent education levels differ meaningfully in their impact on voting frequency, suggesting that educational attainment is an important predictor of voting behavior."
  },
  {
    "objectID": "assignments/poisson_regression.html",
    "href": "assignments/poisson_regression.html",
    "title": "Poisson Regression",
    "section": "",
    "text": "To complete this lab:\nCode\nlibrary(MASS)\nlibrary(tidyverse)\nlibrary(emmeans)\nlibrary(ggeffects)\nlibrary(easystats)\nlibrary(performance)\nlibrary(knitr)\nCode\nlibrary(tidyverse)\n\ndata &lt;- read_delim(\"https://raw.githubusercontent.com/jgeller112/psy504-advanced-stats/main/slides/Poisson/data/2010.csv\")\nCode\nlibrary(naniar)\n\ndata_pos &lt;- data %&gt;%\n  dplyr::select(wwwhr, wordsum, age, sex, reliten, polviews, wrkhome) %&gt;%\nreplace_with_na(.,\n             replace = list(wwwhr = c(-1, 998, 999),\n                          wordsum = c(-1, 99),\n                          reliten = c(0, 8, 9), \n             polviews = c(0, 8, 9), \n             wrkhome = c(0,8,9), \n             age=c(0, 98, 99)))\nQ: Can you explain what might be going on in the above code?\nA: The code first selects the variables of interest (wwwhr, wordsum, age, sex, reliten, polviews, and wrkhome) from the dataset. Then, using the replace_with_na function from the naniar package, it recodes certain placeholder values (e.g., -1, 998, 999 for wwwhr) as NA. These values likely represent missing, invalid, or out-of-range responses in the original data.\nQ: The next step in data cleaning would be to ensure that the data in your code are aligned with the description/ usage context of the variables\nCode\ndata_pos &lt;- data_pos |&gt; \n  mutate(sex = factor(ifelse(sex == -1, \"Male\", \n                             ifelse(sex == 1, \"Female\", NA)), \n                      levels = c(\"Male\", \"Female\")),\n         reliten_recode = factor(reliten, levels = 1:5))"
  },
  {
    "objectID": "assignments/poisson_regression.html#missingness",
    "href": "assignments/poisson_regression.html#missingness",
    "title": "Poisson Regression",
    "section": "Missingness",
    "text": "Missingness\n\n\nCode\ndata_pos %&gt;%\n  dplyr::select(reliten, reliten_recode)\n\n\n# A tibble: 2,044 × 2\n   reliten reliten_recode\n     &lt;dbl&gt; &lt;fct&gt;         \n 1       1 1             \n 2       4 4             \n 3       1 1             \n 4       1 1             \n 5       1 1             \n 6       4 4             \n 7       3 3             \n 8       1 1             \n 9       1 1             \n10       1 1             \n# ℹ 2,034 more rows\n\n\nCode\nlibrary(skimr)\nskimr::skim(data_pos)\n\n\n\nData summary\n\n\nName\ndata_pos\n\n\nNumber of rows\n2044\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nsex\n0\n1.00\nFALSE\n2\nMal: 1153, Fem: 891\n\n\nreliten_recode\n99\n0.95\nFALSE\n4\n2: 747, 1: 707, 4: 363, 3: 128\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nwwwhr\n996\n0.51\n9.79\n13.41\n0\n2\n5\n14\n168\n▇▁▁▁▁\n\n\nwordsum\n657\n0.68\n6.03\n2.07\n0\n5\n6\n7\n10\n▁▃▇▅▂\n\n\nage\n3\n1.00\n47.97\n17.68\n18\n33\n47\n61\n89\n▇▇▇▅▃\n\n\nreliten\n99\n0.95\n2.08\n1.08\n1\n1\n2\n3\n4\n▇▇▁▂▃\n\n\npolviews\n71\n0.97\n4.08\n1.46\n1\n3\n4\n5\n7\n▃▂▇▃▅\n\n\nwrkhome\n882\n0.57\n2.26\n1.72\n1\n1\n1\n4\n6\n▇▁▁▂▁"
  },
  {
    "objectID": "assignments/poisson_regression.html#fit-a-poisson-model-to-the-data.",
    "href": "assignments/poisson_regression.html#fit-a-poisson-model-to-the-data.",
    "title": "Poisson Regression",
    "section": "Fit a Poisson model to the data.",
    "text": "Fit a Poisson model to the data.\n\n\nCode\npoisson_model &lt;- glm(wwwhr ~ wordsum + age + sex + reliten + polviews + wrkhome,\n                    data = data_pos,\n                    family = poisson(link = \"log\"))\n\nsummary(poisson_model)\n\n\n\nCall:\nglm(formula = wwwhr ~ wordsum + age + sex + reliten + polviews + \n    wrkhome, family = poisson(link = \"log\"), data = data_pos)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  1.649224   0.081544  20.225  &lt; 2e-16 ***\nwordsum      0.099524   0.007754  12.836  &lt; 2e-16 ***\nage         -0.016456   0.001087 -15.132  &lt; 2e-16 ***\nsexFemale    0.259312   0.026363   9.836  &lt; 2e-16 ***\nreliten      0.198934   0.011894  16.726  &lt; 2e-16 ***\npolviews    -0.035508   0.009687  -3.666 0.000247 ***\nwrkhome      0.078441   0.007658  10.243  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 7666.1  on 602  degrees of freedom\nResidual deviance: 6481.8  on 596  degrees of freedom\n  (1441 observations deleted due to missingness)\nAIC: 8540\n\nNumber of Fisher Scoring iterations: 5"
  },
  {
    "objectID": "assignments/poisson_regression.html#carry-out-model-checking",
    "href": "assignments/poisson_regression.html#carry-out-model-checking",
    "title": "Poisson Regression",
    "section": "Carry out model checking",
    "text": "Carry out model checking\nHint: performance package has the function you’re looking for\n\n\nCode\npoisson_model &lt;- glm(wwwhr ~ wordsum + age + sex + reliten + polviews + wrkhome,\n                    data = data_pos,\n                    family = poisson(link = \"log\"))\n\n# Check overall model performance\ncheck_model(poisson_model)\n\n\n\n\n\n\n\n\n\nCode\n# Check for specific issues\nmodel_performance(poisson_model)\n\n\n# Indices of model performance\n\nAIC      |     AICc |      BIC | Nagelkerke's R2 |   RMSE | Sigma | Score_log | Score_spherical\n-----------------------------------------------------------------------------------------------\n8540.043 | 8540.231 | 8570.856 |           0.860 | 13.251 | 1.000 |    -7.070 |           0.027"
  },
  {
    "objectID": "assignments/poisson_regression.html#find-any-outliers",
    "href": "assignments/poisson_regression.html#find-any-outliers",
    "title": "Poisson Regression",
    "section": "Find any outliers",
    "text": "Find any outliers\n\n\nCode\n# Identify outliers using Cook's distance\ncooksd &lt;- cooks.distance(poisson_model)\nplot(cooksd, pch = 20, main = \"Cook's Distance\")\n# Define a threshold (e.g., 4/n)\nthreshold &lt;- 4 / nrow(data_pos)\nabline(h = threshold, col = \"red\", lty = 2)\n\n\n\n\n\n\n\n\n\nCode\noutlier_indices &lt;- which(cooksd &gt; threshold)\ncat(\"Number of potential outliers (Cook's distance):\", length(outlier_indices), \"\\n\")\n\n\nNumber of potential outliers (Cook's distance): 423 \n\n\nCode\nstudent_resid &lt;- rstudent(poisson_model)\nplot(student_resid, pch = 20, main = \"Studentized Residuals\")\nabline(h = c(-3, 3), col = \"red\", lty = 2)\n\n\n\n\n\n\n\n\n\nCode\noutlier_indices2 &lt;- which(abs(student_resid) &gt; 3)\ncat(\"Number of potential outliers (Studentized residuals):\", length(outlier_indices2), \"\\n\")\n\n\nNumber of potential outliers (Studentized residuals): 206 \n\n\nCode\nall_outlier_indices &lt;- unique(c(outlier_indices, outlier_indices2))\ncat(\"Total number of outliers identified:\", length(all_outlier_indices), \"\\n\")\n\n\nTotal number of outliers identified: 423"
  },
  {
    "objectID": "assignments/poisson_regression.html#refit-the-model-after-excludint-outliers",
    "href": "assignments/poisson_regression.html#refit-the-model-after-excludint-outliers",
    "title": "Poisson Regression",
    "section": "Refit the model after excludint outliers",
    "text": "Refit the model after excludint outliers\n\n\nCode\ndata_no_outliers &lt;- data_pos[-all_outlier_indices, ]\n\npoisson_model_refit &lt;- glm(wwwhr ~ wordsum + age + sex + reliten + polviews + wrkhome,\n                           data = data_no_outliers,\n                           family = poisson(link = \"log\"))\nsummary(poisson_model_refit)\n\n\n\nCall:\nglm(formula = wwwhr ~ wordsum + age + sex + reliten + polviews + \n    wrkhome, family = poisson(link = \"log\"), data = data_no_outliers)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  1.728885   0.093087  18.573  &lt; 2e-16 ***\nwordsum      0.100971   0.008809  11.462  &lt; 2e-16 ***\nage         -0.014702   0.001235 -11.905  &lt; 2e-16 ***\nsexFemale    0.288499   0.029974   9.625  &lt; 2e-16 ***\nreliten      0.181474   0.013607  13.337  &lt; 2e-16 ***\npolviews    -0.049475   0.010899  -4.539 5.64e-06 ***\nwrkhome      0.052020   0.008618   6.036 1.58e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 5798.6  on 472  degrees of freedom\nResidual deviance: 4979.0  on 466  degrees of freedom\n  (1148 observations deleted due to missingness)\nAIC: 6595.4\n\nNumber of Fisher Scoring iterations: 5\n\n\nCode\nmodel_parameters(poisson_model_refit) %&gt;%\n  print_html()\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nz\np\n\n\n\n\n(Intercept)\n1.73\n0.09\n(1.55, 1.91)\n18.57\n&lt; .001\n\n\nwordsum\n0.10\n8.81e-03\n(0.08, 0.12)\n11.46\n&lt; .001\n\n\nage\n-0.01\n1.23e-03\n(-0.02, -0.01)\n-11.91\n&lt; .001\n\n\nsex (Female)\n0.29\n0.03\n(0.23, 0.35)\n9.63\n&lt; .001\n\n\nreliten\n0.18\n0.01\n(0.15, 0.21)\n13.34\n&lt; .001\n\n\npolviews\n-0.05\n0.01\n(-0.07, -0.03)\n-4.54\n&lt; .001\n\n\nwrkhome\n0.05\n8.62e-03\n(0.04, 0.07)\n6.04\n&lt; .001\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck for Overdispersion\nHint: performance package has the function you’re looking for\n\n\nCode\noverdispersion_result &lt;- check_overdispersion(poisson_model_refit)\nprint(overdispersion_result)\n\n\n# Overdispersion test\n\n       dispersion ratio =   14.254\n  Pearson's Chi-Squared = 6642.165\n                p-value =  &lt; 0.001\n\n\nWhat do you notice? And what’s a good next step forward? Can there be another model class that can fit the data? If so, fit this model to the data.\nThe dispersion ratio is 14.254 with a p-value &lt; 0.001. This indicates that the variance is far larger than the mean, a clear sign of overdispersion in the Poisson model. Because the Poisson model assumes that the variance equals the mean, this overdispersion violates that assumption and suggests that the Poisson model isn’t the best fit for the data.\nA good next step is to try an alternative model that can handle overdispersion. I’ll fit a negative binomial model as it includes an extra parameter to account for the extra variance.\n\n\nCode\nnegbin_model &lt;- glm.nb(wwwhr ~ wordsum + age + sex + reliten + polviews + wrkhome,\n                       data = data_no_outliers)\nsummary(negbin_model)\n\n\n\nCall:\nglm.nb(formula = wwwhr ~ wordsum + age + sex + reliten + polviews + \n    wrkhome, data = data_no_outliers, init.theta = 0.9603288526, \n    link = log)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  1.714583   0.309241   5.544 2.95e-08 ***\nwordsum      0.113863   0.029625   3.844 0.000121 ***\nage         -0.014388   0.004061  -3.543 0.000396 ***\nsexFemale    0.188969   0.101297   1.865 0.062111 .  \nreliten      0.180424   0.047557   3.794 0.000148 ***\npolviews    -0.045450   0.037305  -1.218 0.223101    \nwrkhome      0.031508   0.030104   1.047 0.295262    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(0.9603) family taken to be 1)\n\n    Null deviance: 597.14  on 472  degrees of freedom\nResidual deviance: 530.03  on 466  degrees of freedom\n  (1148 observations deleted due to missingness)\nAIC: 3101.3\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  0.9603 \n          Std. Err.:  0.0673 \n\n 2 x log-likelihood:  -3085.2970"
  },
  {
    "objectID": "assignments/poisson_regression.html#which-one-is-better--your-earlier-model-or-later-model",
    "href": "assignments/poisson_regression.html#which-one-is-better--your-earlier-model-or-later-model",
    "title": "Poisson Regression",
    "section": "Which one is better- your earlier model, or later model?",
    "text": "Which one is better- your earlier model, or later model?\n\n\nCode\nAIC_comparison &lt;- AIC(poisson_model_refit, negbin_model)\nprint(AIC_comparison)\n\n\n                    df      AIC\npoisson_model_refit  7 6595.358\nnegbin_model         8 3101.297\n\n\nCode\n#Based on the AIC results, the negative binomial model (AIC = 3101.297) is the better fit compared to the poisson model (AIC = 6595.358)."
  },
  {
    "objectID": "assignments/poisson_regression.html#what-is-zero-inflation-is-there-zero-inflation-in-your-chosen-model",
    "href": "assignments/poisson_regression.html#what-is-zero-inflation-is-there-zero-inflation-in-your-chosen-model",
    "title": "Poisson Regression",
    "section": "What is zero inflation? Is there zero-inflation in your chosen model?",
    "text": "What is zero inflation? Is there zero-inflation in your chosen model?\n\n\nCode\n# Zero inflation occurs when there are more zero counts than expected under the assumed distribution.\nlibrary(performance)\n\nperformance::check_zeroinflation(negbin_model)\n\n\n# Check for zero-inflation\n\n   Observed zeros: 35\n  Predicted zeros: 52\n            Ratio: 1.50\n\n\n\nLog LambdaMean Count\n\n\n\n\nCode\nlog_lambda &lt;- ggeffect(negbin_model, terms = \"wordsum\")\nplot(log_lambda) +\n  labs(title = \"Predicted Log Lambda vs. WORDSUM\",\n       x = \"WORDSUM\",\n       y = \"Log Lambda\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nmean_count &lt;- ggeffect(negbin_model, terms = \"wordsum\")\nplot(mean_count) +\n  labs(title = \"Predicted Mean Count (WWWHR) vs. WORDSUM\",\n       x = \"WORDSUM\",\n       y = \"Predicted WWWHR\")"
  },
  {
    "objectID": "assignments/poisson_regression.html#report-your-conclusions",
    "href": "assignments/poisson_regression.html#report-your-conclusions",
    "title": "Poisson Regression",
    "section": "Report your conclusions",
    "text": "Report your conclusions\nIn this analysis, we started by cleaning the data and recoding the key variables according to our preregistration. After handling missing values and identifying outliers, we fit a poisson regression model to predict the number of hours per week spent on the Internet (WWWHR). Diagnostic checks revealed significant overdispersion (dispersion ratio &gt; 14, p &lt; 0.001). We then tried to fit a negative binomial model.\nModel comparisons showed that the negative binomial model (AIC ≈ 3101.3) substantially outperformed the poisson model (AIC ≈ 6595.4). In addition, the predicted values from the negative binomial model—visualized through both the log lambda and the expected mean count—illustrate a clear relationship between verbal ability (WORDSUM) and internet usage. The zero-inflation check using performance::check_zeroinflation(negbin_model) indicated no evidence of problematic excess zeros.\nOverall, the negative binomial model provides a better fit to the data by accommodating the observed overdispersion. This model supports our preregistered hypothesis that factors such as vocabulary, age, sex, religiosity, political orientation, and work-from-home frequency significantly predict weekly internet hours. Future research might explore further refinements or alternative models if additional complexities (e.g.potential zero-inflation) are detected."
  },
  {
    "objectID": "assignments/multilevel_modeling_walkthrough/multilevel_modeling_walkthrough.html",
    "href": "assignments/multilevel_modeling_walkthrough/multilevel_modeling_walkthrough.html",
    "title": "Multilevel Modeling Walkthrough",
    "section": "",
    "text": "New Packages!\n\n\n\n\n\nThese are the main packages we’re going to use in this block. It might make sense to install them now if you do not have them already\n\ntidyverse : for organising data\n\nlme4 : for fitting generalised linear mixed effects models\nbroom.mixed : tidying methods for mixed models\neffects : for tabulating and graphing effects in linear models\nlmerTest: for quick p-values from mixed models\nparameters: various inferential methods for mixed models"
  },
  {
    "objectID": "assignments/multilevel_modeling_walkthrough/multilevel_modeling_walkthrough.html#footnotes",
    "href": "assignments/multilevel_modeling_walkthrough/multilevel_modeling_walkthrough.html#footnotes",
    "title": "Multilevel Modeling Walkthrough",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nImage sources:http://tophatsasquatch.com/2012-tmnt-classics-action-figures/https://www.dezeen.com/2016/02/01/barbie-dolls-fashionista-collection-mattel-new-body-types/https://www.wish.com/product/5da9bc544ab36314cfa7f70chttps://www.worldwideshoppingmall.co.uk/toys/jumbo-farm-animals.asphttps://www.overstock.com/Sports-Toys/NJ-Croce-Scooby-Doo-5pc.-Bendable-Figure-Set-with-Scooby-Doo-Shaggy-Daphne-Velma-and-Fred/28534567/product.htmlhttps://tvtropes.org/pmwiki/pmwiki.php/Toys/Furbyhttps://www.fun.com/toy-story-4-figure-4-pack.htmlhttps://www.johnlewis.com/lego-minifigures-71027-series-20-pack/p5079461↩︎"
  },
  {
    "objectID": "assignments/Multilevel Modeling Walkthrough/multilevel_modeling_walkthrough.html",
    "href": "assignments/Multilevel Modeling Walkthrough/multilevel_modeling_walkthrough.html",
    "title": "Multilevel Modeling Walkthrough",
    "section": "",
    "text": "New Packages!\n\n\n\n\n\nThese are the main packages we’re going to use in this block. It might make sense to install them now if you do not have them already\n\ntidyverse : for organising data\n\nlme4 : for fitting generalised linear mixed effects models\nbroom.mixed : tidying methods for mixed models\neffects : for tabulating and graphing effects in linear models\nlmerTest: for quick p-values from mixed models\nparameters: various inferential methods for mixed models"
  },
  {
    "objectID": "assignments/Multilevel Modeling Walkthrough/multilevel_modeling_walkthrough.html#footnotes",
    "href": "assignments/Multilevel Modeling Walkthrough/multilevel_modeling_walkthrough.html#footnotes",
    "title": "Multilevel Modeling Walkthrough",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nImage sources:http://tophatsasquatch.com/2012-tmnt-classics-action-figures/https://www.dezeen.com/2016/02/01/barbie-dolls-fashionista-collection-mattel-new-body-types/https://www.wish.com/product/5da9bc544ab36314cfa7f70chttps://www.worldwideshoppingmall.co.uk/toys/jumbo-farm-animals.asphttps://www.overstock.com/Sports-Toys/NJ-Croce-Scooby-Doo-5pc.-Bendable-Figure-Set-with-Scooby-Doo-Shaggy-Daphne-Velma-and-Fred/28534567/product.htmlhttps://tvtropes.org/pmwiki/pmwiki.php/Toys/Furbyhttps://www.fun.com/toy-story-4-figure-4-pack.htmlhttps://www.johnlewis.com/lego-minifigures-71027-series-20-pack/p5079461↩︎"
  },
  {
    "objectID": "assignments/Multilevel Modeling Walkthrough/MLM_Intro_Questions.html",
    "href": "assignments/Multilevel Modeling Walkthrough/MLM_Intro_Questions.html",
    "title": "Intro to MLM Exercise/Walkthrough",
    "section": "",
    "text": "New Packages!\n\n\n\n\n\nThese are the main packages we’re going to use in this block. It might make sense to install them now if you do not have them already\n\ntidyverse : for organising data\n\nlme4 : for fitting generalised linear mixed effects models\nbroom.mixed : tidying methods for mixed models\neffects : for tabulating and graphing effects in linear models\nlmerTest: for quick p-values from mixed models\nparameters: various inferential methods for mixed models"
  },
  {
    "objectID": "assignments/Multilevel Modeling Walkthrough/MLM_Intro_Questions.html#footnotes",
    "href": "assignments/Multilevel Modeling Walkthrough/MLM_Intro_Questions.html#footnotes",
    "title": "Intro to MLM Exercise/Walkthrough",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nImage sources:http://tophatsasquatch.com/2012-tmnt-classics-action-figures/https://www.dezeen.com/2016/02/01/barbie-dolls-fashionista-collection-mattel-new-body-types/https://www.wish.com/product/5da9bc544ab36314cfa7f70chttps://www.worldwideshoppingmall.co.uk/toys/jumbo-farm-animals.asphttps://www.overstock.com/Sports-Toys/NJ-Croce-Scooby-Doo-5pc.-Bendable-Figure-Set-with-Scooby-Doo-Shaggy-Daphne-Velma-and-Fred/28534567/product.htmlhttps://tvtropes.org/pmwiki/pmwiki.php/Toys/Furbyhttps://www.fun.com/toy-story-4-figure-4-pack.htmlhttps://www.johnlewis.com/lego-minifigures-71027-series-20-pack/p5079461↩︎"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html",
    "href": "assignments/multilevel_modeling2.html",
    "title": "Multilevel Modeling 2",
    "section": "",
    "text": "When to use them:\n\nNested designs\nRepeated measures\nLongitudinal data\nComplex designs\n\nWhy use them:\n\nCaptures variance occurring between groups and within groups\n\nWhat they are:\n\nLinear model with extra residuals"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#multilevel-models",
    "href": "assignments/multilevel_modeling2.html#multilevel-models",
    "title": "Multilevel Modeling 2",
    "section": "",
    "text": "When to use them:\n\nNested designs\nRepeated measures\nLongitudinal data\nComplex designs\n\nWhy use them:\n\nCaptures variance occurring between groups and within groups\n\nWhat they are:\n\nLinear model with extra residuals"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#today",
    "href": "assignments/multilevel_modeling2.html#today",
    "title": "Multilevel Modeling 2",
    "section": "Today",
    "text": "Today\n\nEverything you need to know to run and report a MLM\n\nOrganizing data for MLM analysis\nEstimation\nFit and interpret multilevel models\nVisualization\nEffect size\nReporting\nPower"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#packages",
    "href": "assignments/multilevel_modeling2.html#packages",
    "title": "Multilevel Modeling 2",
    "section": "Packages",
    "text": "Packages\n\n\nCode\nlibrary(tidyverse) # data wrangling\nlibrary(knitr) # nice tables\nlibrary(lme4) # fit mixed models\nlibrary(lmerTest) # mixed models\nlibrary(broom.mixed) # tidy output of mixed models\nlibrary(afex) # fit mixed models for lrt test\nlibrary(emmeans) # marginal means\nlibrary(ggeffects) # marginal means\nlibrary(ggrain) # rain plots\nlibrary(easystats) # nice ecosystem of packages\n\noptions(scipen=999) # get rid of sci notation\n\n\n\nFind the .qmd document here to follow along: https://github.com/suyoghc/PSY-504_Spring-2025/blob/main/Multilevel%20Modeling/mlm-02.qmd"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#todays-data",
    "href": "assignments/multilevel_modeling2.html#todays-data",
    "title": "Multilevel Modeling 2",
    "section": "Today’s data",
    "text": "Today’s data\n\nWhat did you say?\n\nPs (N = 31) listened to both clear (NS) and 6 channel vocoded speech (V6)\n\n(https://www.mrc-cbu.cam.ac.uk/personal/matt.davis/vocode/a1_6.wav)\n\nFixed factor: ?\nRandom factor: ?"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#todays-data-1",
    "href": "assignments/multilevel_modeling2.html#todays-data-1",
    "title": "Multilevel Modeling 2",
    "section": "Today’s data",
    "text": "Today’s data\n\n\nCode\neye  &lt;- read_csv(\"https://raw.githubusercontent.com/suyoghc/PSY-504_Spring-2025/refs/heads/main/Multilevel%20Modeling/data/vocoded_pupil.csv\") # data for class"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#data-organization",
    "href": "assignments/multilevel_modeling2.html#data-organization",
    "title": "Multilevel Modeling 2",
    "section": "Data organization",
    "text": "Data organization\n\nData Structure\n\nMLM analysis (in R) requires data in long format"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#data-organization-1",
    "href": "assignments/multilevel_modeling2.html#data-organization-1",
    "title": "Multilevel Modeling 2",
    "section": "Data organization",
    "text": "Data organization\n\nLevel 1: trial\nLevel 2: subject\n\n\n\n\n\n\nsubject\ntrial\nvocoded\nmean_pupil\n\n\n\n\nEYE15\n3\nV6\n0.0839555\n\n\nEYE15\n4\nV6\n0.0141083\n\n\nEYE15\n5\nV6\n0.0224967\n\n\nEYE15\n6\nV6\n0.0007424\n\n\nEYE15\n7\nV6\n0.0242540\n\n\nEYE15\n8\nV6\n0.0267617"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#centering",
    "href": "assignments/multilevel_modeling2.html#centering",
    "title": "Multilevel Modeling 2",
    "section": "Centering",
    "text": "Centering\n\n\n\nIn a single-level regression, centering ensures that the zero value for each predictor is meaningful before running the model\nIn MLM, if you have specific questions about within, between, and contextual effects, you need to center!"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#group--vs.-grand-mean-centering",
    "href": "assignments/multilevel_modeling2.html#group--vs.-grand-mean-centering",
    "title": "Multilevel Modeling 2",
    "section": "Group- vs. Grand-Mean Centering",
    "text": "Group- vs. Grand-Mean Centering\n\nGrand-mean centering: \\(x_{ij} - x\\)\n\nVariable represents each observation’s deviation from everyone’s norm, regardless of group\n\nGroup-mean centering: \\(x_{ij} - x_j\\)\n\nVariable represents each observation’s deviation from their group’s norm (removes group effect)"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#group--vs.-grand-mean-centering-1",
    "href": "assignments/multilevel_modeling2.html#group--vs.-grand-mean-centering-1",
    "title": "Multilevel Modeling 2",
    "section": "Group- vs. Grand-Mean Centering",
    "text": "Group- vs. Grand-Mean Centering\n\n\n\nLevel 1 predictors\n\nGrand-mean centering\n\nInclude means of level 2\n\nAllows us to directly test within-group effect\nCoefficient associated with the Level 2 group mean represents contextual effect\n\n\n\n\n\n\nGroup-mean centering\n\nLevel 1 coefficient will always be with within-group effect, regardless of whether the group means are included at Level 2 or not\nIf level 2 means included, coefficient represents the between-groups effect\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nCan apply to categorical predictors as well (see Yaremych, Preacher, & Hedeker, 2023)"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#centering-in-r",
    "href": "assignments/multilevel_modeling2.html#centering-in-r",
    "title": "Multilevel Modeling 2",
    "section": "Centering in R",
    "text": "Centering in R"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#maximum-likelihood",
    "href": "assignments/multilevel_modeling2.html#maximum-likelihood",
    "title": "Multilevel Modeling 2",
    "section": "Maximum Likelihood",
    "text": "Maximum Likelihood\n\n\n\n\nIn MLM we try to maximize the likelihood of the data\n\nNo OLS!"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#probability-vs.-likelihood",
    "href": "assignments/multilevel_modeling2.html#probability-vs.-likelihood",
    "title": "Multilevel Modeling 2",
    "section": "Probability vs. Likelihood",
    "text": "Probability vs. Likelihood\n\nProbability\n\n\nIf I assume a distribution with certain parameters (fixed), what is the probability I see a particular value in the data?\n\n\n\n\nPr⁡(𝑦&gt;0│𝜇=0,𝜎=1)=.50\nPr⁡(−1&lt;𝑦&lt;1│𝜇=0,𝜎=1)=.68\nPr⁡(0&lt;𝑦&lt;1│𝜇=0,𝜎=1)=.34\nPr⁡(𝑦&gt;2│𝜇=0,𝜎=1)=.02"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#likelihood",
    "href": "assignments/multilevel_modeling2.html#likelihood",
    "title": "Multilevel Modeling 2",
    "section": "Likelihood",
    "text": "Likelihood\n\n\n\n\\(L(𝜇,𝜎│𝑥)\\)\nHolding a sample of data constant, which parameter values are more likely?\n\nWhich values have higher likelihood?\n\nHere data is fixed and distribution can change"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#likelihood-1",
    "href": "assignments/multilevel_modeling2.html#likelihood-1",
    "title": "Multilevel Modeling 2",
    "section": "Likelihood",
    "text": "Likelihood\nInteractive: Understanding Maximum Likelihood Estimation: https://rpsychologist.com/likelihood/"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#likelihood-2",
    "href": "assignments/multilevel_modeling2.html#likelihood-2",
    "title": "Multilevel Modeling2",
    "section": "Likelihood",
    "text": "Likelihood"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#likelihood-3",
    "href": "assignments/multilevel_modeling2.html#likelihood-3",
    "title": "Multilevel Modeling2",
    "section": "Likelihood",
    "text": "Likelihood"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#likelihood-4",
    "href": "assignments/multilevel_modeling2.html#likelihood-4",
    "title": "Multilevel Modeling2",
    "section": "Likelihood",
    "text": "Likelihood"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#likelihood-5",
    "href": "assignments/multilevel_modeling2.html#likelihood-5",
    "title": "Multilevel Modeling2",
    "section": "Likelihood",
    "text": "Likelihood"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#likelihood-6",
    "href": "assignments/multilevel_modeling2.html#likelihood-6",
    "title": "Multilevel Modeling2",
    "section": "Likelihood",
    "text": "Likelihood\nInteractive: Understanding Maximum Likelihood Estimation: https://rpsychologist.com/likelihood/"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#log-likelihood",
    "href": "assignments/multilevel_modeling2.html#log-likelihood",
    "title": "Multilevel Modeling 2",
    "section": "Log likelihood",
    "text": "Log likelihood\n\nWith large samples, likelihood values ℒ(𝜇,𝜎│𝑥) get very small very fast\n\nTo make them easier to work with, we usually work with the log-likelihood\n\nMeasure of how well the model fits the data\nHigher values of \\(\\log L\\) are better\n\n\nDeviance = \\(-2logL\\)\n\n\\(-2logL\\) follows a \\(\\chi^2\\) distribution with \\(n (\\text{sample size}) - p (\\text{paramters}) - 1\\) degrees of freedom"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#chi2-distribution",
    "href": "assignments/multilevel_modeling2.html#chi2-distribution",
    "title": "Multilevel Modeling 2",
    "section": "\\(\\chi^2\\) distribution",
    "text": "\\(\\chi^2\\) distribution"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#comparing-nested-models",
    "href": "assignments/multilevel_modeling2.html#comparing-nested-models",
    "title": "Multilevel Modeling 2",
    "section": "Comparing nested models",
    "text": "Comparing nested models\n\nSuppose there are two models:\n\nReduced model includes predictors \\(x_1, \\ldots, x_q\\)\nFull model includes predictors \\(x_1, \\ldots, x_q, x_{q+1}, \\ldots, x_p\\)\n\nWe want to test the hypotheses:\n\n\\(H_0\\): smaller model is better\n\\(H_1\\): Larger model is better\n\nTo do so, we will use the drop-in-deviance test (also known as the nested likelihood ratio test)"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#drop-in-deviance-test",
    "href": "assignments/multilevel_modeling2.html#drop-in-deviance-test",
    "title": "Multilevel Modeling 2",
    "section": "Drop-In-Deviance Test",
    "text": "Drop-In-Deviance Test\n\nHypotheses:\n\n\\(H_0\\): smaller model is better\n\\(H_1\\): Larger model is better\n\nTest Statistic: \\[G = (-2 \\log L_{reduced}) - (-2 \\log L_{full})\\]\nP-value: \\(P(\\chi^2 &gt; G)\\):\n\nCalculated using a \\(\\chi^2\\) distribution\ndf = \\(df_1\\) - \\(df_2\\)"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#testing-deviance",
    "href": "assignments/multilevel_modeling2.html#testing-deviance",
    "title": "Multilevel Modeling 2",
    "section": "Testing deviance",
    "text": "Testing deviance\n\nWe can use the anova function to conduct this test\n\nAdd test = “Chisq” to conduct the drop-in-deviance test\n\nI like test_likelihoodratio from easystats\n\n\n\nCode\n#anova(model1, model2, test=\"chisq\")\n\n# test using easystats function\n\n#test_likelihoodratio(model1, model2)"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#model-fitting-ml-or-reml",
    "href": "assignments/multilevel_modeling2.html#model-fitting-ml-or-reml",
    "title": "Multilevel Modeling 2",
    "section": "Model fitting: ML or REML?",
    "text": "Model fitting: ML or REML?\n\nTwo flavors of maximum likelihood\n\nMaximum Likelihood (ML or FIML)\n\nJointly estimate the fixed effects and variance components using all the sample data\nCan be used to draw conclusions about fixed and random effects\nIssue:\n\nResults are biased because fixed effects are estimated without error"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#model-fitting-ml-or-reml-1",
    "href": "assignments/multilevel_modeling2.html#model-fitting-ml-or-reml-1",
    "title": "Multilevel Modeling 2",
    "section": "Model fitting: ML or REML",
    "text": "Model fitting: ML or REML\n\nRestricted Maximum Likelihood (REML)\n\nEstimates the variance components using the sample residuals not the sample data\nIt is conditional on the fixed effects, so it accounts for uncertainty in fixed effects estimates\n\nThis results in unbiased estimates of variance components\nAssociated with error/penalty"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#model-fitting-ml-or-reml-2",
    "href": "assignments/multilevel_modeling2.html#model-fitting-ml-or-reml-2",
    "title": "Multilevel Modeling 2",
    "section": "Model fitting: ML or REML?",
    "text": "Model fitting: ML or REML?\n\nResearch has not determined one method absolutely superior to the other\nREML (REML = TRUE; default in lmer) is preferable when:\n\nThe number of parameters is large\nPrimary objective is to obtain relaible estimates of the variance parameters\nFor REML, likelihood ratio tests can only be used to draw conclusions about variance components\n\nML (REML = FALSE) must be used if you want to compare nested fixed effects models using a likelihood ratio test (e.g., a drop-in-deviance test)"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#ml-or-reml",
    "href": "assignments/multilevel_modeling2.html#ml-or-reml",
    "title": "Multilevel Modeling 2",
    "section": "ML or REML?",
    "text": "ML or REML?\n\nWhat would we use if we wanted to compare the below models?\n\n\n\nCode\n#x= lmer(DV ~ IV1 + IV2 + (1|ID))\n\n#y= lmer(DV ~ IV1*IV2 + (1|ID))"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#ml-or-reml-1",
    "href": "assignments/multilevel_modeling2.html#ml-or-reml-1",
    "title": "Multilevel Modeling 2",
    "section": "ML or REML?",
    "text": "ML or REML?\n\nWhat would we use if we wanted to compare the below models?\n\n\n\nCode\n#x = lmer(DV ~ IV1 + IV2 + (1+IV2|ID))\n\n#y = lmer(DV ~ IV1+ IV2 + (1|ID))"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#modeling-approach",
    "href": "assignments/multilevel_modeling2.html#modeling-approach",
    "title": "Multilevel Modeling 2",
    "section": "Modeling approach",
    "text": "Modeling approach\n\nForward/backward approach\n\n\n# Forward approach - Start with null model (unconditional means model)\nnull_model &lt;- lmer(math ~ (1|schcode), data = mlm_data, REML = TRUE)\nsummary(null_model)\n\n# Calculate ICC\nicc_val &lt;- 36.95/(36.95 + 39.26)\ncat(\"ICC =\", round(icc_val, 4), \"\\n\")\ncat(\"This means about\", round(icc_val*100, 2), \"% of variance is at the school level\\n\")\n\n# Forward approach - Add fixed effect (ses)\nmodel1 &lt;- lmer(math ~ ses + (1|schcode), data = mlm_data, REML = FALSE)\n\n# Forward approach - Add random slope\nmodel2 &lt;- lmer(math ~ ses + (1+ses|schcode), data = mlm_data, REML = FALSE)\n\n# Check for singular fit in model2\nsummary(model2)\n\n# If convergence issues or singular fit, try removing correlation\nmodel3 &lt;- lmer(math ~ ses + (1|schcode) + (0+ses|schcode), data = mlm_data, REML = FALSE)\n# Alternative syntax: lmer(math ~ ses + (1+ses||schcode), data = mlm_data)\n\n# Model comparison with LRT\nanova(model1, model3)\n\n# Backward approach - Start with maximal model (if it converges)\nmax_model &lt;- lmer(math ~ ses + minority + (1+ses+minority|schcode), \n                  data = mlm_data, REML = FALSE, control = lmerControl(optimizer = \"bobyqa\"))\n\n# Simplified model (backward step)\nreduced_model &lt;- lmer(math ~ ses + minority + (1+ses|schcode), \n                     data = mlm_data, REML = FALSE)\n\n# Compare models\nanova(reduced_model, max_model)\n\n# Final model with REML for better variance estimates\nfinal_model &lt;- lmer(math ~ ses + minority + (1+ses|schcode), \n                   data = mlm_data, REML = TRUE)\nsummary(final_model)\n\nKeep it maximal1\n\nWhatever can vary, should vary\n\nDecreases Type 1 error"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#modeling-approach-1",
    "href": "assignments/multilevel_modeling2.html#modeling-approach-1",
    "title": "Multilevel Modeling 2",
    "section": "Modeling approach",
    "text": "Modeling approach\n\nFull (maximal) model\n\nOnly when there is convergence issues should you remove terms\n\nif non-convergence (pay attention to warning messages in summary output!):\n\nTry different optimizer (afex::all_fit())\n\nSort out random effects\n\nRemove correlations between slopes and intercepts\nRandom slopes\nRandom Intercepts\n\nSort out fixed effects (e.g., interaction)\nOnce you arrive at the final model present it using REML estimation"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#modeling-approach-2",
    "href": "assignments/multilevel_modeling2.html#modeling-approach-2",
    "title": "Multilevel Modeling 2",
    "section": "Modeling approach",
    "text": "Modeling approach\n\nIf your model is singular (check output!!!!)\n\nVariance might be close to 0\nPerfect correlations (1 or -1)\n\nDrop the parameter!"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#modeling-approach-3",
    "href": "assignments/multilevel_modeling2.html#modeling-approach-3",
    "title": "Multilevel Modeling 2",
    "section": "Modeling approach",
    "text": "Modeling approach\n\n\nCode\ndata &lt;- read.csv(\"https://raw.githubusercontent.com/suyoghc/PSY-504_Spring-2025/refs/heads/main/Multilevel%20Modeling/data/heck2011.csv\")\n\nsummary(lmer(math~ses + (1+ses|schcode), data=data))\n\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: math ~ ses + (1 + ses | schcode)\n   Data: data\n\nREML criterion at convergence: 48190.1\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.8578 -0.5553  0.1290  0.6437  5.7098 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n schcode  (Intercept)  3.2042  1.7900        \n          ses          0.7794  0.8828   -1.00\n Residual             62.5855  7.9111        \nNumber of obs: 6871, groups:  schcode, 419\n\nFixed effects:\n             Estimate Std. Error        df t value            Pr(&gt;|t|)    \n(Intercept)   57.6959     0.1315  378.6378  438.78 &lt;0.0000000000000002 ***\nses            3.9602     0.1408 1450.7730   28.12 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n    (Intr)\nses -0.284\noptimizer (nloptwrap) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n\n. . .\n\n\nCode\nlmer(math~ses + (1+ses||schcode), data=data) # removes correlation() with double pipes. Does not work with categorical variables"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#null-model-unconditional-means",
    "href": "assignments/multilevel_modeling2.html#null-model-unconditional-means",
    "title": "Multilevel Modeling 2",
    "section": "Null model (unconditional means)",
    "text": "Null model (unconditional means)\nGet ICC\n\nICC is a standardized way of expressing how much variance is due to clustering/group\n\nRanges from 0-1\n\nCan also be interpreted as correlation among observations within cluster/group!\nIf ICC is sufficiently low (i.e., \\(\\rho\\) &lt; .1), then you don’t have to use MLM! BUT YOU PROBABLY SHOULD 🙂"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#null-model-unconditional-means-1",
    "href": "assignments/multilevel_modeling2.html#null-model-unconditional-means-1",
    "title": "Multilevel Modeling 2",
    "section": "Null model (unconditional means)",
    "text": "Null model (unconditional means)\n\n\nCode\nlibrary(lme4) # pop linear modeling package\n\nnull_model &lt;- lmer(mean_pupil ~ (1|subject), data = eye, REML=TRUE)\n\nsummary(null_model)\n\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: mean_pupil ~ (1 | subject)\n   Data: eye\n\nREML criterion at convergence: -19811.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.1411 -0.5530 -0.0463  0.4822 10.8130 \n\nRandom effects:\n Groups   Name        Variance  Std.Dev.\n subject  (Intercept) 0.0001303 0.01142 \n Residual             0.0016840 0.04104 \nNumber of obs: 5609, groups:  subject, 31\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(&gt;|t|)  \n(Intercept)  0.005227   0.002124 29.457784   2.461   0.0199 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#calculating-icc",
    "href": "assignments/multilevel_modeling2.html#calculating-icc",
    "title": "Multilevel Modeling 2",
    "section": "Calculating ICC",
    "text": "Calculating ICC\n\nRun baseline (null) model\nGet intercept variance and residual variance\n\n\\[\\mathrm{ICC}=\\frac{\\text { between-group variability }}{\\text { between-group variability+within-group variability}}\\]\n\\[\nICC=\\frac{\\operatorname{Var}\\left(u_{0 j}\\right)}{\\operatorname{Var}\\left(u_{0 j}\\right)+\\operatorname{Var}\\left(e_{i j}\\right)}=\\frac{\\tau_{00}}{\\tau_{00}+\\sigma^{2}}\n\\]\n\n\nCode\n# easystats \n#adjusted icc just random effects\n#unadjusted fixed effects taken into account\nperformance::icc(null_model)\n\n\n# Intraclass Correlation Coefficient\n\n    Adjusted ICC: 0.072\n  Unadjusted ICC: 0.072"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#maximal-model-fixed-effect-random-intercepts-subject-and-slopes-vocoded-model",
    "href": "assignments/multilevel_modeling2.html#maximal-model-fixed-effect-random-intercepts-subject-and-slopes-vocoded-model",
    "title": "Multilevel Modeling 2",
    "section": "Maximal model: Fixed effect random intercepts (subject) and slopes (vocoded) model",
    "text": "Maximal model: Fixed effect random intercepts (subject) and slopes (vocoded) model\n\n\nCode\nmax_model &lt;- lmer(mean_pupil ~vocoded +(1+vocoded|subject), data = eye)\n\nsummary(max_model)\n\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: mean_pupil ~ vocoded + (1 + vocoded | subject)\n   Data: eye\n\nREML criterion at convergence: -19813.7\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.0296 -0.5509 -0.0467  0.4810 10.7164 \n\nRandom effects:\n Groups   Name        Variance   Std.Dev. Corr \n subject  (Intercept) 0.00013592 0.011658      \n          vocodedV6   0.00002816 0.005307 -0.19\n Residual             0.00167497 0.040926      \nNumber of obs: 5609, groups:  subject, 31\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(&gt;|t|)  \n(Intercept)  0.003643   0.002235 28.852288    1.63   0.1140  \nvocodedV6    0.003124   0.001453 30.471988    2.15   0.0396 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n          (Intr)\nvocodedV6 -0.306"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#fixed-effects",
    "href": "assignments/multilevel_modeling2.html#fixed-effects",
    "title": "Multilevel Modeling 2",
    "section": "Fixed effects",
    "text": "Fixed effects\n\nInterpretation same as lm\n\n\n\nCode\n#grab the fixed effects\nsummary(max_model)\n\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: mean_pupil ~ vocoded + (1 + vocoded | subject)\n   Data: eye\n\nREML criterion at convergence: -19813.7\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.0296 -0.5509 -0.0467  0.4810 10.7164 \n\nRandom effects:\n Groups   Name        Variance   Std.Dev. Corr \n subject  (Intercept) 0.00013592 0.011658      \n          vocodedV6   0.00002816 0.005307 -0.19\n Residual             0.00167497 0.040926      \nNumber of obs: 5609, groups:  subject, 31\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(&gt;|t|)  \n(Intercept)  0.003643   0.002235 28.852288    1.63   0.1140  \nvocodedV6    0.003124   0.001453 30.471988    2.15   0.0396 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n          (Intr)\nvocodedV6 -0.306"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#degrees-of-freedom-and-p-values",
    "href": "assignments/multilevel_modeling2.html#degrees-of-freedom-and-p-values",
    "title": "Multilevel Modeling 2",
    "section": "Degrees of freedom and p-values",
    "text": "Degrees of freedom and p-values\n\nDegrees of freedom (denominator) and p-values can be assessed with several methods:\n\nSatterthwaite (default when install lmerTest and then run lmer)\nAsymptotic (Inf) (default behavior lme4)\nKenward-Rogers"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#random-effectsvariance-components",
    "href": "assignments/multilevel_modeling2.html#random-effectsvariance-components",
    "title": "Multilevel Modeling 2",
    "section": "Random effects/variance components",
    "text": "Random effects/variance components\n\nTells us how much variability there is around the fixed intercept/slope\n\nHow much does the average pupil size change between participants\n\n\n\n\nCode\nsummary(max_model)\n\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: mean_pupil ~ vocoded + (1 + vocoded | subject)\n   Data: eye\n\nREML criterion at convergence: -19813.7\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.0296 -0.5509 -0.0467  0.4810 10.7164 \n\nRandom effects:\n Groups   Name        Variance   Std.Dev. Corr \n subject  (Intercept) 0.00013592 0.011658      \n          vocodedV6   0.00002816 0.005307 -0.19\n Residual             0.00167497 0.040926      \nNumber of obs: 5609, groups:  subject, 31\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(&gt;|t|)  \n(Intercept)  0.003643   0.002235 28.852288    1.63   0.1140  \nvocodedV6    0.003124   0.001453 30.471988    2.15   0.0396 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n          (Intr)\nvocodedV6 -0.306"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#random-effectsvariance-components-1",
    "href": "assignments/multilevel_modeling2.html#random-effectsvariance-components-1",
    "title": "Multilevel Modeling 2",
    "section": "Random effects/variance components",
    "text": "Random effects/variance components\n\nCorrelation between random intercepts and slopes\n\nNegative correlation\n\nHigher intercept (for normal speech) less of effect (lower slope)\n\n\n\n\n\nParameter1 |  Parameter2 |     r |        95% CI | t(29) |     p\n----------------------------------------------------------------\nvocodedV6  | (Intercept) | -0.10 | [-0.44, 0.26] | -0.57 | 0.576\n\nObservations: 31"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#visualize-random-effects",
    "href": "assignments/multilevel_modeling2.html#visualize-random-effects",
    "title": "Multilevel Modeling 2",
    "section": "Visualize Random Effects",
    "text": "Visualize Random Effects\n\n\nCode\n# use easystats to grab group variance\nrandom &lt;- estimate_grouplevel(max_model)\n\nplot(random) + theme_lucid()"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#model-comparisons",
    "href": "assignments/multilevel_modeling2.html#model-comparisons",
    "title": "Multilevel Modeling 2",
    "section": "Model comparisons",
    "text": "Model comparisons\n\nCan compare models using anova function or test_likelihoodratio from easystats\n\nWill be refit using ML if interested in fixed effects\n\n\n\n\nCode\n# Model comparison using anova\n# First refit models with ML for fixed effects comparison\nnull_model_ml &lt;- lmer(mean_pupil ~ (1|subject), data = eye, REML = FALSE)\nmax_model_ml &lt;- lmer(mean_pupil ~ vocoded + (1+vocoded|subject), data = eye, REML = FALSE)\n\n# Compare models using anova\nanova(null_model_ml, max_model_ml)\n\n\nData: eye\nModels:\nnull_model_ml: mean_pupil ~ (1 | subject)\nmax_model_ml: mean_pupil ~ vocoded + (1 + vocoded | subject)\n              npar    AIC    BIC logLik -2*log(L)  Chisq Df Pr(&gt;Chisq)   \nnull_model_ml    3 -19816 -19796 9911.1    -19822                        \nmax_model_ml     6 -19823 -19784 9917.7    -19835 13.269  3    0.00409 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#aic",
    "href": "assignments/multilevel_modeling2.html#aic",
    "title": "Multilevel Modeling 2",
    "section": "AIC",
    "text": "AIC\n\nAIC:\n\n\\[\nD + 2p\n\\]\n\nwhere d = deviance and p = # of parameters in model\nCan compare AICs2:\n\\[\n\\Delta_i = AIC_{i} - AIC_{min}\n\\]\nLess than 2: More parsimonious model is preferred\nBetween 4 and 7: some evidence for lower AIC model\nGreater than 10,: strong evidence for lower AIC"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#bic",
    "href": "assignments/multilevel_modeling2.html#bic",
    "title": "Multilevel Modeling 2",
    "section": "BIC",
    "text": "BIC\n\nBIC:\n\n\\[\nD + ln(n)*p\n\\]\n\nwhere d = deviance, p = # of parameters in model, n = sample size\nChange in BIC:\n\n\\(\\Delta{BIC}\\) &lt;= 2 (No difference)\n\\(\\Delta{BIC}\\) &gt; 3 (evidence for smaller BIC model)"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#aicbic-1",
    "href": "assignments/multilevel_modeling2.html#aicbic-1",
    "title": "Multilevel Modeling 2",
    "section": "AIC/BIC",
    "text": "AIC/BIC\n\n\nCode\nperformance::model_performance(max_model) %&gt;% # easystats\n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAIC\nAICc\nBIC\nR2_conditional\nR2_marginal\nICC\nRMSE\nSigma\n\n\n\n\n-19801.66\n-19801.65\n-19761.87\n0.0773469\n0.0013442\n0.076105\n0.0407697\n0.0409264"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#hypothesis-testing",
    "href": "assignments/multilevel_modeling2.html#hypothesis-testing",
    "title": "Multilevel Modeling 2",
    "section": "Hypothesis testing",
    "text": "Hypothesis testing\n\nMultiple options\n\nt/F tests with approximate degrees of freedom (Kenward-Rogers or Satterwaithe)\nParametric bootstrap\nLikelihood ratio test (LRT)\n\n\nCan be interpreted as main effects and interactions\nUse afex package to do that"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#hypothesis-testing---afex",
    "href": "assignments/multilevel_modeling2.html#hypothesis-testing---afex",
    "title": "Multilevel Modeling 2",
    "section": "Hypothesis testing - afex",
    "text": "Hypothesis testing - afex\n\n\nCode\nlibrary(afex) # load afex in \n\nm &lt;- mixed(mean_pupil ~ 1 + vocoded +  (1+vocoded|subject), data =eye, method = \"LRT\") # fit lmer using afex\n\nnice(m) %&gt;%\n  kable()\n\n\n\n\n\nEffect\ndf\nChisq\np.value\n\n\n\n\nvocoded\n1\n4.47 *\n.034"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#using-emmeans",
    "href": "assignments/multilevel_modeling2.html#using-emmeans",
    "title": "Multilevel Modeling 2",
    "section": "Using emmeans",
    "text": "Using emmeans\n\nGet means and contrasts\n\n\n\nCode\nlibrary(emmeans) # get marginal means \n\nemmeans(max_model, specs = \"vocoded\") %&gt;% \n  kable() # grabs means/SEs for each level of vocode \n\n\n\n\n\nvocoded\nemmean\nSE\ndf\nasymp.LCL\nasymp.UCL\n\n\n\n\nNS\n0.0036427\n0.0022348\nInf\n-0.0007374\n0.0080229\n\n\nV6\n0.0067668\n0.0022618\nInf\n0.0023337\n0.0111999\n\n\n\n\n\nCode\npairs(emmeans(max_model, specs = \"vocoded\")) %&gt;%\n  confint() %&gt;%\n  kable()\n\n\n\n\n\ncontrast\nestimate\nSE\ndf\nasymp.LCL\nasymp.UCL\n\n\n\n\nNS - V6\n-0.0031241\n0.0014532\nInf\n-0.0059723\n-0.0002759\n\n\n\n\n\nCode\n# use this to get pariwise compairsons between levels of factors"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#check-assumptions",
    "href": "assignments/multilevel_modeling2.html#check-assumptions",
    "title": "Multilevel Modeling 2",
    "section": "Check assumptions",
    "text": "Check assumptions\n\n\n\nLinearity\nNormality\n\nLevel 1 residuals are normally distributed around zero\nLevel 2 residuals are multivariate-normal with a mean of zero\n\nHomoskedacticity\n\nLevel 1/Level 2 predictors and residuals are homoskedastic\n\n\n\n\nCollinearity\nOutliers"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#assumptions-1",
    "href": "assignments/multilevel_modeling2.html#assumptions-1",
    "title": "Multilevel Modeling 2",
    "section": "Assumptions",
    "text": "Assumptions\n\n\nCode\nlibrary(easystats) # performance package\n\ncheck_model(max_model)"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#visualization",
    "href": "assignments/multilevel_modeling2.html#visualization",
    "title": "Multilevel Modeling 2",
    "section": "Visualization",
    "text": "Visualization"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#ggeffects",
    "href": "assignments/multilevel_modeling2.html#ggeffects",
    "title": "Multilevel Modeling 2",
    "section": "ggeffects",
    "text": "ggeffects\n\n\nCode\nggemmeans(max_model, terms=c(\"vocoded\")) %&gt;% plot()"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#effect-size",
    "href": "assignments/multilevel_modeling2.html#effect-size",
    "title": "Multilevel Modeling 2",
    "section": "Effect size",
    "text": "Effect size\n\nReport pseudo-\\(R^2\\) for marginal (fixed) and conditional model (full model) (Nakagawa et al. 2017)\n\n\\[\nR^2_{LMM(c)} = \\frac{\\sigma_f^2\\text{fixed} + \\sigma_a^2\\text{random}}{\\sigma_f^2\\text{fixed} + \\sigma_a^2\\text{random} + \\sigma_e^2\\text{residual}}\n\\]\n\\[\nR^2_{\\text{LMM}(m)} = \\frac{\\sigma_f^2\\text{fixed}}{\\sigma_f^2\\text{fixed} + \\sigma_a^2\\text{random} + \\sigma_e^2\\text{residual}}\n\\]\n\nReport semi-partial \\(R^2\\) for each predictor variable\n\n\\(R^2_\\beta\\)\n\npartR2 package in R does this for you"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#effect-size-1",
    "href": "assignments/multilevel_modeling2.html#effect-size-1",
    "title": "Multilevel Modeling 2",
    "section": "Effect size",
    "text": "Effect size\n\n\nCode\n#get r2 for model with performance from easystats\n\nperformance::r2(max_model) \n\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.077\n     Marginal R2: 0.001\n\n\n\n\nCode\n# get semi-part\nlibrary(partR2)\n# does not work with random slopes for some reason :/\n#R2_3 &lt;- partR2(max_model,data=eye, \n  #partvars = c(\"vocoded\"),\n  #R2_type = \"marginal\", nboot = 10, CI = 0.95\n#)"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#effect-size-2",
    "href": "assignments/multilevel_modeling2.html#effect-size-2",
    "title": "Multilevel Modeling 2",
    "section": "Effect size",
    "text": "Effect size\n\nCohen’s \\(d\\) for treatment effects/categorical predictions3\n\n\\[\nd = \\frac{\\text{Effect}}{\\sqrt{\\sigma^2_\\text{Intercept} + \\sigma^2_\\text{slope} + \\sigma^2_\\text{residual}}}\n\\]\n\n\nCode\nlibrary(emmeans)\n\n# Then later in your code\nemmeans(max_model, ~ vocoded) %&gt;% \n  eff_size(sigma=.04, edf=30)\n\n\n contrast effect.size     SE  df asymp.LCL asymp.UCL\n NS - V6      -0.0781 0.0377 Inf    -0.152  -0.00421\n\nsigma used for effect sizes: 0.04 \nDegrees-of-freedom method: inherited from asymptotic when re-gridding \nConfidence level used: 0.95"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#describing-a-mlm-analysis---structure",
    "href": "assignments/multilevel_modeling2.html#describing-a-mlm-analysis---structure",
    "title": "Multilevel Modeling 2",
    "section": "Describing a MLM analysis - Structure",
    "text": "Describing a MLM analysis - Structure\n\nWhat was the nested data structure (e.g., how many levels; what were the units at each level?)\n\nLevel 1: Individual students (n = 1,427) Level 2: Clusters (classrooms/schools) identified by clu_id (n = 222)\n• How many units were in each level, on average? \nOn average, there were 6.43 students per cluster\n• What was the range of the number of lower-level units in each group/cluster?\nThe range of students per cluster was 2 to 18 The outcome variable was RTW_SpringK (reading/writing ability in Spring of Kindergarten)"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#describing-a-mlm-analysis---model",
    "href": "assignments/multilevel_modeling2.html#describing-a-mlm-analysis---model",
    "title": "Multilevel Modeling 2",
    "section": "Describing a MLM analysis - Model",
    "text": "Describing a MLM analysis - Model\n\n\n\nWhat equation can best represent your model?\n\n\\[\nY_{ij} = \\beta_0 + \\beta_1 X_{ij} + \\beta_2 Z_j + u_{0j} + u_{1j}X_{ij} + e_{ij}\n\\]\nWhere \\(Y_{ij}\\) is the reading/writing score, \\(X_{ij}\\) is the vocabulary score, and \\(Z_j\\) is the treatment indicator.\n\nWhat estimation method was used (e.g., ML, REML)?\n\nREML\n\nIf there were convergence issues, how was this addressed?\n\nNo convergence issues were encountered in the final model\n\nWhat software (and version) was used (when using R, what packages as well)?\n\nPackage lme4 in R was used for model fitting, with lmerTest for p-values\n\n\nIf degrees of freedom were used, what kind?\n\nSatterthwaite included in the lmerTest package\n\nWhat type of models were estimated (i.e., unconditional, random intercept, random slope, max)?\n\nThree types of models were estimated: 1. Unconditional means model (null model with only random intercepts) 2. Random intercept model with fixed effects for TRT and PPVT_FallK 3. Random intercept and slope model (adding random slopes for PPVT_FallK)\n\nWhat variables were centered and what kind of centering was used?\n\nppvtc1 appears to be group-mean centered PPVT scores ppvtc2 appears to be grand-mean centered PPVT scores ctrt_e1 and ctrt_e2 suggest centered treatment effect variables\nWe used both group-mean and grand-mean centering.\n\nWhat model assumptions were checked and what were the results?\n\nNormality of residuals - Residuals showed approximately normal distribution with no substantial deviations Homoscedasticity - Plots of residuals versus fitted values indicated relatively constant variance Linearity - The relationship between predictors and the outcome appeared reasonably linear Independence - The multilevel structure accounted for the clustering in the data\nNo severe violations of model assumptions were detected that would invalidate the inferences from the model."
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#describing-a-mlm-analysis---results",
    "href": "assignments/multilevel_modeling2.html#describing-a-mlm-analysis---results",
    "title": "Multilevel Modeling 2",
    "section": "Describing a MLM analysis - Results",
    "text": "Describing a MLM analysis - Results\n\n\n\nWhat was the ICC of the outcome variable?\n\nThe ICC of the reading/writing outcome variable was 0.23, indicating that 23% of the total variance in reading/writing scores was attributable to between-cluster differences, while 77% was due to within-cluster (individual student) differences.\n\nAre fixed effects and variance components reported?\n\nYes, both fixed effects and variance components were reported. The fixed effects included the intercept (β₀), treatment effect (β₂), and vocabulary score effect (β₁). Variance components included random intercept variance (1.74), random slope variance for PPVT_FallK (0.002), the correlation between random effects (-0.26), and residual variance (10.02).\n\nWhat inferential statistics were used (e.g., t-statistics, LRTs)?\n\nLRTs were used to compare nested models. Additionally, t-statistics were reported for individual fixed effects parameters with p-values.\n\nHow precise were the results (report the standard errors and/or confidence intervals)?\n\nThe results included standard errors for fixed effects: treatment effect (SE = 0.19) and vocabulary score effect (SE = 0.01). Confidence intervals were reported for the treatment effect, with the 95% CI [0.44, 1.18] indicating the plausible range of the treatment effect.\n\n\nWere model comparisons performed (e.g., AIC, BIC, if using an LRT,report the χ2, degrees of freedom, and p value)?\n\nYes, model comparisons were performed using likelihood ratio tests. The comparison between the null model and the random intercept model showed χ² = 261.89, df = 2, p &lt; 0.001. The comparison between the random intercept model and random intercept & slope model showed χ² = 9.88, df = 2, p = 0.007. AIC and BIC values were also reported for all models.\n\nWere effect sizes reported for overall model and individual predictors (e.g., Cohen’s d, \\(R^2\\) )?\n\nYes, effect sizes were reported. For the overall model, marginal R² was 0.29 (variance explained by fixed effects) and conditional R² was 0.45 (variance explained by the entire model). Cohen’s d for the treatment effect was 0.24, indicating a small-to-medium effect size."
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#write-up",
    "href": "assignments/multilevel_modeling2.html#write-up",
    "title": "Multilevel Modeling 2",
    "section": "Write-up",
    "text": "Write-up\n\n\nCode\nlibrary(modelsummary)\nmodelsummary(max_model, stars = TRUE)\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)\n                  0.004\n                \n                \n                  \n                  (0.002)\n                \n                \n                  vocodedV6\n                  0.003*\n                \n                \n                  \n                  (0.001)\n                \n                \n                  SD (Intercept subject)\n                  0.012\n                \n                \n                  SD (vocodedV6 subject)\n                  0.005\n                \n                \n                  Cor (Intercept~vocodedV6 subject)\n                  -0.195\n                \n                \n                  SD (Observations)\n                  0.041\n                \n                \n                  Num.Obs.\n                  5609\n                \n                \n                  R2 Marg.\n                  0.001\n                \n                \n                  R2 Cond.\n                  0.077\n                \n                \n                  AIC\n                  -19801.7\n                \n                \n                  BIC\n                  -19761.9\n                \n                \n                  ICC\n                  0.1\n                \n                \n                  RMSE\n                  0.04\n                \n        \n      \n    \n\n\n\nCode\n##| eval: false\n#report::report(max_model) # easystats report function\n\n\nWe fitted a linear mixed model (estimated using REML and nloptwrap optimizer) to predict mean_pupil with vocoded (formula: mean_pupil ~ vocoded). The model included vocoded as random effects (formula: ~1 + vocoded | subject). The model’s total explanatory power is weak (conditional R2 = 0.08) and the part related to the fixed effects alone (marginal R2) is of 1.34e-03. The model’s intercept, corresponding to vocoded = NS, is at 3.64e-03 (95% CI [-7.38e-04, 8.02e-03], t(5603) = 1.63, p = 0.103). Within this model:\n\nThe effect of vocoded [V6] is statistically significant and positive (beta = 3.12e-03, 95% CI [2.75e-04, 5.97e-03], t(5603) = 2.15, p = 0.032; Std. beta = 0.07, 95% CI [6.49e-03, 0.14])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using a Wald t-distribution approximation."
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#table",
    "href": "assignments/multilevel_modeling2.html#table",
    "title": "Multilevel Modeling 2",
    "section": "Table",
    "text": "Table\n\n\nCode\nmodelsummary::modelsummary(list(\"max model\" = max_model), output=\"html\") # modelsummary package\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                max model\n              \n        \n        \n        \n                \n                  (Intercept)\n                  0.004\n                \n                \n                  \n                  (0.002)\n                \n                \n                  vocodedV6\n                  0.003\n                \n                \n                  \n                  (0.001)\n                \n                \n                  SD (Intercept subject)\n                  0.012\n                \n                \n                  SD (vocodedV6 subject)\n                  0.005\n                \n                \n                  Cor (Intercept~vocodedV6 subject)\n                  −0.195\n                \n                \n                  SD (Observations)\n                  0.041\n                \n                \n                  Num.Obs.\n                  5609\n                \n                \n                  R2 Marg.\n                  0.001\n                \n                \n                  R2 Cond.\n                  0.077\n                \n                \n                  AIC\n                  −19801.7\n                \n                \n                  BIC\n                  −19761.9\n                \n                \n                  ICC\n                  0.1\n                \n                \n                  RMSE\n                  0.04"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#power",
    "href": "assignments/multilevel_modeling2.html#power",
    "title": "Multilevel Modeling 2",
    "section": "Power",
    "text": "Power\n\nSimulation-based power analyses\n\nSimulate new data\n\nfaux (https://debruine.github.io/faux/articles/sim_mixed.html)\n\nUse pilot data (what I would do)\n\nmixedpower(https://link.springer.com/article/10.3758/s13428-021-01546-0)\nsimr (https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.12504)"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#footnotes",
    "href": "assignments/multilevel_modeling2.html#footnotes",
    "title": "Multilevel Modeling 2",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBarr, D. J., Levy, R., Scheepers, C., & Tily, H. J. (2013). Random effects structure for confirmatory hypothesis testing: Keep it maximal. Journal of memory and language, 68(3), 10.1016/j.jml.2012.11.001. https://doi.org/10.1016/j.jml.2012.11.001↩︎\nBURNHAM, ANDERSON, & HUYVAERT (2011)↩︎\nBrysbaert, M., & Debeer, D. (2023, September 12). How to run linear mixed effects analysis for pairwise comparisons? A tutorial and a proposal for the calculation of standardized effect sizes. https://doi.org/10.31234/osf.io/esnku↩︎"
  }
]