[
  {
    "objectID": "assignments/index.html",
    "href": "assignments/index.html",
    "title": "ordinal regression",
    "section": "",
    "text": "If you are fitting a model, display the model output in a neatly formatted table. (The tidy and kable functions can help!)\nIf you are creating a plot, use clear labels for all axes, titles, etc.\nIf you are using Github, don’t forget to commit and push your work to to it regularly, at least after each exercise. Write short and informative commit messages. Else, if you are submitting on Canvas, make sure that the version you submit is the latest, and that it runs/knits without any errors.\nWhen you’re done, we should be able to knit the final version of the QMD in your GitHub as a HTML."
  },
  {
    "objectID": "assignments/index.html#instructions",
    "href": "assignments/index.html#instructions",
    "title": "ordinal regression",
    "section": "",
    "text": "If you are fitting a model, display the model output in a neatly formatted table. (The tidy and kable functions can help!)\nIf you are creating a plot, use clear labels for all axes, titles, etc.\nIf you are using Github, don’t forget to commit and push your work to to it regularly, at least after each exercise. Write short and informative commit messages. Else, if you are submitting on Canvas, make sure that the version you submit is the latest, and that it runs/knits without any errors.\nWhen you’re done, we should be able to knit the final version of the QMD in your GitHub as a HTML."
  },
  {
    "objectID": "assignments/index.html#load-packages",
    "href": "assignments/index.html#load-packages",
    "title": "ordinal regression",
    "section": "Load packages:",
    "text": "Load packages:\n\n\nCode\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(performance)\nlibrary(ordinal) #clm\nlibrary(car) # anova\nlibrary(ggeffects) #  viz\nlibrary(gofcat) # brant\nlibrary(brms)\nlibrary(emmeans) # contrasts\nlibrary(knitr)"
  },
  {
    "objectID": "assignments/index.html#load-data",
    "href": "assignments/index.html#load-data",
    "title": "ordinal regression",
    "section": "Load data",
    "text": "Load data\n\nMake sure only the top 3 ranks are being used. For some reason, there are missing ranks (my guess is they did not announce rank on TV)\n\n\n\nCode\ngbbo &lt;- read_csv(\"https://raw.githubusercontent.com/suyoghc/PSY-504_Spring-2025/refs/heads/main/Ordinal%20Regression/data/GBBO.csv\")\n\n# Enter code to filter. Think about the data type that would be relevant for Rank\ngb &lt;- gbbo %&gt;% \n  filter(!is.na(`Technical Rank`) & `Technical Rank` %in% c(1, 2, 3))"
  },
  {
    "objectID": "assignments/index.html#explore",
    "href": "assignments/index.html#explore",
    "title": "ordinal regression",
    "section": "Explore",
    "text": "Explore\n\nPlot two figures showing the percentage of bakers in each rank— create one for Gender and Age\n\n\nCode\n#plot percentage of bakers in each rank by gender\ngender_rank &lt;- gb %&gt;%\n  group_by(Gender, `Technical Rank`) %&gt;%\n  summarise(n = n(), .groups = 'drop') %&gt;%\n  mutate(perc = n / sum(n) * 100)\n\nggplot(gender_rank, aes(x = factor(`Technical Rank`), y = perc, fill = Gender)) +\n  geom_bar(stat = \"identity\", position = position_dodge(width = 0.8)) +\n  labs(title = \"Percentage of Bakers in Each Technical Rank by Gender\",\n   x = \"Technical Rank\",\n   y = \"Percentage\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n #plot percentage of bakers in each rank by age group\ngb &lt;- gb %&gt;% \n  mutate(AgeGroup = cut(Age, \n                        breaks = seq(floor(min(Age, na.rm = TRUE)), ceiling(max(Age, na.rm = TRUE)), by = 10),\n                        include.lowest = TRUE, right = FALSE))\n\nage_rank &lt;- gb %&gt;%\n  group_by(AgeGroup, `Technical Rank`) %&gt;%\n  summarise(n = n(), .groups = 'drop') %&gt;%\n  mutate(perc = n / sum(n) * 100)\n\nggplot(age_rank, aes(x = AgeGroup, y = perc, fill = factor(`Technical Rank`))) +\n  geom_bar(stat = \"identity\", position = position_dodge(width = 0.8)) +\n  labs(title = \"Percentage of Bakers in Each Technical Rank by Age Group\",\n       x = \"Age Group\",\n       y = \"Percentage\",\n       fill = \"Technical Rank\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\nCode\n#here's a plot without grouping age into bins, which is less informative visually so I prefer the above plot\nage_rank_cont &lt;- gb %&gt;%\n  group_by(Age, `Technical Rank`) %&gt;%\n  summarise(n = n(), .groups = 'drop') %&gt;%\n  \n  group_by(Age) %&gt;%\n  mutate(perc = n / sum(n) * 100) %&gt;%\n  ungroup()\n\nggplot(age_rank_cont, aes(x = factor(Age), y = perc, fill = factor(`Technical Rank`))) +\n  geom_bar(stat = \"identity\", position = position_dodge(width = 0.8)) +\n  labs(title = \"Percentage of Bakers in Each Technical Rank by Age\",\n       x = \"Age\",\n       y = \"Percentage\",\n       fill = \"Technical Rank\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "assignments/index.html#ordinal-analysis",
    "href": "assignments/index.html#ordinal-analysis",
    "title": "ordinal regression",
    "section": "Ordinal Analysis",
    "text": "Ordinal Analysis\n\nIf you haven’t already, convert the outcome variable to an ordered factor. What does the order here represent?\n\n\nCode\n#The order now represents the ranking from best (1) to worst (3) among the top three.\ngb &lt;- gb %&gt;%\n  mutate(`Technical Rank` = factor(`Technical Rank`, levels = c(1, 2, 3), ordered = TRUE))\n\n\nConvert input variables to categorical factors as appropriate.\n\n\nCode\ngb &lt;- gb %&gt;%\n  mutate(Gender = factor(Gender, levels = c(\"M\", \"F\")))\n\n\nRun a ordinal logistic regression model against all relevant input variables. Interpret the effects for Gender, Age and Gender*Age (even if they are non-significant).\n\n\nCode\nmodel_int &lt;- clm(`Technical Rank` ~ Gender * Age, data = gb)\n\ntidy(model_int) %&gt;% kable(digits = 3)\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\ncoef.type\n\n\n\n\n1|2\n-0.267\n0.499\n-0.535\n0.593\nintercept\n\n\n2|3\n1.154\n0.504\n2.291\n0.022\nintercept\n\n\nGenderF\n1.149\n0.673\n1.708\n0.088\nlocation\n\n\nAge\n0.016\n0.014\n1.147\n0.252\nlocation\n\n\nGenderF:Age\n-0.039\n0.019\n-2.093\n0.036\nlocation\n\n\n\n\n\n\ninterpretation:\ngender: The coefficient for GenderF is 1.149 (p = 0.088). This means that, holding Age constant, females have a 1.149 unit higher latent score compared to males. Although this effect is only marginally significant (p = 0.088), it suggests that being female may be associated with a shift toward a higher rank category.\nage: The coefficient for Age is -0.013 (p = 0.000). This means that for each additional year of age, there is a 0.016 unit increase in the latent score. However, this effect is not statistically significant (p = 0.252), so we do not have strong evidence that Age alone affects technical rank.\ngenderxage: The interaction has an estimate of -0.039 (p = 0.036), which is statistically significant. This indicates that the effect of Age on technical rank differs by gender. Specifically, for females, the impact of Age is 0.039 units lower than for males.\n\nTest if the interaction is warranted\n\n#Hint: You need to create two models with clm(); one with interaction and one without. #Then you compare them using the anova test using anova()\n::: {.cell}\n\n```{.r .cell-code}\nmodel_int &lt;- clm(`Technical Rank` ~ Gender * Age, data = gb)\nmodel_no_int &lt;- clm(`Technical Rank` ~ Gender + Age, data = gb)\nanova(model_no_int, model_int)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLikelihood ratio tests of cumulative link models:\n \n             formula:                        link: threshold:\nmodel_no_int `Technical Rank` ~ Gender + Age logit flexible  \nmodel_int    `Technical Rank` ~ Gender * Age logit flexible  \n\n             no.par    AIC  logLik LR.stat df Pr(&gt;Chisq)  \nmodel_no_int      4 685.72 -338.86                        \nmodel_int         5 683.28 -336.64   4.437  1    0.03517 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n**interpretation: Including the Gender*Age interaction significantly improves the model (p &lt; .05), meaning that the effect of Age on the technical rank is different for different genders. In other words, the relationship between Age and technical rank depends on whether the baker is male or female.**\n\nUse ggemmeans to create a figure showing the interaction between Gender and Age as a function of rank. Plot predicted probabilities from the model.\n\n\nCode\ngb &lt;- gb %&gt;% rename(Technical_Rank = `Technical Rank`)\nmodel_int &lt;- clm(Technical_Rank ~ Gender * Age, data = gb)\npreds &lt;- ggemmeans(model_int, terms = c(\"Age [all]\", \"Gender\"), regrid = FALSE)\nplot(preds) +\n  labs(title = \"Predicted Probabilities for Technical_Rank by Age and Gender\",\n   x = \"Age\",\n   y = \"Predicted Probability\")\n\n\n\n\n\n\n\n\n\n\n\nLatent Visualization\n\n\nCode\nols_clm &lt;- MASS::polr(Technical_Rank ~ Gender * Age, data = gb)\n\nggeffect(ols_clm, terms = c(\"Age[all]\", \"Gender\"), latent = TRUE) %&gt;% \n  plot() +\n  labs(title = \"Effect of Age and Gender on Technical_Rank\",\n       x = \"Age\",\n       y = \"Latent Variable\")\n\n\n\n\n\n\n\n\n\n\nUse the Brant test to support or reject the hypothesis that the proportional odds assumption holds for your simplified model.\n\n\nCode\nbrant.test(ols_clm)\n\n\n\nBrant Test:\n               chi-sq   df   pr(&gt;chi)\nOmnibus         1.295    3       0.73\nGenderF         0.585    1       0.44\nAge             1.052    1       0.31\nGenderF:Age     0.924    1       0.34\n\nH0: Proportional odds assumption holds\n\n\n\nThe omnibus chi-square statistic is 1.295 with 3 degrees of freedom and a p-value of 0.73. Since this p-value is not significant, we do not reject the null hypothesis. This indicates that overall the proportional odds assumption holds for the model. None of the tests (including all the individual tests) are statistically significant, we conclude that there is no evidence to reject the null hypothesis. Thus, the proportional odds assumption holds for the simplified model.\n## `brms`\n\nBelow is a model implementation using the brms package. We will just use the default priors for this. The exercise is to run this code and note your observations. What are salient differences you observe in how the model fitting takes place With respect to the results, how do you compare the results of the model you fit with clm and the one you fit with brms?\n\n\n\nCode\n  ols2_brm &lt;- brm(Technical_Rank ~ Gender * Age, \n                  data = gb, \n                  family = cumulative, \n                  cores = 4, \n                  chains = 4, \n                  seed = 123)\n\n\nWhile both clm and brms provide very similar point estimates and conclusions regarding the effects of Gender, Age, and their interaction on Technical_Rank, the Bayesian approach (brms) offers richer information regarding uncertainty (CI) and additional convergence diagnostics.\n\nThe conditional_effects function is used to plot predicted probabilities by Gender and Age across each rank.\n::: {.cell}\n\nCode\nconditional_effects(ols2_brm, categorical = T)\n\n::: {.cell-output-display}  :::\n::: {.cell-output-display}  ::: :::\ncheck_predictions from the easystats performance package is used for examining model fit (i.e., does the data fit the model being used?). Run the below code. What do you think?\n\n\n\nCode\ncheck_predictions(ols2_brm)\n\n\n\n\n\n\n\n\n\nThis function shows how well the Bayesian model’s predictions align with the data. The green dots represent the actual counts (or frequencies) for each category of Technical_Rank in the dataset. The blue dots and error bars represent the mean (or median) model-predicted counts, while the error bars reflect the uncertainty (95% CI) around the predictions. The model predictions align fairly well with the actual data."
  },
  {
    "objectID": "assignments/ordinal_regression.html",
    "href": "assignments/ordinal_regression.html",
    "title": "Ordinal Regression Assignment",
    "section": "",
    "text": "If you are fitting a model, display the model output in a neatly formatted table. (The tidy and kable functions can help!)\nIf you are creating a plot, use clear labels for all axes, titles, etc.\nIf you are using Github, don’t forget to commit and push your work to to it regularly, at least after each exercise. Write short and informative commit messages. Else, if you are submitting on Canvas, make sure that the version you submit is the latest, and that it runs/knits without any errors.\nWhen you’re done, we should be able to knit the final version of the QMD in your GitHub as a HTML."
  },
  {
    "objectID": "assignments/ordinal_regression.html#instructions",
    "href": "assignments/ordinal_regression.html#instructions",
    "title": "Ordinal Regression Assignment",
    "section": "",
    "text": "If you are fitting a model, display the model output in a neatly formatted table. (The tidy and kable functions can help!)\nIf you are creating a plot, use clear labels for all axes, titles, etc.\nIf you are using Github, don’t forget to commit and push your work to to it regularly, at least after each exercise. Write short and informative commit messages. Else, if you are submitting on Canvas, make sure that the version you submit is the latest, and that it runs/knits without any errors.\nWhen you’re done, we should be able to knit the final version of the QMD in your GitHub as a HTML."
  },
  {
    "objectID": "assignments/ordinal_regression.html#load-packages",
    "href": "assignments/ordinal_regression.html#load-packages",
    "title": "Ordinal Regression Assignment",
    "section": "Load packages:",
    "text": "Load packages:\n\n\nCode\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(performance)\nlibrary(ordinal) #clm\nlibrary(car) # anova\nlibrary(ggeffects) #  viz\nlibrary(gofcat) # brant\nlibrary(brms)\nlibrary(emmeans) # contrasts\nlibrary(knitr)"
  },
  {
    "objectID": "assignments/ordinal_regression.html#load-data",
    "href": "assignments/ordinal_regression.html#load-data",
    "title": "Ordinal Regression Assignment",
    "section": "Load data",
    "text": "Load data\n\nMake sure only the top 3 ranks are being used. For some reason, there are missing ranks (my guess is they did not announce rank on TV)\n\n\n\nCode\ngbbo &lt;- read_csv(\"https://raw.githubusercontent.com/suyoghc/PSY-504_Spring-2025/refs/heads/main/Ordinal%20Regression/data/GBBO.csv\")\n\n# Enter code to filter. Think about the data type that would be relevant for Rank\ngb &lt;- gbbo %&gt;% \n  filter(!is.na(`Technical Rank`) & `Technical Rank` %in% c(1, 2, 3))"
  },
  {
    "objectID": "assignments/ordinal_regression.html#explore",
    "href": "assignments/ordinal_regression.html#explore",
    "title": "Ordinal Regression Assignment",
    "section": "Explore",
    "text": "Explore\n\nPlot two figures showing the percentage of bakers in each rank— create one for Gender and Age\n\n\nCode\n#plot percentage of bakers in each rank by gender\ngender_rank &lt;- gb %&gt;%\n  group_by(Gender, `Technical Rank`) %&gt;%\n  summarise(n = n(), .groups = 'drop') %&gt;%\n  mutate(perc = n / sum(n) * 100)\n\nggplot(gender_rank, aes(x = factor(`Technical Rank`), y = perc, fill = Gender)) +\n  geom_bar(stat = \"identity\", position = position_dodge(width = 0.8)) +\n  labs(title = \"Percentage of Bakers in Each Technical Rank by Gender\",\n   x = \"Technical Rank\",\n   y = \"Percentage\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n #plot percentage of bakers in each rank by age group\ngb &lt;- gb %&gt;% \n  mutate(AgeGroup = cut(Age, \n                        breaks = seq(floor(min(Age, na.rm = TRUE)), ceiling(max(Age, na.rm = TRUE)), by = 10),\n                        include.lowest = TRUE, right = FALSE))\n\nage_rank &lt;- gb %&gt;%\n  group_by(AgeGroup, `Technical Rank`) %&gt;%\n  summarise(n = n(), .groups = 'drop') %&gt;%\n  mutate(perc = n / sum(n) * 100)\n\nggplot(age_rank, aes(x = AgeGroup, y = perc, fill = factor(`Technical Rank`))) +\n  geom_bar(stat = \"identity\", position = position_dodge(width = 0.8)) +\n  labs(title = \"Percentage of Bakers in Each Technical Rank by Age Group\",\n       x = \"Age Group\",\n       y = \"Percentage\",\n       fill = \"Technical Rank\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\nCode\n#here's a plot without grouping age into bins, which is less informative visually so I prefer the above plot\nage_rank_cont &lt;- gb %&gt;%\n  group_by(Age, `Technical Rank`) %&gt;%\n  summarise(n = n(), .groups = 'drop') %&gt;%\n  \n  group_by(Age) %&gt;%\n  mutate(perc = n / sum(n) * 100) %&gt;%\n  ungroup()\n\nggplot(age_rank_cont, aes(x = factor(Age), y = perc, fill = factor(`Technical Rank`))) +\n  geom_bar(stat = \"identity\", position = position_dodge(width = 0.8)) +\n  labs(title = \"Percentage of Bakers in Each Technical Rank by Age\",\n       x = \"Age\",\n       y = \"Percentage\",\n       fill = \"Technical Rank\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "assignments/ordinal_regression.html#ordinal-analysis",
    "href": "assignments/ordinal_regression.html#ordinal-analysis",
    "title": "Ordinal Regression Assignment",
    "section": "Ordinal Analysis",
    "text": "Ordinal Analysis\n\nIf you haven’t already, convert the outcome variable to an ordered factor. What does the order here represent?\n\n\nCode\n#The order now represents the ranking from best (1) to worst (3) among the top three.\ngb &lt;- gb %&gt;%\n  mutate(`Technical Rank` = factor(`Technical Rank`, levels = c(1, 2, 3), ordered = TRUE))\n\n\nConvert input variables to categorical factors as appropriate.\n\n\nCode\ngb &lt;- gb %&gt;%\n  mutate(Gender = factor(Gender, levels = c(\"M\", \"F\")))\n\n\nRun a ordinal logistic regression model against all relevant input variables. Interpret the effects for Gender, Age and Gender*Age (even if they are non-significant).\n\n\nCode\nmodel_int &lt;- clm(`Technical Rank` ~ Gender * Age, data = gb)\n\ntidy(model_int) %&gt;% kable(digits = 3)\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\ncoef.type\n\n\n\n\n1|2\n-0.267\n0.499\n-0.535\n0.593\nintercept\n\n\n2|3\n1.154\n0.504\n2.291\n0.022\nintercept\n\n\nGenderF\n1.149\n0.673\n1.708\n0.088\nlocation\n\n\nAge\n0.016\n0.014\n1.147\n0.252\nlocation\n\n\nGenderF:Age\n-0.039\n0.019\n-2.093\n0.036\nlocation\n\n\n\n\n\n\ninterpretation:\ngender: The coefficient for GenderF is 1.149 (p = 0.088). This means that, holding Age constant, females have a 1.149 unit higher latent score compared to males. Although this effect is only marginally significant (p = 0.088), it suggests that being female may be associated with a shift toward a higher rank category.\nage: The coefficient for Age is -0.013 (p = 0.000). This means that for each additional year of age, there is a 0.016 unit increase in the latent score. However, this effect is not statistically significant (p = 0.252), so we do not have strong evidence that Age alone affects technical rank.\ngenderxage: The interaction has an estimate of -0.039 (p = 0.036), which is statistically significant. This indicates that the effect of Age on technical rank differs by gender. Specifically, for females, the impact of Age is 0.039 units lower than for males.\n\nTest if the interaction is warranted\n\n#Hint: You need to create two models with clm(); one with interaction and one without. #Then you compare them using the anova test using anova()\n::: {.cell}\n\n```{.r .cell-code}\nmodel_int &lt;- clm(`Technical Rank` ~ Gender * Age, data = gb)\nmodel_no_int &lt;- clm(`Technical Rank` ~ Gender + Age, data = gb)\nanova(model_no_int, model_int)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLikelihood ratio tests of cumulative link models:\n \n             formula:                        link: threshold:\nmodel_no_int `Technical Rank` ~ Gender + Age logit flexible  \nmodel_int    `Technical Rank` ~ Gender * Age logit flexible  \n\n             no.par    AIC  logLik LR.stat df Pr(&gt;Chisq)  \nmodel_no_int      4 685.72 -338.86                        \nmodel_int         5 683.28 -336.64   4.437  1    0.03517 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n**interpretation: Including the Gender*Age interaction significantly improves the model (p &lt; .05), meaning that the effect of Age on the technical rank is different for different genders. In other words, the relationship between Age and technical rank depends on whether the baker is male or female.**\n\nUse ggemmeans to create a figure showing the interaction between Gender and Age as a function of rank. Plot predicted probabilities from the model.\n\n\nCode\ngb &lt;- gb %&gt;% rename(Technical_Rank = `Technical Rank`)\nmodel_int &lt;- clm(Technical_Rank ~ Gender * Age, data = gb)\npreds &lt;- ggemmeans(model_int, terms = c(\"Age [all]\", \"Gender\"), regrid = FALSE)\nplot(preds) +\n  labs(title = \"Predicted Probabilities for Technical_Rank by Age and Gender\",\n   x = \"Age\",\n   y = \"Predicted Probability\")\n\n\n\n\n\n\n\n\n\n\n\nLatent Visualization\n\n\nCode\nols_clm &lt;- MASS::polr(Technical_Rank ~ Gender * Age, data = gb)\n\nggeffect(ols_clm, terms = c(\"Age[all]\", \"Gender\"), latent = TRUE) %&gt;% \n  plot() +\n  labs(title = \"Effect of Age and Gender on Technical_Rank\",\n       x = \"Age\",\n       y = \"Latent Variable\")\n\n\n\n\n\n\n\n\n\n\nUse the Brant test to support or reject the hypothesis that the proportional odds assumption holds for your simplified model.\n\n\nCode\nbrant.test(ols_clm)\n\n\n\nBrant Test:\n               chi-sq   df   pr(&gt;chi)\nOmnibus         1.295    3       0.73\nGenderF         0.585    1       0.44\nAge             1.052    1       0.31\nGenderF:Age     0.924    1       0.34\n\nH0: Proportional odds assumption holds\n\n\n\nThe omnibus chi-square statistic is 1.295 with 3 degrees of freedom and a p-value of 0.73. Since this p-value is not significant, we do not reject the null hypothesis. This indicates that overall the proportional odds assumption holds for the model. None of the tests (including all the individual tests) are statistically significant, we conclude that there is no evidence to reject the null hypothesis. Thus, the proportional odds assumption holds for the simplified model.\n## `brms`\n\nBelow is a model implementation using the brms package. We will just use the default priors for this. The exercise is to run this code and note your observations. What are salient differences you observe in how the model fitting takes place With respect to the results, how do you compare the results of the model you fit with clm and the one you fit with brms?\n\n\n\nCode\n  ols2_brm &lt;- brm(Technical_Rank ~ Gender * Age, \n                  data = gb, \n                  family = cumulative, \n                  cores = 4, \n                  chains = 4, \n                  seed = 123)\n\n\nWhile both clm and brms provide very similar point estimates and conclusions regarding the effects of Gender, Age, and their interaction on Technical_Rank, the Bayesian approach (brms) offers richer information regarding uncertainty (CI) and additional convergence diagnostics.\n\nThe conditional_effects function is used to plot predicted probabilities by Gender and Age across each rank.\n::: {.cell}\n\nCode\nconditional_effects(ols2_brm, categorical = T)\n\n::: {.cell-output-display}  :::\n::: {.cell-output-display}  ::: :::\ncheck_predictions from the easystats performance package is used for examining model fit (i.e., does the data fit the model being used?). Run the below code. What do you think?\n\n\n\nCode\ncheck_predictions(ols2_brm)\n\n\n\n\n\n\n\n\n\nThis function shows how well the Bayesian model’s predictions align with the data. The green dots represent the actual counts (or frequencies) for each category of Technical_Rank in the dataset. The blue dots and error bars represent the mean (or median) model-predicted counts, while the error bars reflect the uncertainty (95% CI) around the predictions. The model predictions align fairly well with the actual data."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PSY504 Assignments Blog",
    "section": "",
    "text": "Lab 2 Logistic Regression Assignment\nLab 3 Ordinal Regression Assignment\nLab 4 Multinomial Regression Assignment\nLab 5 Poisson Regression Assignment\nLab 6 Multilevel Modeling Walkthrough Assignment\nLab 7 Multilevel Modeling2 Assignment\nLab 8 Bayesian Modeling Assignment\nLab 9 Bayesian Modeling2 Assignment\nLab 10a Bayesian Modeling Priors Predictive Checks Assignment\nLab 10b Bayesian Modeling HMC Diagnostics Assignment\nLab 11 Missing Data Assignment"
  },
  {
    "objectID": "assignments/logistic_regression.html",
    "href": "assignments/logistic_regression.html",
    "title": "Logistic Regression Assignment",
    "section": "",
    "text": "Assignment requirements:\n\nIf you are using Github (recommended), make sure to commit and push your work to GitHub regularly, at least after each exercise. Write short and informative commit messages, and share the link to your assignment with me. If not, you can also send me the rmd & rendered file via Canvas.\nIn this assignment, you will not need to code from scratch. Rather, you’ll need to fill in code where needed. This assignment has a logisitic regression implementation for a scenario from EDA down to model comparison (and would be useful for whenever you may encounter such a situation in the future).\nI want the assignments to begin reflecting a bit more of how you’d be doing things on your own, where you have some prior knowledge and you figure other things out (by referring to documentation, etc.) . In addition to the rmd, I also want you to submit to me notes of anything new that you learn while finishing the assignment. And any pain-points, and we’ll discuss more.\n\nNote:\n\nIf you are fitting a model, display the model output in a neatly formatted table. (The gt tidy and kable functions can help!). Modelsummary also looks good(https://vincentarelbundock.github.io/modelsummary/articles/modelsummary.html)\nMake sure that your plots are clearly labeled – for all axes, titles, etc."
  },
  {
    "objectID": "assignments/logistic_regression.html#data-general-social-survey",
    "href": "assignments/logistic_regression.html#data-general-social-survey",
    "title": "Logistic Regression Assignment",
    "section": "Data: General Social Survey",
    "text": "Data: General Social Survey\nThe General Social Survey (GSS) has been used to measure trends in attitudes and behaviors in American society since 1972. In addition to collecting demographic information, the survey includes questions used to gauge attitudes about government spending priorities, confidence in institutions, lifestyle, and many other topics. A full description of the survey may be found here.\nThe data for this lab are from the 2016 General Social Survey. The original data set contains 2867 observations and 935 variables. We will use and abbreviated data set that includes the following variables:\nnatmass: Respondent’s answer to the following prompt:\n“We are faced with many problems in this country, none of which can be solved easily or inexpensively. I’m going to name some of these problems, and for each one I’d like you to tell me whether you think we’re spending too much money on it, too little money, or about the right amount…are we spending too much, too little, or about the right amount on mass transportation?”\nage: Age in years.\nsex: Sex recorded as male or female\nsei10: Socioeconomic index from 0 to 100\nregion: Region where interview took place\npolviews: Respondent’s answer to the following prompt:\n“We hear a lot of talk these days about liberals and conservatives. I’m going to show you a seven-point scale on which the political views that people might hold are arranged from extremely liberal - point 1 - to extremely conservative - point 7. Where would you place yourself on this scale?”\nThe data are in gss2016.csv in the data folder."
  },
  {
    "objectID": "assignments/logistic_regression.html#eda",
    "href": "assignments/logistic_regression.html#eda",
    "title": "Logistic Regression Assignment",
    "section": "EDA",
    "text": "EDA\n\nLet’s begin by making a binary variable for respondents’ views on spending on mass transportation. Create a new variable that is equal to “1” if a respondent said spending on mass transportation is about right and “0” otherwise. Then plot the proportion of the response variable, using informative labels for each category.\n\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(modelsummary)\nlibrary(tidyr)\nlibrary(knitr)\nlibrary(easystats)\nlibrary(broom)\nlibrary(emmeans)\nlibrary(marginaleffects)\nlibrary(performance)\nlibrary(arm)\nlibrary(modelsummary)\n\n\n\n\nCode\n# load data\ndata &lt;- read.csv(\"gss2016.csv\")\n\n\nFill in the “____” below to encode the binary variable\n\n\nCode\ndata &lt;- data %&gt;%\n   mutate(mass_trans_spend_right = ifelse(natmass == \"About right\", 1, 0))\n\n\n\n\nCode\n#Get proportions\nmass_spend_summary &lt;- data %&gt;%\n  count(mass_trans_spend_right) %&gt;%\n  mutate(proportion = n / sum(n))\n\n\n#Look at the dataframe structure. And make sure it's in a format that you can use for plotting.\n#Change structure if neederd\nmass_spend_long &lt;- mass_spend_summary %&gt;%\n  mutate(spend_category = ifelse(mass_trans_spend_right == 1, \"About right\", \"Not right\"))\n\n#Factorise for plot\nmass_spend_long$mass_trans_spend_right &lt;- as.factor(mass_spend_long$mass_trans_spend_right)\n\n#Make plot\n#Hint: geom_bar lets you make stacked bar charts\n\nggplot(mass_spend_summary, aes(x = factor(mass_trans_spend_right), y = proportion, fill = factor(mass_trans_spend_right))) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_manual(values = c(\"#E69F00\", \"#56B4E9\"),\n                   labels = c(\"Not right\", \"About right\")) +\n  labs(title = \"Proportion of Responses on Mass Transportation Spending\",\n       x = \"Response\",\n       y = \"Proportion\",\n       fill = \"Spending View\") +\n  scale_x_discrete(labels = c(\"Not right\", \"About right\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nRecode polviews so it is a factor with levels that are in an order that is consistent with question on the survey. Note how the categories are spelled in the data.\n\n\n\nCode\ndata &lt;- data %&gt;%\n  mutate(polviews = factor(polviews,\n                          levels = c(\"Extremely liberal\", \"Liberal\", \"Slightly liberal\", \n                                   \"Moderate\", \"Slghtly conservative\", \"Conservative\", \n                                   \"Extrmly conservative\"),\n                          ordered = TRUE))\n\n\n\nMake a plot of the distribution of polviews\n\n\n\nCode\n#Get proportions, format, and produce a plot like you did previously for mass_trans_spend_right\npalette &lt;- c(\n  \"#772e25\", \"#c44536\", \"#ee9b00\", \"#197278\", \"#283d3b\", \n  \"#9CC5A1\", \"#6195C6\", \"#ADA7C9\", \"#4D4861\", \"grey50\",\n  \"#d4a373\", \"#8a5a44\", \"#4a6a74\", \"#5c80a8\", \"#a9c5a0\",\n  \"#7b9b8e\", \"#e1b16a\", \"#a69b7c\", \"#9d94c4\", \"#665c54\"\n)\n\npalette_condition = c(\"#ee9b00\", \"#c44536\",\"#005f73\", \"#283d3b\", \"#9CC5A1\", \"#6195C6\", \"#ADA7C9\", \"#4D4861\")\n\nplot_aes = theme_minimal() +\n  theme(legend.position = \"top\",\n        legend.text = element_text(size = 12),\n        text = element_text(size = 16, family = \"Futura Medium\"),\n        axis.text = element_text(color = \"black\"),\n        axis.ticks.y = element_blank())\n\npolviews_summary &lt;- data %&gt;% #proportion\n  count(polviews) %&gt;%\n  mutate(proportion = n / sum(n))\n\nggplot(polviews_summary, aes(x = polviews, y = proportion, fill = polviews)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_manual(values = palette) + \n  labs(title = \"Distribution of Political Views\",\n       x = \"Political Views\",\n       y = \"Proportion\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\nCode\n  plot_aes\n\n\nList of 136\n $ line                            :List of 6\n  ..$ colour       : chr \"black\"\n  ..$ linewidth    : num 0.5\n  ..$ linetype     : num 1\n  ..$ lineend      : chr \"butt\"\n  ..$ arrow        : logi FALSE\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_line\" \"element\"\n $ rect                            :List of 5\n  ..$ fill         : chr \"white\"\n  ..$ colour       : chr \"black\"\n  ..$ linewidth    : num 0.5\n  ..$ linetype     : num 1\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_rect\" \"element\"\n $ text                            :List of 11\n  ..$ family       : chr \"Futura Medium\"\n  ..$ face         : chr \"plain\"\n  ..$ colour       : chr \"black\"\n  ..$ size         : num 16\n  ..$ hjust        : num 0.5\n  ..$ vjust        : num 0.5\n  ..$ angle        : num 0\n  ..$ lineheight   : num 0.9\n  ..$ margin       : 'margin' num [1:4] 0points 0points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : logi FALSE\n  ..$ inherit.blank: logi FALSE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ title                           : NULL\n $ aspect.ratio                    : NULL\n $ axis.title                      : NULL\n $ axis.title.x                    :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 1\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 2.75points 0points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.title.x.top                :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 0\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 0points 2.75points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.title.x.bottom             : NULL\n $ axis.title.y                    :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 1\n  ..$ angle        : num 90\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 2.75points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.title.y.left               : NULL\n $ axis.title.y.right              :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 1\n  ..$ angle        : num -90\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 0points 0points 2.75points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text                       :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : chr \"black\"\n  ..$ size         : 'rel' num 0.8\n  ..$ hjust        : NULL\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi FALSE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text.x                     :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 1\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 2.2points 0points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text.x.top                 :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 0\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 0points 2.2points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text.x.bottom              : NULL\n $ axis.text.y                     :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : num 1\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 2.2points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text.y.left                : NULL\n $ axis.text.y.right               :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : num 0\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 0points 0points 2.2points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text.theta                 : NULL\n $ axis.text.r                     :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : num 0.5\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 2.2points 0points 2.2points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.ticks                      : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ axis.ticks.x                    : NULL\n $ axis.ticks.x.top                : NULL\n $ axis.ticks.x.bottom             : NULL\n $ axis.ticks.y                    : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ axis.ticks.y.left               : NULL\n $ axis.ticks.y.right              : NULL\n $ axis.ticks.theta                : NULL\n $ axis.ticks.r                    : NULL\n $ axis.minor.ticks.x.top          : NULL\n $ axis.minor.ticks.x.bottom       : NULL\n $ axis.minor.ticks.y.left         : NULL\n $ axis.minor.ticks.y.right        : NULL\n $ axis.minor.ticks.theta          : NULL\n $ axis.minor.ticks.r              : NULL\n $ axis.ticks.length               : 'simpleUnit' num 2.75points\n  ..- attr(*, \"unit\")= int 8\n $ axis.ticks.length.x             : NULL\n $ axis.ticks.length.x.top         : NULL\n $ axis.ticks.length.x.bottom      : NULL\n $ axis.ticks.length.y             : NULL\n $ axis.ticks.length.y.left        : NULL\n $ axis.ticks.length.y.right       : NULL\n $ axis.ticks.length.theta         : NULL\n $ axis.ticks.length.r             : NULL\n $ axis.minor.ticks.length         : 'rel' num 0.75\n $ axis.minor.ticks.length.x       : NULL\n $ axis.minor.ticks.length.x.top   : NULL\n $ axis.minor.ticks.length.x.bottom: NULL\n $ axis.minor.ticks.length.y       : NULL\n $ axis.minor.ticks.length.y.left  : NULL\n $ axis.minor.ticks.length.y.right : NULL\n $ axis.minor.ticks.length.theta   : NULL\n $ axis.minor.ticks.length.r       : NULL\n $ axis.line                       : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ axis.line.x                     : NULL\n $ axis.line.x.top                 : NULL\n $ axis.line.x.bottom              : NULL\n $ axis.line.y                     : NULL\n $ axis.line.y.left                : NULL\n $ axis.line.y.right               : NULL\n $ axis.line.theta                 : NULL\n $ axis.line.r                     : NULL\n $ legend.background               : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ legend.margin                   : 'margin' num [1:4] 5.5points 5.5points 5.5points 5.5points\n  ..- attr(*, \"unit\")= int 8\n $ legend.spacing                  : 'simpleUnit' num 11points\n  ..- attr(*, \"unit\")= int 8\n $ legend.spacing.x                : NULL\n $ legend.spacing.y                : NULL\n $ legend.key                      : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ legend.key.size                 : 'simpleUnit' num 1.2lines\n  ..- attr(*, \"unit\")= int 3\n $ legend.key.height               : NULL\n $ legend.key.width                : NULL\n $ legend.key.spacing              : 'simpleUnit' num 5.5points\n  ..- attr(*, \"unit\")= int 8\n $ legend.key.spacing.x            : NULL\n $ legend.key.spacing.y            : NULL\n $ legend.frame                    : NULL\n $ legend.ticks                    : NULL\n $ legend.ticks.length             : 'rel' num 0.2\n $ legend.axis.line                : NULL\n $ legend.text                     :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : num 12\n  ..$ hjust        : NULL\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi FALSE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ legend.text.position            : NULL\n $ legend.title                    :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : num 0\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ legend.title.position           : NULL\n $ legend.position                 : chr \"top\"\n $ legend.position.inside          : NULL\n $ legend.direction                : NULL\n $ legend.byrow                    : NULL\n $ legend.justification            : chr \"center\"\n $ legend.justification.top        : NULL\n $ legend.justification.bottom     : NULL\n $ legend.justification.left       : NULL\n $ legend.justification.right      : NULL\n $ legend.justification.inside     : NULL\n $ legend.location                 : NULL\n $ legend.box                      : NULL\n $ legend.box.just                 : NULL\n $ legend.box.margin               : 'margin' num [1:4] 0cm 0cm 0cm 0cm\n  ..- attr(*, \"unit\")= int 1\n $ legend.box.background           : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ legend.box.spacing              : 'simpleUnit' num 11points\n  ..- attr(*, \"unit\")= int 8\n  [list output truncated]\n - attr(*, \"class\")= chr [1:2] \"theme\" \"gg\"\n - attr(*, \"complete\")= logi TRUE\n - attr(*, \"validate\")= logi TRUE\n\n\n\nWhich political view occurs most frequently in this data set?"
  },
  {
    "objectID": "assignments/logistic_regression.html#logistic-regression",
    "href": "assignments/logistic_regression.html#logistic-regression",
    "title": "Logistic Regression Assignment",
    "section": "Logistic regression",
    "text": "Logistic regression\n\nLet’s start by fitting a logistic regression model with just the intercept\n\n\n\nCode\nintercept_only_model &lt;- glm(\n  mass_trans_spend_right ~ 1,\n  family = binomial(link = \"logit\"),\n  data = data\n)\nsummary(intercept_only_model)\n\n\n\nCall:\nglm(formula = mass_trans_spend_right ~ 1, family = binomial(link = \"logit\"), \n    data = data)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)  0.11906    0.03937   3.024  0.00249 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 3581.3  on 2589  degrees of freedom\nResidual deviance: 3581.3  on 2589  degrees of freedom\nAIC: 3583.3\n\nNumber of Fisher Scoring iterations: 3\n\n\n\nInterpret the intercept in the context of the data. You can do this by converting the \\(\\beta_0\\) parameter out of the log-odds metric to the probability metric. Make sure to include the 95% confidence intervals. Then interpret the results in a sentence or two–what is the basic thing this probability tells us about?\n\n\n\nCode\nb0 &lt;- coef(intercept_only_model)[1] # get coef\n\nb0_transformed &lt;- exp(b0) / (1 + exp(b0)) # logistic transform\nprint(b0_transformed)\n\n\n(Intercept) \n  0.5297297 \n\n\nCode\nci_lower = b0 - 1.96 * 0.0393685\nci_upper = b0 + 1.96 * 0.0393685\n\n#transforming confidence intervals of coefficients into probabilities\np_lower = exp(ci_lower) / (1 + exp(ci_lower))\np_upper = exp(ci_upper) / (1 + exp(ci_upper))\nprint(paste(\"95% CI: [\", round(p_lower, 3), \",\", round(p_upper, 3), \"]\"))\n\n\n[1] \"95% CI: [ 0.51 , 0.549 ]\"\n\n\nInterpretation: The intercept-only model predicts that approximately 53% of respondents (95% CI: [51%, 55%]) think mass transportation spending is “about right”. This represents the overall proportion of people satisfied with current mass transportation spending levels, without accounting for other factors such as demographic or political views.\n\nNow let’s fit a model using the demographic factors - age,sex, sei10 - to predict the odds a person is satisfied with spending on mass transportation. Make any necessary adjustments to the variables so the intercept will have a meaningful interpretation. Neatly display the model coefficients (do not display the summary output)\n\n\n\nCode\n#make sure that sex is a factor (i.e. to make sure R knows it's binary/categorical, and not continuous)\ndata$sex &lt;- as.factor(data$sex)\n\n#fit with glm()\nm1 &lt;- glm(\n  mass_trans_spend_right ~ sex + age + sei10,\n  family = binomial(link = \"logit\"),\n  data = data\n)\nm1 %&gt;%  #produce tidy output of model coefficients\n  tidy() %&gt;%  \n  kable()    \n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.8254509\n0.1395587\n5.914722\n0.0000000\n\n\nsexMale\n-0.2557439\n0.0798020\n-3.204732\n0.0013519\n\n\nage\n-0.0061659\n0.0022824\n-2.701502\n0.0069027\n\n\nsei10\n-0.0062271\n0.0016609\n-3.749229\n0.0001774\n\n\n\n\n\n\nConsider the relationship between sex and one’s opinion about spending on mass transportation. Interpret the coefficient of sex in terms of the logs odds and OR of being satisfied with spending on mass transportation. What are the predicted probabilities for males and females on support for spending on mass transportation? Please include the 95% CIs around each estimate.\n\n\n\nCode\nm1 %&gt;% \n  tidy() %&gt;%\n  kable()\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.8254509\n0.1395587\n5.914722\n0.0000000\n\n\nsexMale\n-0.2557439\n0.0798020\n-3.204732\n0.0013519\n\n\nage\n-0.0061659\n0.0022824\n-2.701502\n0.0069027\n\n\nsei10\n-0.0062271\n0.0016609\n-3.749229\n0.0001774\n\n\n\n\n\nCode\nm1 %&gt;% \n  tidy(exponentiate = TRUE) %&gt;%\n  kable()\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n2.2829100\n0.1395587\n5.914722\n0.0000000\n\n\nsexMale\n0.7743403\n0.0798020\n-3.204732\n0.0013519\n\n\nage\n0.9938530\n0.0022824\n-2.701502\n0.0069027\n\n\nsei10\n0.9937922\n0.0016609\n-3.749229\n0.0001774\n\n\n\n\n\nCode\nbsex &lt;- coef(m1)[\"sexMale\"]\n\nci_lower_lo = bsex - 1.96 * 0.0798020\nci_upper_lo = bsex + 1.96 * 0.0798020\n\nci_lower_or = 1.29 - 1.96 * 0.0798020\nci_upper_or = 1.29 + 1.96 * 0.0798020\n\nlist(\n  \"CI for log-odds\" = c(ci_lower_lo, ci_upper_lo),\n  \"CI for Odds Ratio\" = c(ci_lower_or, ci_upper_or)\n)\n\n\n$`CI for log-odds`\n    sexMale     sexMale \n-0.41215578 -0.09933194 \n\n$`CI for Odds Ratio`\n[1] 1.133588 1.446412\n\n\nCode\nemm_sex &lt;- emmeans(m1, \"sex\", type = \"response\")\n\n\nIf you did this right, you’ll find that being female (as compared to male) is associated with an increase in the log-odds of being satisfied with spending on mass transportation by 0.2557439 units (95% CI [0.09, 0.41]), holding all other variables constant. This equates to the odds of thinking the spending amount is right in females being 1.29 times the odds of thinking this in men (95% CI [1.13, 1.44]).\nThe predicted probability for females to be satisfied with spending on mass transportation is 55.9% (95% CI [53.3%, 58.5%]) and that of males is 49.5% (95% CI [46.7%, 52.4%]).\n\nVerify this.\n\nNext, consider the relationship between age and one’s opinion about spending on mass transportation. Interpret the coefficient of age in terms of the logs odds and OR of being satisfied with spending on mass transportation. Please include the 95% CIs around each estimate.\n\n\n\nCode\nbage &lt;- coef(m1)[\"age\"]\nse_age &lt;- sqrt(vcov(m1)[\"age\", \"age\"])\nprint(bage)\n\n\n        age \n-0.00616594 \n\n\nCode\nprint(se_age)\n\n\n[1] 0.002282412\n\n\nCode\nci_lower_lo = bage - 1.96 * se_age\nci_upper_lo = bage + 1.96 * se_age\n\nor_age &lt;- exp(bage)\nci_lower_or = exp(ci_lower_lo)\nci_upper_or = exp(ci_upper_lo)\nprint(or_age)\n\n\n     age \n0.993853 \n\n\nCode\nlist(\n  \"CI for log-odds\" = c(ci_lower_lo, ci_upper_lo),\n  \"CI for Odds Ratio\" = c(ci_lower_or, ci_upper_or)\n)\n\n\n$`CI for log-odds`\n         age          age \n-0.010639467 -0.001692412 \n\n$`CI for Odds Ratio`\n      age       age \n0.9894169 0.9983090 \n\n\nA one unit increase in age is associated with a decrease in the log-odds of being satisfied with spending on mass transportation by -0.0062, holding all other variables constant. The odds ratio is 0.994, which confirms the negative relationship implied by the log-odds coefficient. Specifically, for each additional unit of age, the odds of being satisfied with mass transportation spending decrease by a factor of about 0.994, or approximately 0.6% per unit increase in age, holding other factors constant.\n\nConsider the relationship between SES and one’s opinion about spending on mass transportation. Interpret the coefficient of SES in terms of the logs odds and OR of being satisfied with spending on mass transportation. Please include the 95% CIs around each estimate. ß\n\n\n\nCode\nbses &lt;- coef(m1)\nbses &lt;- coef(m1)[\"sei10\"]\n\n\nbses &lt;- coef(m1)[\"sei10\"]\nses_se &lt;- sqrt(vcov(m1)[\"sei10\", \"sei10\"])\n\nci_lower_log_odds &lt;- bses - 1.96 * ses_se\nci_upper_log_odds &lt;- bses + 1.96 * ses_se\n\nodds_ratio &lt;- exp(bses)\nci_lower_odds_ratio &lt;- exp(ci_lower_log_odds)\nci_upper_odds_ratio &lt;- exp(ci_upper_log_odds)\n\nresults &lt;- data.frame(\n  Metric = c(\"Log-odds coefficient\", \n             \"95% CI for log-odds\", \n             \"Odds Ratio\", \n             \"95% CI for Odds Ratio\"),\n  Value = c(\n    format(round(bses, 4), nsmall = 4),\n    paste0(\"[\", format(round(ci_lower_log_odds, 4), nsmall = 4), \", \", \n           format(round(ci_upper_log_odds, 4), nsmall = 4), \"]\"),\n    format(round(odds_ratio, 4), nsmall = 4),\n    paste0(\"[\", format(round(ci_lower_odds_ratio, 4), nsmall = 4), \", \", \n           format(round(ci_upper_odds_ratio, 4), nsmall = 4), \"]\")\n  )\n)\n\n# Display using kable\nkable(results)\n\n\n\n\n\nMetric\nValue\n\n\n\n\nLog-odds coefficient\n-0.0062\n\n\n95% CI for log-odds\n[-0.0095, -0.0030]\n\n\nOdds Ratio\n0.9938\n\n\n95% CI for Odds Ratio\n[0.9906, 0.9970]\n\n\n\n\n\nA one unit increase in SES index is associated with a decrease in the log-odds of being satisfied with spending on mass transportation by 0.0062 units (95% CI [-0.0107, -0.0017]), holding all other variables constant. The odds ratio is less than 1 (0.9937922), which confirms the negative relationship implied by the log-odds coefficient. Specifically, for each additional unit of SES index, the odds of being satisfied with mass transportation spending decrease by a factor of about 0.993, or approximately 0.7% per unit increase in SES index, holding other factors constant (95% CI [0.989, 0.998])."
  },
  {
    "objectID": "assignments/logistic_regression.html#marginal-effects",
    "href": "assignments/logistic_regression.html#marginal-effects",
    "title": "Logistic Regression Assignment",
    "section": "Marginal effects",
    "text": "Marginal effects\n\nLet’s examine the results on the probability scale.\n\n\nCalculate the marginal effects of sex, age, and SES on mass transportation spending. You can use the margins package function margins discussed in your textbook or you can use the marginaleffects package avg_slope avg_comparisons discussed in lecture. Interpret each estimate.\n\n\n\nCode\navg_comparisons(m1, comparison = \"difference\") %&gt;% \n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\ncontrast\nestimate\nstd.error\nstatistic\np.value\ns.value\nconf.low\nconf.high\n\n\n\n\nage\n+1\n-0.0015153\n0.0005579\n-2.716128\n0.0066050\n7.242218\n-0.0026088\n-0.0004219\n\n\nsei10\n+1\n-0.0015304\n0.0004039\n-3.789362\n0.0001510\n12.692832\n-0.0023219\n-0.0007388\n\n\nsex\nMale - Female\n-0.0630688\n0.0196461\n-3.210251\n0.0013262\n9.558494\n-0.1015743\n-0.0245632\n\n\n\n\n\n\nThe marginal effect of age is -0.00152 (95% CI [0.00261, -0.00042]). So, for each additional unit increase of age, the probability of being satisfied with mass transportation spending decreases by approximately 0.15 percentage points, holding other factors constant (p = 0.007).\nThe marginal effect of SES is -0.00153 (95% CI [0.00232, -0.00074]). For each one-unit increase in the socioeconomic index, the probability of being satisfied with mass transportation spending decreases by approximately 0.15 percentage points, holding other variables constant.\nThe marginal effect for being female compared to male is 0.06 (95% CI [0.025, 0.102]). This indicates that females are, on average, about 6.31 percentage points more likely than males to be satisfied with mass transportation spending, holding other factors constant."
  },
  {
    "objectID": "assignments/logistic_regression.html#model-comparison",
    "href": "assignments/logistic_regression.html#model-comparison",
    "title": "Logistic Regression Assignment",
    "section": "Model comparison",
    "text": "Model comparison\n\nNow let’s see whether a person’s political views has a significant impact on their odds of being satisfied with spending on mass transportation, after accounting for the demographic factors.\n\n\nConduct a drop-in-deviance/likelihood ratio test to determine if polviews is a significant predictor of attitude towards spending on mass transportation. Name these two models fit2 and fit3, respectively. Compare the two models.\n\n\n\nCode\nfit2 &lt;- glm(\n  mass_trans_spend_right ~ sex + age + sei10,\n  family = binomial(link = \"logit\"),\n  data = data\n)\n\nfit3 &lt;- glm(\n  mass_trans_spend_right ~ sex + age + sei10 + polviews,\n  family = binomial(link = \"logit\"),\n  data = data\n)\n\ntest_likelihoodratio(fit2, fit3) %&gt;% \n  kable()\n\n\n\n\n\n\nName\nModel\ndf\ndf_diff\nChi2\np\n\n\n\n\nfit2\nfit2\nglm\n4\nNA\nNA\nNA\n\n\nfit3\nfit3\nglm\n10\n6\n63.02844\n0\n\n\n\n\n\n\nIs the model with polviews better than the model without?\n\n\nYes."
  },
  {
    "objectID": "assignments/logistic_regression.html#visualization",
    "href": "assignments/logistic_regression.html#visualization",
    "title": "Logistic Regression Assignment",
    "section": "Visualization",
    "text": "Visualization\n\nLet’s plot the results\nWe next use the model to produce visualizations:\n\nGiven the code below, interpet what is being plotted:\n\npol_plot : This plot shows the predicted probability of being satisfied with mass transportation spending across different political views. It shows that as political ideology becomes more conservative, the probability of being satisfied increases.\nsex_plot : This plot shows the predicted probability of satisfaction with mass transportation spending for males and females. It highlights that females are more likely than males to consider the current spending level as “about right.”\nses_plot: This plot shows the effect of SES on the predicted probability of satisfaction with mass transportation spending. It suggests that as SES increases, satisfaction decreases. The shaded region represents the confidence interval around the prediction.\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nadjust the various settings in your plot to make it look professional.\nYou can use ggeffects to get the predicted probabilities for these models.\n\n\n\n\n\n\nCode\nlibrary(ggeffects)\n\n\ncolors &lt;- c(\"Extremely liberal\" = \"black\",\n            \"Liberal\" = \"#0e2f44\",  # Dark blue\n            \"Slightly liberal\" = \"#1d5a6c\",  # Less dark blue\n            \"Moderate\" = \"#358ca3\",  # Medium blue\n            \"Slghtly conservative\" = \"#71b9d1\",  # Light blue\n            \"Conservative\" = \"#a6dcef\",  # Lighter blue\n            \"Extrmly conservative\" = \"#d0f0fd\")  # Very light blue\n\npp_pol &lt;- ggemmeans(fit3, terms = c(\"polviews\"))\n\n# Adjusted plot with gradient colors\npol_plot &lt;- ggplot(pp_pol, aes(x = x, y = predicted, color = x)) +\n  geom_point(size = 2) +\n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) +\n  scale_color_manual(values = colors) +\n  labs(title = \"Effect of Political Views on Satisfaction with Mass Transportation\",\n       x = \"Political Views\", y = \"Predicted Probability\",\n       color = \"Political Views\") +\n  theme_minimal()\n\npol_plot\n\n\n\n\n\n\n\n\n\nCode\npp_sex &lt;- ggemmeans(fit3, terms = c(\"sex\"))\n\nsex_plot &lt;- ggplot(pp_sex, aes(x = x, y = predicted, color = x)) +\n  geom_point(size = 2) +\n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) +\n  labs(title = \"Effect of Sex on Satisfaction with Mass Transportation\",\n       x = \"Sex\", y = \"Predicted Probability\",\n       color = \"Sex\") +\n  theme_minimal()\n\npp_sex\n\n\n# Predicted probabilities of mass_trans_spend_right\n\nsex    | Predicted |     95% CI\n-------------------------------\nFemale |      0.55 | 0.51, 0.58\nMale   |      0.48 | 0.44, 0.51\n\nAdjusted for:\n*   age = 48.90\n* sei10 = 46.07\n\n\nCode\npp_ses &lt;- ggemmeans(fit3, terms = \"sei10\")\n\n\nses_plot &lt;-  ggplot(pp_ses, aes(x = x, y = predicted)) +\n  geom_line(color = \"#2c7fb8\", size = 1) + \n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), fill = \"#2c7fb8\", alpha = 0.2) +  # Add a confidence interval band\n  labs(title = \"Effect of SES on Satisfaction with Mass Transportation\",\n       x = \"Socioeconomic Status\", y = \"Predicted Probability\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")  \nses_plot"
  },
  {
    "objectID": "assignments/logistic_regression.html#model-assumptions",
    "href": "assignments/logistic_regression.html#model-assumptions",
    "title": "Logistic Regression Assignment",
    "section": "Model Assumptions",
    "text": "Model Assumptions\n\nIs the logistic model a good choice for this data?\n\n\n\nCode\nbinned_residuals(fit2)\n\n\nWarning: About 86% of the residuals are inside the error bounds (~95% or higher would be good).\n\n\n\n\n\n\n\n\nNote\n\n\n\nAnswer: The plots do not reveal any major systematic deviations from randomness. This suggests that the logistic regression model is a good choice for these data."
  },
  {
    "objectID": "assignments/logistic_regression.html#model-fit",
    "href": "assignments/logistic_regression.html#model-fit",
    "title": "Logistic Regression Assignment",
    "section": "Model fit",
    "text": "Model fit\n\nCalculate the \\(R^2\\) for this model\n\n\n\nCode\nr2_mcfadden(fit2)\n\n\n# R2 for Generalized Linear Regression\n       R2: 0.010\n  adj. R2: 0.009\n\n\n\nR2 interpretation: The R^2 value suggests that while the predictors have a statistically significant effect, a substantial portion of the variability in satisfaction remains unexplained.\nNext, Take a look at the binned residual plots for each continuous predictor variable and look at linearity. Is there a predictor that sticks out? What can we do to improve model fit in this case?\n\n\n\nCode\nbinned_residuals(fit2, term=\"sei10\")\n\n\nWarning: About 88% of the residuals are inside the error bounds (~95% or higher would be good).\n\n\nCode\nbinned_residuals(fit2, term=\"age\")\n\n\nOk: About 98% of the residuals are inside the error bounds.\n\n\nCode\nbinned_residuals(fit2, term=\"sei10\") %&gt;% plot(show_dots=TRUE)\n\n\n\n\n\n\n\n\n\nCode\nbinned_residuals(fit2, term=\"age\") %&gt;% plot(show_dots=TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe binned residual plot for sei10 curves slightly, suggesting that SES may not have a simple linear relationship with satisfaction. The plot for age looks more linear. To improve the model, we could try adding a squared term for SES."
  },
  {
    "objectID": "assignments/logistic_regression.html#testing-polviews",
    "href": "assignments/logistic_regression.html#testing-polviews",
    "title": "Logistic Regression Assignment",
    "section": "Testing Polviews",
    "text": "Testing Polviews\n\n\nCode\nemmeans(fit3, \"polviews\") %&gt;% pairs() %&gt;% as.data.frame() %&gt;% filter(p.value &lt; .05)\n\n\n contrast                                   estimate        SE  df z.ratio\n Extremely liberal - Moderate             -0.9266262 0.1950664 Inf  -4.750\n Extremely liberal - Slghtly conservative -0.8487137 0.2127293 Inf  -3.990\n Extremely liberal - Conservative         -0.9935486 0.2108369 Inf  -4.712\n Extremely liberal - Extrmly conservative -1.3402621 0.2792876 Inf  -4.799\n Liberal - Moderate                       -0.7090022 0.1308520 Inf  -5.418\n Liberal - Slghtly conservative           -0.6310897 0.1555805 Inf  -4.056\n Liberal - Conservative                   -0.7759246 0.1532081 Inf  -5.065\n Liberal - Extrmly conservative           -1.1226380 0.2392048 Inf  -4.693\n Slightly liberal - Extrmly conservative  -0.7334002 0.2412625 Inf  -3.040\n p.value\n  &lt;.0001\n  0.0013\n  0.0001\n  &lt;.0001\n  &lt;.0001\n  0.0010\n  &lt;.0001\n  0.0001\n  0.0382\n\nResults are averaged over the levels of: sex \nResults are given on the log odds ratio (not the response) scale. \nP value adjustment: tukey method for comparing a family of 7 estimates \n\n\nCode\nemmeans(fit3, \"polviews\", type=\"response\") %&gt;% pairs() %&gt;% as.data.frame() %&gt;% filter(p.value &lt; .05)\n\n\n contrast                                 odds.ratio         SE  df null\n Extremely liberal / Moderate              0.3958871 0.07722426 Inf    1\n Extremely liberal / Slghtly conservative  0.4279651 0.09104070 Inf    1\n Extremely liberal / Conservative          0.3702605 0.07806458 Inf    1\n Extremely liberal / Extrmly conservative  0.2617771 0.07311109 Inf    1\n Liberal / Moderate                        0.4921350 0.06439684 Inf    1\n Liberal / Slghtly conservative            0.5320118 0.08277063 Inf    1\n Liberal / Conservative                    0.4602780 0.07051835 Inf    1\n Liberal / Extrmly conservative            0.3254202 0.07784206 Inf    1\n Slightly liberal / Extrmly conservative   0.4802732 0.11587191 Inf    1\n z.ratio p.value\n  -4.750  &lt;.0001\n  -3.990  0.0013\n  -4.712  0.0001\n  -4.799  &lt;.0001\n  -5.418  &lt;.0001\n  -4.056  0.0010\n  -5.065  &lt;.0001\n  -4.693  0.0001\n  -3.040  0.0382\n\nResults are averaged over the levels of: sex \nP value adjustment: tukey method for comparing a family of 7 estimates \nTests are performed on the log odds ratio scale \n\n\n\nConservatives are 2.7 times more likely to support mass transit spending compared to extremely liberal and liberal\nExtreme liberals are 2.5 times more likely to support spending compared to conservatives, moderates and slight conservatives\nExtrm conservatives are 3.1 times more likely to support mass spending than liberals and slight liberals\nLiberals are 2.0 times more likely to support spending than moderates and slight conservatives."
  },
  {
    "objectID": "assignments/logistic_regression.html#conclusion",
    "href": "assignments/logistic_regression.html#conclusion",
    "title": "Logistic Regression Assignment",
    "section": "Conclusion",
    "text": "Conclusion\nBased on the model summary below, and the three figures, we can conclude that both demographic factors and political views significantly influence satisfaction with mass transportation spending. Specifically, women are more likely to support current spending than men, older individuals are less likely to support current spending, and individuals with higher SES are less likely to support current spending. Political views also play a significant role, with conservatives more likely to support current spending than liberal individuals.\nThe logistic regression model fits the data reasonably well, but the residual analysis suggests SES might have a non-linear relationship with satisfaction. The R^2 value indicates that while the model explains some variability, additional factors may be at play. Future research could explore more complex relationships between SES and satisfaction to improve model fit.\n\n\n\n\nDf\nDeviance\nResid. Df\nResid. Dev\nPr(&gt;Chi)\n\n\n\n\nNULL\nNA\nNA\n2589\n3581.340\nNA\n\n\nsex\n1\n11.31903\n2588\n3570.021\n0.0007672\n\n\nage\n1\n10.10603\n2587\n3559.915\n0.0014778\n\n\nsei10\n1\n14.11908\n2586\n3545.796\n0.0001716\n\n\npolviews\n6\n63.02844\n2580\n3482.768\n0.0000000\n\n\n\nTable 1\n\n\n\n\n\nFigure 1: Effect of Sex on Satisfaction with Mass Transportation\n\n\n\n\n\n\n\n\n\nFigure 2: Effect of SES on Satisfaction with Mass Transportation\n\n\n\n\n\n\n\n\n\nFigure 3: Effect of Political Views on Satisfaction with Mass Transportation"
  },
  {
    "objectID": "assignments/your_assignment.html",
    "href": "assignments/your_assignment.html",
    "title": "Your Assignment Title",
    "section": "",
    "text": "yaml\nLab Goal: Predict voting frequency using demographic variables Data source: FiveThirtyEight “Why Many Americans Don’t Vote” survey Method: Multinomial logistic regression"
  },
  {
    "objectID": "assignments/your_assignment.html#data",
    "href": "assignments/your_assignment.html#data",
    "title": "Your Assignment Title",
    "section": "Data",
    "text": "Data\nThe data for this assignment comes from an online Ipsos survey that was conducted for the FiveThirtyEight article “Why Many Americans Don’t Vote”. You can read more about the survey design and respondents in the README of the GitHub repo for the data.\nRespondents were asked a variety of questions about their political beliefs, thoughts on multiple issues, and voting behavior. We will focus on using the demographic variables and someone’s party identification to understand whether a person is a probable voter.\nThe variables we’ll focus on were (definitions from the codebook in data set GitHub repo):\n\nppage: Age of respondent\neduc: Highest educational attainment category.\n\nrace: Race of respondent, census categories. Note: all categories except Hispanic were non-Hispanic.\ngender: Gender of respondent\nincome_cat: Household income category of respondent\nQ30: Response to the question “Generally speaking, do you think of yourself as a…”\n\n1: Republican\n2: Democrat\n3: Independent\n4: Another party, please specify\n5: No preference\n-1: No response\n\nvoter_category: past voting behavior:\n\nalways: respondent voted in all or all-but-one of the elections they were eligible in\nsporadic: respondent voted in at least two, but fewer than all-but-one of the elections they were eligible in\nrarely/never: respondent voted in 0 or 1 of the elections they were eligible in\n\n\nYou can read in the data directly from the GitHub repo:\n\n\nCode\nlibrary(nnet)\nlibrary(car)\nlibrary(tidyverse)\nlibrary(emmeans)\nlibrary(ggeffects)\nlibrary(knitr)\nlibrary(patchwork)\nlibrary(broom)\nlibrary(parameters)\nlibrary(easystats)\n\n\n\n\nCode\nvoter_data &lt;- read_csv(\"https://raw.githubusercontent.com/fivethirtyeight/data/master/non-voters/nonvoters_data.csv\")"
  },
  {
    "objectID": "assignments/your_assignment.html#lrt",
    "href": "assignments/your_assignment.html#lrt",
    "title": "Your Assignment Title",
    "section": "LRT",
    "text": "LRT\n\nRun the full model and report overall significance of each of the terms\n\n\n\nCode\nlibrary(car)\nAnova(model_with_pol, type = \"II\")\n\n\nAnalysis of Deviance Table (Type II tests)\n\nResponse: voter_category\n              LR Chisq Df Pr(&gt;Chisq)    \nppage_c         638.30  2  &lt; 2.2e-16 ***\nrace             52.65  6  1.379e-09 ***\ngender            6.03  2     0.0491 *  \nincome_cat       67.72  6  1.198e-12 ***\neduc            154.14  4  &lt; 2.2e-16 ***\npol_ident_new   153.84  6  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "assignments/your_assignment.html#marginal-effects-political-group---emmeans",
    "href": "assignments/your_assignment.html#marginal-effects-political-group---emmeans",
    "title": "Your Assignment Title",
    "section": "Marginal Effects Political Group - Emmeans",
    "text": "Marginal Effects Political Group - Emmeans\n\n\nCode\n#Get estimated marginal means from the model\n\nlibrary(emmeans)\n\nmultinomial_analysis &lt;- emmeans(model_with_pol, ~ pol_ident_new | voter_category)\n\ncoefs &lt;- contrast(regrid(multinomial_analysis, \"log\"), \"trt.vs.ctrl1\", by = \"pol_ident_new\")\n\nupdate(coefs, by = \"contrast\") %&gt;% \n  kable(format = \"markdown\", digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontrast\npol_ident_new\nestimate\nSE\ndf\nt.ratio\np.value\n\n\n\n\nsporadic - (rarely/never)\nDem\n0.961\n0.070\n28\n13.722\n0.000\n\n\nalways - (rarely/never)\nDem\n0.480\n0.074\n28\n6.498\n0.000\n\n\nsporadic - (rarely/never)\nIndep\n0.591\n0.077\n28\n7.643\n0.000\n\n\nalways - (rarely/never)\nIndep\n-0.049\n0.084\n28\n-0.590\n0.900\n\n\nsporadic - (rarely/never)\nOther\n0.078\n0.087\n28\n0.902\n0.747\n\n\nalways - (rarely/never)\nOther\n-0.835\n0.110\n28\n-7.577\n0.000\n\n\nsporadic - (rarely/never)\nRep\n0.883\n0.084\n28\n10.469\n0.000\n\n\nalways - (rarely/never)\nRep\n0.327\n0.089\n28\n3.672\n0.004"
  },
  {
    "objectID": "assignments/your_assignment.html#marginal-effects-of-education---emmeans",
    "href": "assignments/your_assignment.html#marginal-effects-of-education---emmeans",
    "title": "Your Assignment Title",
    "section": "Marginal Effects of Education - Emmeans",
    "text": "Marginal Effects of Education - Emmeans\n\n\nCode\nmulti_an_educ &lt;- emmeans(model_with_pol, ~ educ | voter_category)\n\ncoefs_educ &lt;- contrast(regrid(multi_an_educ, \"log\"), \"trt.vs.ctrl1\", by = \"educ\")\nupdate(coefs_educ, by = \"contrast\") %&gt;% \n  kable(format = \"markdown\", digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontrast\neduc\nestimate\nSE\ndf\nt.ratio\np.value\n\n\n\n\nsporadic - (rarely/never)\nCollege\n0.986\n0.076\n28\n12.904\n0.000\n\n\nalways - (rarely/never)\nCollege\n0.477\n0.080\n28\n5.960\n0.000\n\n\nsporadic - (rarely/never)\nHigh school or less\n0.187\n0.069\n28\n2.705\n0.031\n\n\nalways - (rarely/never)\nHigh school or less\n-0.711\n0.080\n28\n-8.883\n0.000\n\n\nsporadic - (rarely/never)\nSome college\n0.707\n0.074\n28\n9.512\n0.000\n\n\nalways - (rarely/never)\nSome college\n0.167\n0.079\n28\n2.114\n0.112\n\n\n\n\n\n\nNext, plot the predicted probabilities of voter category as a function of Age and Party ID\n\n\n\nCode\nlibrary(ggeffects)\nlibrary(ggplot2)\n\n# Get predicted probabilities for age and party identification\ndf_preds &lt;- ggemmeans(model_with_pol, terms = c(\"ppage_c\", \"pol_ident_new [all]\"))\n\nggplot(df_preds, aes(x = x, y = predicted, fill = response.level)) +\n  geom_area() +\n  geom_rug(sides = \"b\", position = \"jitter\", alpha = 0.5) +\n  labs(\n    x = \"Centered Age\",\n    y = \"Predicted Probability\",\n    title = \"Predicted Probabilities of Voting Frequency\\nby Age and Party Identification\",\n    fill = \"Voter Category\"\n  ) +\n  scale_fill_manual(\n    values = c(\n      \"always\" = \"#F6B533\", \n      \"sporadic\" = \"#D07EA2\", \n      \"rarely/never\" = \"#9854F7\"\n    ),\n    labels = c(\"ALMOST ALWAYS VOTE\", \"SOMETIMES VOTE\", \"RARELY OR NEVER VOTE\")\n  ) +\n  facet_wrap(~group) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nPlot predicted probabilities as a function of education and voting frequency.\n\n\nCode\nggemmeans(model_with_pol, terms = \"educ\") %&gt;%\n  ggplot(aes(x = x, y = predicted, fill = response.level)) +\n  geom_bar(stat = \"identity\", position = \"fill\") +\n  labs(x = \"Education\", y = \"Predicted Probability\", \n       title = \"Predicted Probabilities of Voting Frequency by Education\") +\n  scale_fill_manual(\n    name = \"Voter Category\",\n    values = c(\"always\" = \"#F6B533\", \"sporadic\" = \"#D07EA2\", \"rarely/never\" = \"#9854F7\"),\n    labels = c(\"ALMOST ALWAYS VOTE\", \"SOMETIMES VOTE\", \"RARELY OR NEVER VOTE\")\n  ) +\n    coord_flip() +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n### Write-up\n\nDifferences between political groups and voting behavior - Emmeans\n\n\nCode\nmultinomial_analysis &lt;- emmeans(model_with_pol, ~ pol_ident_new | voter_category)\n\ncoefs &lt;- contrast(regrid(multinomial_analysis, \"log\"), \"trt.vs.ctrl1\", by = \"pol_ident_new\")\n\nupdate(coefs, by = \"contrast\") %&gt;% \n  kable(format = \"markdown\", digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontrast\npol_ident_new\nestimate\nSE\ndf\nt.ratio\np.value\n\n\n\n\nsporadic - (rarely/never)\nDem\n0.961\n0.070\n28\n13.722\n0.000\n\n\nalways - (rarely/never)\nDem\n0.480\n0.074\n28\n6.498\n0.000\n\n\nsporadic - (rarely/never)\nIndep\n0.591\n0.077\n28\n7.643\n0.000\n\n\nalways - (rarely/never)\nIndep\n-0.049\n0.084\n28\n-0.590\n0.900\n\n\nsporadic - (rarely/never)\nOther\n0.078\n0.087\n28\n0.902\n0.747\n\n\nalways - (rarely/never)\nOther\n-0.835\n0.110\n28\n-7.577\n0.000\n\n\nsporadic - (rarely/never)\nRep\n0.883\n0.084\n28\n10.469\n0.000\n\n\nalways - (rarely/never)\nRep\n0.327\n0.089\n28\n3.672\n0.004\n\n\n\n\n\nCode\n# Pairwise comparisons (reverse pairwise contrasts)\ncontrast(coefs, \"revpairwise\", by = \"contrast\") %&gt;%\n  kable(format = \"markdown\", digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontrast1\ncontrast\nestimate\nSE\ndf\nt.ratio\np.value\n\n\n\n\nIndep - Dem\nsporadic - (rarely/never)\n-0.370\n0.094\n28\n-3.933\n0.003\n\n\nOther - Dem\nsporadic - (rarely/never)\n-0.883\n0.103\n28\n-8.578\n0.000\n\n\nOther - Indep\nsporadic - (rarely/never)\n-0.513\n0.107\n28\n-4.807\n0.000\n\n\nRep - Dem\nsporadic - (rarely/never)\n-0.078\n0.099\n28\n-0.787\n0.860\n\n\nRep - Indep\nsporadic - (rarely/never)\n0.292\n0.099\n28\n2.965\n0.029\n\n\nRep - Other\nsporadic - (rarely/never)\n0.805\n0.109\n28\n7.404\n0.000\n\n\nIndep - Dem\nalways - (rarely/never)\n-0.529\n0.101\n28\n-5.255\n0.000\n\n\nOther - Dem\nalways - (rarely/never)\n-1.315\n0.125\n28\n-10.508\n0.000\n\n\nOther - Indep\nalways - (rarely/never)\n-0.786\n0.129\n28\n-6.072\n0.000\n\n\nRep - Dem\nalways - (rarely/never)\n-0.153\n0.104\n28\n-1.470\n0.468\n\n\nRep - Indep\nalways - (rarely/never)\n0.376\n0.104\n28\n3.605\n0.006\n\n\nRep - Other\nalways - (rarely/never)\n1.162\n0.130\n28\n8.969\n0.000\n\n\n\n\n\nCode\n#This analysis shows that the differences in predicted probabilities across political groups are statistically significant. For instance, respondents with a Democrat or Independent affiliation may have higher odds of being consistent voters compared to those with Republican or Other affiliations. These differences are confirmed by significant pairwise contrasts.\n\n\n\n\nDifferences between education level and voting behavior - Emmeans\nLast part of the assignment: Interpret the results from running the following code for your model\n\n\nCode\nmultinomial_analysis &lt;- emmeans(model_with_pol, ~ educ | voter_category)\n\ncoefs &lt;- contrast(regrid(multinomial_analysis, \"log\"), \"trt.vs.ctrl1\", by = \"educ\")\n\nupdate(coefs, by = \"contrast\") %&gt;% \n  kable(format = \"markdown\", digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontrast\neduc\nestimate\nSE\ndf\nt.ratio\np.value\n\n\n\n\nsporadic - (rarely/never)\nCollege\n0.986\n0.076\n28\n12.904\n0.000\n\n\nalways - (rarely/never)\nCollege\n0.477\n0.080\n28\n5.960\n0.000\n\n\nsporadic - (rarely/never)\nHigh school or less\n0.187\n0.069\n28\n2.705\n0.031\n\n\nalways - (rarely/never)\nHigh school or less\n-0.711\n0.080\n28\n-8.883\n0.000\n\n\nsporadic - (rarely/never)\nSome college\n0.707\n0.074\n28\n9.512\n0.000\n\n\nalways - (rarely/never)\nSome college\n0.167\n0.079\n28\n2.114\n0.112\n\n\n\n\n\nCode\ncontrast(coefs, \"revpairwise\", by = \"contrast\") %&gt;%\n  kable(format = \"markdown\", digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontrast1\ncontrast\nestimate\nSE\ndf\nt.ratio\np.value\n\n\n\n\nHigh school or less - College\nsporadic - (rarely/never)\n-0.799\n0.095\n28\n-8.416\n0.000\n\n\nSome college - College\nsporadic - (rarely/never)\n-0.278\n0.092\n28\n-3.030\n0.014\n\n\nSome college - High school or less\nsporadic - (rarely/never)\n0.520\n0.088\n28\n5.920\n0.000\n\n\nHigh school or less - College\nalways - (rarely/never)\n-1.188\n0.104\n28\n-11.394\n0.000\n\n\nSome college - College\nalways - (rarely/never)\n-0.310\n0.097\n28\n-3.207\n0.009\n\n\nSome college - High school or less\nalways - (rarely/never)\n0.878\n0.098\n28\n8.995\n0.000\n\n\n\n\n\nEnter your interpretation here: The emmeans analysis for education level indicates significant differences in voting behavior across education groups. In this model, higher educational attainment is associated with higher predicted probabilities of being a consistent voter (“always”), while lower education levels are linked to increased probabilities of being sporadic or rarely/never voters. And these differences are statistically significant. The pairwise comparisons further confirm that adjacent education levels differ meaningfully in their impact on voting frequency, suggesting that educational attainment is an important predictor of voting behavior."
  },
  {
    "objectID": "assignments/multinomial_regression.html",
    "href": "assignments/multinomial_regression.html",
    "title": "Multinomidal Regression Assignment",
    "section": "",
    "text": "Lab Goal: Predict voting frequency using demographic variables Data source: FiveThirtyEight “Why Many Americans Don’t Vote” survey Method: Multinomial logistic regression"
  },
  {
    "objectID": "assignments/multinomial_regression.html#data",
    "href": "assignments/multinomial_regression.html#data",
    "title": "Multinomidal Regression Assignment",
    "section": "Data",
    "text": "Data\nThe data for this assignment comes from an online Ipsos survey that was conducted for the FiveThirtyEight article “Why Many Americans Don’t Vote”. You can read more about the survey design and respondents in the README of the GitHub repo for the data.\nRespondents were asked a variety of questions about their political beliefs, thoughts on multiple issues, and voting behavior. We will focus on using the demographic variables and someone’s party identification to understand whether a person is a probable voter.\nThe variables we’ll focus on were (definitions from the codebook in data set GitHub repo):\n\nppage: Age of respondent\neduc: Highest educational attainment category.\n\nrace: Race of respondent, census categories. Note: all categories except Hispanic were non-Hispanic.\ngender: Gender of respondent\nincome_cat: Household income category of respondent\nQ30: Response to the question “Generally speaking, do you think of yourself as a…”\n\n1: Republican\n2: Democrat\n3: Independent\n4: Another party, please specify\n5: No preference\n-1: No response\n\nvoter_category: past voting behavior:\n\nalways: respondent voted in all or all-but-one of the elections they were eligible in\nsporadic: respondent voted in at least two, but fewer than all-but-one of the elections they were eligible in\nrarely/never: respondent voted in 0 or 1 of the elections they were eligible in\n\n\nYou can read in the data directly from the GitHub repo:\n\n\nCode\nlibrary(nnet)\nlibrary(car)\nlibrary(tidyverse)\nlibrary(emmeans)\nlibrary(ggeffects)\nlibrary(knitr)\nlibrary(patchwork)\nlibrary(broom)\nlibrary(parameters)\nlibrary(easystats)\n\n\n\n\nCode\nvoter_data &lt;- read_csv(\"https://raw.githubusercontent.com/fivethirtyeight/data/master/non-voters/nonvoters_data.csv\")"
  },
  {
    "objectID": "assignments/multinomial_regression.html#lrt",
    "href": "assignments/multinomial_regression.html#lrt",
    "title": "Multinomidal Regression Assignment",
    "section": "LRT",
    "text": "LRT\n\nRun the full model and report overall significance of each of the terms\n\n\n\nCode\nlibrary(car)\nAnova(model_with_pol, type = \"II\")\n\n\nAnalysis of Deviance Table (Type II tests)\n\nResponse: voter_category\n              LR Chisq Df Pr(&gt;Chisq)    \nppage_c         638.30  2  &lt; 2.2e-16 ***\nrace             52.65  6  1.379e-09 ***\ngender            6.03  2     0.0491 *  \nincome_cat       67.72  6  1.198e-12 ***\neduc            154.14  4  &lt; 2.2e-16 ***\npol_ident_new   153.84  6  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "assignments/multinomial_regression.html#marginal-effects-political-group---emmeans",
    "href": "assignments/multinomial_regression.html#marginal-effects-political-group---emmeans",
    "title": "Multinomidal Regression Assignment",
    "section": "Marginal Effects Political Group - Emmeans",
    "text": "Marginal Effects Political Group - Emmeans\n\n\nCode\n#Get estimated marginal means from the model\n\nlibrary(emmeans)\n\nmultinomial_analysis &lt;- emmeans(model_with_pol, ~ pol_ident_new | voter_category)\n\ncoefs &lt;- contrast(regrid(multinomial_analysis, \"log\"), \"trt.vs.ctrl1\", by = \"pol_ident_new\")\n\nupdate(coefs, by = \"contrast\") %&gt;% \n  kable(format = \"markdown\", digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontrast\npol_ident_new\nestimate\nSE\ndf\nt.ratio\np.value\n\n\n\n\nsporadic - (rarely/never)\nDem\n0.961\n0.070\n28\n13.722\n0.000\n\n\nalways - (rarely/never)\nDem\n0.480\n0.074\n28\n6.498\n0.000\n\n\nsporadic - (rarely/never)\nIndep\n0.591\n0.077\n28\n7.643\n0.000\n\n\nalways - (rarely/never)\nIndep\n-0.049\n0.084\n28\n-0.590\n0.900\n\n\nsporadic - (rarely/never)\nOther\n0.078\n0.087\n28\n0.902\n0.747\n\n\nalways - (rarely/never)\nOther\n-0.835\n0.110\n28\n-7.577\n0.000\n\n\nsporadic - (rarely/never)\nRep\n0.883\n0.084\n28\n10.469\n0.000\n\n\nalways - (rarely/never)\nRep\n0.327\n0.089\n28\n3.672\n0.004"
  },
  {
    "objectID": "assignments/multinomial_regression.html#marginal-effects-of-education---emmeans",
    "href": "assignments/multinomial_regression.html#marginal-effects-of-education---emmeans",
    "title": "Multinomidal Regression Assignment",
    "section": "Marginal Effects of Education - Emmeans",
    "text": "Marginal Effects of Education - Emmeans\n\n\nCode\nmulti_an_educ &lt;- emmeans(model_with_pol, ~ educ | voter_category)\n\ncoefs_educ &lt;- contrast(regrid(multi_an_educ, \"log\"), \"trt.vs.ctrl1\", by = \"educ\")\nupdate(coefs_educ, by = \"contrast\") %&gt;% \n  kable(format = \"markdown\", digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontrast\neduc\nestimate\nSE\ndf\nt.ratio\np.value\n\n\n\n\nsporadic - (rarely/never)\nCollege\n0.986\n0.076\n28\n12.904\n0.000\n\n\nalways - (rarely/never)\nCollege\n0.477\n0.080\n28\n5.960\n0.000\n\n\nsporadic - (rarely/never)\nHigh school or less\n0.187\n0.069\n28\n2.705\n0.031\n\n\nalways - (rarely/never)\nHigh school or less\n-0.711\n0.080\n28\n-8.883\n0.000\n\n\nsporadic - (rarely/never)\nSome college\n0.707\n0.074\n28\n9.512\n0.000\n\n\nalways - (rarely/never)\nSome college\n0.167\n0.079\n28\n2.114\n0.112\n\n\n\n\n\n\nNext, plot the predicted probabilities of voter category as a function of Age and Party ID\n\n\n\nCode\nlibrary(ggeffects)\nlibrary(ggplot2)\n\n# Get predicted probabilities for age and party identification\ndf_preds &lt;- ggemmeans(model_with_pol, terms = c(\"ppage_c\", \"pol_ident_new [all]\"))\n\nggplot(df_preds, aes(x = x, y = predicted, fill = response.level)) +\n  geom_area() +\n  geom_rug(sides = \"b\", position = \"jitter\", alpha = 0.5) +\n  labs(\n    x = \"Centered Age\",\n    y = \"Predicted Probability\",\n    title = \"Predicted Probabilities of Voting Frequency\\nby Age and Party Identification\",\n    fill = \"Voter Category\"\n  ) +\n  scale_fill_manual(\n    values = c(\n      \"always\" = \"#F6B533\", \n      \"sporadic\" = \"#D07EA2\", \n      \"rarely/never\" = \"#9854F7\"\n    ),\n    labels = c(\"ALMOST ALWAYS VOTE\", \"SOMETIMES VOTE\", \"RARELY OR NEVER VOTE\")\n  ) +\n  facet_wrap(~group) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nPlot predicted probabilities as a function of education and voting frequency.\n\n\nCode\nggemmeans(model_with_pol, terms = \"educ\") %&gt;%\n  ggplot(aes(x = x, y = predicted, fill = response.level)) +\n  geom_bar(stat = \"identity\", position = \"fill\") +\n  labs(x = \"Education\", y = \"Predicted Probability\", \n       title = \"Predicted Probabilities of Voting Frequency by Education\") +\n  scale_fill_manual(\n    name = \"Voter Category\",\n    values = c(\"always\" = \"#F6B533\", \"sporadic\" = \"#D07EA2\", \"rarely/never\" = \"#9854F7\"),\n    labels = c(\"ALMOST ALWAYS VOTE\", \"SOMETIMES VOTE\", \"RARELY OR NEVER VOTE\")\n  ) +\n    coord_flip() +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n### Write-up\n\nDifferences between political groups and voting behavior - Emmeans\n\n\nCode\nmultinomial_analysis &lt;- emmeans(model_with_pol, ~ pol_ident_new | voter_category)\n\ncoefs &lt;- contrast(regrid(multinomial_analysis, \"log\"), \"trt.vs.ctrl1\", by = \"pol_ident_new\")\n\nupdate(coefs, by = \"contrast\") %&gt;% \n  kable(format = \"markdown\", digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontrast\npol_ident_new\nestimate\nSE\ndf\nt.ratio\np.value\n\n\n\n\nsporadic - (rarely/never)\nDem\n0.961\n0.070\n28\n13.722\n0.000\n\n\nalways - (rarely/never)\nDem\n0.480\n0.074\n28\n6.498\n0.000\n\n\nsporadic - (rarely/never)\nIndep\n0.591\n0.077\n28\n7.643\n0.000\n\n\nalways - (rarely/never)\nIndep\n-0.049\n0.084\n28\n-0.590\n0.900\n\n\nsporadic - (rarely/never)\nOther\n0.078\n0.087\n28\n0.902\n0.747\n\n\nalways - (rarely/never)\nOther\n-0.835\n0.110\n28\n-7.577\n0.000\n\n\nsporadic - (rarely/never)\nRep\n0.883\n0.084\n28\n10.469\n0.000\n\n\nalways - (rarely/never)\nRep\n0.327\n0.089\n28\n3.672\n0.004\n\n\n\n\n\nCode\n# Pairwise comparisons (reverse pairwise contrasts)\ncontrast(coefs, \"revpairwise\", by = \"contrast\") %&gt;%\n  kable(format = \"markdown\", digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontrast1\ncontrast\nestimate\nSE\ndf\nt.ratio\np.value\n\n\n\n\nIndep - Dem\nsporadic - (rarely/never)\n-0.370\n0.094\n28\n-3.933\n0.003\n\n\nOther - Dem\nsporadic - (rarely/never)\n-0.883\n0.103\n28\n-8.578\n0.000\n\n\nOther - Indep\nsporadic - (rarely/never)\n-0.513\n0.107\n28\n-4.807\n0.000\n\n\nRep - Dem\nsporadic - (rarely/never)\n-0.078\n0.099\n28\n-0.787\n0.860\n\n\nRep - Indep\nsporadic - (rarely/never)\n0.292\n0.099\n28\n2.965\n0.029\n\n\nRep - Other\nsporadic - (rarely/never)\n0.805\n0.109\n28\n7.404\n0.000\n\n\nIndep - Dem\nalways - (rarely/never)\n-0.529\n0.101\n28\n-5.255\n0.000\n\n\nOther - Dem\nalways - (rarely/never)\n-1.315\n0.125\n28\n-10.508\n0.000\n\n\nOther - Indep\nalways - (rarely/never)\n-0.786\n0.129\n28\n-6.072\n0.000\n\n\nRep - Dem\nalways - (rarely/never)\n-0.153\n0.104\n28\n-1.470\n0.468\n\n\nRep - Indep\nalways - (rarely/never)\n0.376\n0.104\n28\n3.605\n0.006\n\n\nRep - Other\nalways - (rarely/never)\n1.162\n0.130\n28\n8.969\n0.000\n\n\n\n\n\nCode\n#This analysis shows that the differences in predicted probabilities across political groups are statistically significant. For instance, respondents with a Democrat or Independent affiliation may have higher odds of being consistent voters compared to those with Republican or Other affiliations. These differences are confirmed by significant pairwise contrasts.\n\n\n\n\nDifferences between education level and voting behavior - Emmeans\nLast part of the assignment: Interpret the results from running the following code for your model\n\n\nCode\nmultinomial_analysis &lt;- emmeans(model_with_pol, ~ educ | voter_category)\n\ncoefs &lt;- contrast(regrid(multinomial_analysis, \"log\"), \"trt.vs.ctrl1\", by = \"educ\")\n\nupdate(coefs, by = \"contrast\") %&gt;% \n  kable(format = \"markdown\", digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontrast\neduc\nestimate\nSE\ndf\nt.ratio\np.value\n\n\n\n\nsporadic - (rarely/never)\nCollege\n0.986\n0.076\n28\n12.904\n0.000\n\n\nalways - (rarely/never)\nCollege\n0.477\n0.080\n28\n5.960\n0.000\n\n\nsporadic - (rarely/never)\nHigh school or less\n0.187\n0.069\n28\n2.705\n0.031\n\n\nalways - (rarely/never)\nHigh school or less\n-0.711\n0.080\n28\n-8.883\n0.000\n\n\nsporadic - (rarely/never)\nSome college\n0.707\n0.074\n28\n9.512\n0.000\n\n\nalways - (rarely/never)\nSome college\n0.167\n0.079\n28\n2.114\n0.112\n\n\n\n\n\nCode\ncontrast(coefs, \"revpairwise\", by = \"contrast\") %&gt;%\n  kable(format = \"markdown\", digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontrast1\ncontrast\nestimate\nSE\ndf\nt.ratio\np.value\n\n\n\n\nHigh school or less - College\nsporadic - (rarely/never)\n-0.799\n0.095\n28\n-8.416\n0.000\n\n\nSome college - College\nsporadic - (rarely/never)\n-0.278\n0.092\n28\n-3.030\n0.014\n\n\nSome college - High school or less\nsporadic - (rarely/never)\n0.520\n0.088\n28\n5.920\n0.000\n\n\nHigh school or less - College\nalways - (rarely/never)\n-1.188\n0.104\n28\n-11.394\n0.000\n\n\nSome college - College\nalways - (rarely/never)\n-0.310\n0.097\n28\n-3.207\n0.009\n\n\nSome college - High school or less\nalways - (rarely/never)\n0.878\n0.098\n28\n8.995\n0.000\n\n\n\n\n\nEnter your interpretation here: The emmeans analysis for education level indicates significant differences in voting behavior across education groups. In this model, higher educational attainment is associated with higher predicted probabilities of being a consistent voter (“always”), while lower education levels are linked to increased probabilities of being sporadic or rarely/never voters. And these differences are statistically significant. The pairwise comparisons further confirm that adjacent education levels differ meaningfully in their impact on voting frequency, suggesting that educational attainment is an important predictor of voting behavior."
  },
  {
    "objectID": "assignments/poisson_regression.html",
    "href": "assignments/poisson_regression.html",
    "title": "Poisson Regression",
    "section": "",
    "text": "To complete this lab:\nCode\nlibrary(MASS)\nlibrary(tidyverse)\nlibrary(emmeans)\nlibrary(ggeffects)\nlibrary(easystats)\nlibrary(performance)\nlibrary(knitr)\nCode\nlibrary(tidyverse)\n\ndata &lt;- read_delim(\"https://raw.githubusercontent.com/jgeller112/psy504-advanced-stats/main/slides/Poisson/data/2010.csv\")\nCode\nlibrary(naniar)\n\ndata_pos &lt;- data %&gt;%\n  dplyr::select(wwwhr, wordsum, age, sex, reliten, polviews, wrkhome) %&gt;%\nreplace_with_na(.,\n             replace = list(wwwhr = c(-1, 998, 999),\n                          wordsum = c(-1, 99),\n                          reliten = c(0, 8, 9), \n             polviews = c(0, 8, 9), \n             wrkhome = c(0,8,9), \n             age=c(0, 98, 99)))\nQ: Can you explain what might be going on in the above code?\nA: The code first selects the variables of interest (wwwhr, wordsum, age, sex, reliten, polviews, and wrkhome) from the dataset. Then, using the replace_with_na function from the naniar package, it recodes certain placeholder values (e.g., -1, 998, 999 for wwwhr) as NA. These values likely represent missing, invalid, or out-of-range responses in the original data.\nQ: The next step in data cleaning would be to ensure that the data in your code are aligned with the description/ usage context of the variables\nCode\ndata_pos &lt;- data_pos |&gt; \n  mutate(sex = factor(ifelse(sex == -1, \"Male\", \n                             ifelse(sex == 1, \"Female\", NA)), \n                      levels = c(\"Male\", \"Female\")),\n         reliten_recode = factor(reliten, levels = 1:5))"
  },
  {
    "objectID": "assignments/poisson_regression.html#missingness",
    "href": "assignments/poisson_regression.html#missingness",
    "title": "Poisson Regression",
    "section": "Missingness",
    "text": "Missingness\n\n\nCode\ndata_pos %&gt;%\n  dplyr::select(reliten, reliten_recode)\n\n\n# A tibble: 2,044 × 2\n   reliten reliten_recode\n     &lt;dbl&gt; &lt;fct&gt;         \n 1       1 1             \n 2       4 4             \n 3       1 1             \n 4       1 1             \n 5       1 1             \n 6       4 4             \n 7       3 3             \n 8       1 1             \n 9       1 1             \n10       1 1             \n# ℹ 2,034 more rows\n\n\nCode\nlibrary(skimr)\nskimr::skim(data_pos)\n\n\n\nData summary\n\n\nName\ndata_pos\n\n\nNumber of rows\n2044\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nsex\n0\n1.00\nFALSE\n2\nMal: 1153, Fem: 891\n\n\nreliten_recode\n99\n0.95\nFALSE\n4\n2: 747, 1: 707, 4: 363, 3: 128\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nwwwhr\n996\n0.51\n9.79\n13.41\n0\n2\n5\n14\n168\n▇▁▁▁▁\n\n\nwordsum\n657\n0.68\n6.03\n2.07\n0\n5\n6\n7\n10\n▁▃▇▅▂\n\n\nage\n3\n1.00\n47.97\n17.68\n18\n33\n47\n61\n89\n▇▇▇▅▃\n\n\nreliten\n99\n0.95\n2.08\n1.08\n1\n1\n2\n3\n4\n▇▇▁▂▃\n\n\npolviews\n71\n0.97\n4.08\n1.46\n1\n3\n4\n5\n7\n▃▂▇▃▅\n\n\nwrkhome\n882\n0.57\n2.26\n1.72\n1\n1\n1\n4\n6\n▇▁▁▂▁"
  },
  {
    "objectID": "assignments/poisson_regression.html#fit-a-poisson-model-to-the-data.",
    "href": "assignments/poisson_regression.html#fit-a-poisson-model-to-the-data.",
    "title": "Poisson Regression",
    "section": "Fit a Poisson model to the data.",
    "text": "Fit a Poisson model to the data.\n\n\nCode\npoisson_model &lt;- glm(wwwhr ~ wordsum + age + sex + reliten + polviews + wrkhome,\n                    data = data_pos,\n                    family = poisson(link = \"log\"))\n\nsummary(poisson_model)\n\n\n\nCall:\nglm(formula = wwwhr ~ wordsum + age + sex + reliten + polviews + \n    wrkhome, family = poisson(link = \"log\"), data = data_pos)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  1.649224   0.081544  20.225  &lt; 2e-16 ***\nwordsum      0.099524   0.007754  12.836  &lt; 2e-16 ***\nage         -0.016456   0.001087 -15.132  &lt; 2e-16 ***\nsexFemale    0.259312   0.026363   9.836  &lt; 2e-16 ***\nreliten      0.198934   0.011894  16.726  &lt; 2e-16 ***\npolviews    -0.035508   0.009687  -3.666 0.000247 ***\nwrkhome      0.078441   0.007658  10.243  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 7666.1  on 602  degrees of freedom\nResidual deviance: 6481.8  on 596  degrees of freedom\n  (1441 observations deleted due to missingness)\nAIC: 8540\n\nNumber of Fisher Scoring iterations: 5"
  },
  {
    "objectID": "assignments/poisson_regression.html#carry-out-model-checking",
    "href": "assignments/poisson_regression.html#carry-out-model-checking",
    "title": "Poisson Regression",
    "section": "Carry out model checking",
    "text": "Carry out model checking\nHint: performance package has the function you’re looking for\n\n\nCode\npoisson_model &lt;- glm(wwwhr ~ wordsum + age + sex + reliten + polviews + wrkhome,\n                    data = data_pos,\n                    family = poisson(link = \"log\"))\n\n# Check overall model performance\ncheck_model(poisson_model)\n\n\n\n\n\n\n\n\n\nCode\n# Check for specific issues\nmodel_performance(poisson_model)\n\n\n# Indices of model performance\n\nAIC      |     AICc |      BIC | Nagelkerke's R2 |   RMSE | Sigma | Score_log | Score_spherical\n-----------------------------------------------------------------------------------------------\n8540.043 | 8540.231 | 8570.856 |           0.860 | 13.251 | 1.000 |    -7.070 |           0.027"
  },
  {
    "objectID": "assignments/poisson_regression.html#find-any-outliers",
    "href": "assignments/poisson_regression.html#find-any-outliers",
    "title": "Poisson Regression",
    "section": "Find any outliers",
    "text": "Find any outliers\n\n\nCode\n# Identify outliers using Cook's distance\ncooksd &lt;- cooks.distance(poisson_model)\nplot(cooksd, pch = 20, main = \"Cook's Distance\")\n# Define a threshold (e.g., 4/n)\nthreshold &lt;- 4 / nrow(data_pos)\nabline(h = threshold, col = \"red\", lty = 2)\n\n\n\n\n\n\n\n\n\nCode\noutlier_indices &lt;- which(cooksd &gt; threshold)\ncat(\"Number of potential outliers (Cook's distance):\", length(outlier_indices), \"\\n\")\n\n\nNumber of potential outliers (Cook's distance): 423 \n\n\nCode\nstudent_resid &lt;- rstudent(poisson_model)\nplot(student_resid, pch = 20, main = \"Studentized Residuals\")\nabline(h = c(-3, 3), col = \"red\", lty = 2)\n\n\n\n\n\n\n\n\n\nCode\noutlier_indices2 &lt;- which(abs(student_resid) &gt; 3)\ncat(\"Number of potential outliers (Studentized residuals):\", length(outlier_indices2), \"\\n\")\n\n\nNumber of potential outliers (Studentized residuals): 206 \n\n\nCode\nall_outlier_indices &lt;- unique(c(outlier_indices, outlier_indices2))\ncat(\"Total number of outliers identified:\", length(all_outlier_indices), \"\\n\")\n\n\nTotal number of outliers identified: 423"
  },
  {
    "objectID": "assignments/poisson_regression.html#refit-the-model-after-excludint-outliers",
    "href": "assignments/poisson_regression.html#refit-the-model-after-excludint-outliers",
    "title": "Poisson Regression",
    "section": "Refit the model after excludint outliers",
    "text": "Refit the model after excludint outliers\n\n\nCode\ndata_no_outliers &lt;- data_pos[-all_outlier_indices, ]\n\npoisson_model_refit &lt;- glm(wwwhr ~ wordsum + age + sex + reliten + polviews + wrkhome,\n                           data = data_no_outliers,\n                           family = poisson(link = \"log\"))\nsummary(poisson_model_refit)\n\n\n\nCall:\nglm(formula = wwwhr ~ wordsum + age + sex + reliten + polviews + \n    wrkhome, family = poisson(link = \"log\"), data = data_no_outliers)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  1.728885   0.093087  18.573  &lt; 2e-16 ***\nwordsum      0.100971   0.008809  11.462  &lt; 2e-16 ***\nage         -0.014702   0.001235 -11.905  &lt; 2e-16 ***\nsexFemale    0.288499   0.029974   9.625  &lt; 2e-16 ***\nreliten      0.181474   0.013607  13.337  &lt; 2e-16 ***\npolviews    -0.049475   0.010899  -4.539 5.64e-06 ***\nwrkhome      0.052020   0.008618   6.036 1.58e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 5798.6  on 472  degrees of freedom\nResidual deviance: 4979.0  on 466  degrees of freedom\n  (1148 observations deleted due to missingness)\nAIC: 6595.4\n\nNumber of Fisher Scoring iterations: 5\n\n\nCode\nmodel_parameters(poisson_model_refit) %&gt;%\n  print_html()\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nz\np\n\n\n\n\n(Intercept)\n1.73\n0.09\n(1.55, 1.91)\n18.57\n&lt; .001\n\n\nwordsum\n0.10\n8.81e-03\n(0.08, 0.12)\n11.46\n&lt; .001\n\n\nage\n-0.01\n1.23e-03\n(-0.02, -0.01)\n-11.91\n&lt; .001\n\n\nsex (Female)\n0.29\n0.03\n(0.23, 0.35)\n9.63\n&lt; .001\n\n\nreliten\n0.18\n0.01\n(0.15, 0.21)\n13.34\n&lt; .001\n\n\npolviews\n-0.05\n0.01\n(-0.07, -0.03)\n-4.54\n&lt; .001\n\n\nwrkhome\n0.05\n8.62e-03\n(0.04, 0.07)\n6.04\n&lt; .001\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck for Overdispersion\nHint: performance package has the function you’re looking for\n\n\nCode\noverdispersion_result &lt;- check_overdispersion(poisson_model_refit)\nprint(overdispersion_result)\n\n\n# Overdispersion test\n\n       dispersion ratio =   14.254\n  Pearson's Chi-Squared = 6642.165\n                p-value =  &lt; 0.001\n\n\nWhat do you notice? And what’s a good next step forward? Can there be another model class that can fit the data? If so, fit this model to the data.\nThe dispersion ratio is 14.254 with a p-value &lt; 0.001. This indicates that the variance is far larger than the mean, a clear sign of overdispersion in the Poisson model. Because the Poisson model assumes that the variance equals the mean, this overdispersion violates that assumption and suggests that the Poisson model isn’t the best fit for the data.\nA good next step is to try an alternative model that can handle overdispersion. I’ll fit a negative binomial model as it includes an extra parameter to account for the extra variance.\n\n\nCode\nnegbin_model &lt;- glm.nb(wwwhr ~ wordsum + age + sex + reliten + polviews + wrkhome,\n                       data = data_no_outliers)\nsummary(negbin_model)\n\n\n\nCall:\nglm.nb(formula = wwwhr ~ wordsum + age + sex + reliten + polviews + \n    wrkhome, data = data_no_outliers, init.theta = 0.9603288526, \n    link = log)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  1.714583   0.309241   5.544 2.95e-08 ***\nwordsum      0.113863   0.029625   3.844 0.000121 ***\nage         -0.014388   0.004061  -3.543 0.000396 ***\nsexFemale    0.188969   0.101297   1.865 0.062111 .  \nreliten      0.180424   0.047557   3.794 0.000148 ***\npolviews    -0.045450   0.037305  -1.218 0.223101    \nwrkhome      0.031508   0.030104   1.047 0.295262    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(0.9603) family taken to be 1)\n\n    Null deviance: 597.14  on 472  degrees of freedom\nResidual deviance: 530.03  on 466  degrees of freedom\n  (1148 observations deleted due to missingness)\nAIC: 3101.3\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  0.9603 \n          Std. Err.:  0.0673 \n\n 2 x log-likelihood:  -3085.2970"
  },
  {
    "objectID": "assignments/poisson_regression.html#which-one-is-better--your-earlier-model-or-later-model",
    "href": "assignments/poisson_regression.html#which-one-is-better--your-earlier-model-or-later-model",
    "title": "Poisson Regression",
    "section": "Which one is better- your earlier model, or later model?",
    "text": "Which one is better- your earlier model, or later model?\n\n\nCode\nAIC_comparison &lt;- AIC(poisson_model_refit, negbin_model)\nprint(AIC_comparison)\n\n\n                    df      AIC\npoisson_model_refit  7 6595.358\nnegbin_model         8 3101.297\n\n\nCode\n#Based on the AIC results, the negative binomial model (AIC = 3101.297) is the better fit compared to the poisson model (AIC = 6595.358)."
  },
  {
    "objectID": "assignments/poisson_regression.html#what-is-zero-inflation-is-there-zero-inflation-in-your-chosen-model",
    "href": "assignments/poisson_regression.html#what-is-zero-inflation-is-there-zero-inflation-in-your-chosen-model",
    "title": "Poisson Regression",
    "section": "What is zero inflation? Is there zero-inflation in your chosen model?",
    "text": "What is zero inflation? Is there zero-inflation in your chosen model?\n\n\nCode\n# Zero inflation occurs when there are more zero counts than expected under the assumed distribution.\nlibrary(performance)\n\nperformance::check_zeroinflation(negbin_model)\n\n\n# Check for zero-inflation\n\n   Observed zeros: 35\n  Predicted zeros: 52\n            Ratio: 1.50\n\n\n\nLog LambdaMean Count\n\n\n\n\nCode\nlog_lambda &lt;- ggeffect(negbin_model, terms = \"wordsum\")\nplot(log_lambda) +\n  labs(title = \"Predicted Log Lambda vs. WORDSUM\",\n       x = \"WORDSUM\",\n       y = \"Log Lambda\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nmean_count &lt;- ggeffect(negbin_model, terms = \"wordsum\")\nplot(mean_count) +\n  labs(title = \"Predicted Mean Count (WWWHR) vs. WORDSUM\",\n       x = \"WORDSUM\",\n       y = \"Predicted WWWHR\")"
  },
  {
    "objectID": "assignments/poisson_regression.html#report-your-conclusions",
    "href": "assignments/poisson_regression.html#report-your-conclusions",
    "title": "Poisson Regression",
    "section": "Report your conclusions",
    "text": "Report your conclusions\nIn this analysis, we started by cleaning the data and recoding the key variables according to our preregistration. After handling missing values and identifying outliers, we fit a poisson regression model to predict the number of hours per week spent on the Internet (WWWHR). Diagnostic checks revealed significant overdispersion (dispersion ratio &gt; 14, p &lt; 0.001). We then tried to fit a negative binomial model.\nModel comparisons showed that the negative binomial model (AIC ≈ 3101.3) substantially outperformed the poisson model (AIC ≈ 6595.4). In addition, the predicted values from the negative binomial model—visualized through both the log lambda and the expected mean count—illustrate a clear relationship between verbal ability (WORDSUM) and internet usage. The zero-inflation check using performance::check_zeroinflation(negbin_model) indicated no evidence of problematic excess zeros.\nOverall, the negative binomial model provides a better fit to the data by accommodating the observed overdispersion. This model supports our preregistered hypothesis that factors such as vocabulary, age, sex, religiosity, political orientation, and work-from-home frequency significantly predict weekly internet hours. Future research might explore further refinements or alternative models if additional complexities (e.g.potential zero-inflation) are detected."
  },
  {
    "objectID": "assignments/multilevel_modeling_walkthrough/multilevel_modeling_walkthrough.html",
    "href": "assignments/multilevel_modeling_walkthrough/multilevel_modeling_walkthrough.html",
    "title": "Multilevel Modeling Walkthrough",
    "section": "",
    "text": "New Packages!\n\n\n\n\n\nThese are the main packages we’re going to use in this block. It might make sense to install them now if you do not have them already\n\ntidyverse : for organising data\n\nlme4 : for fitting generalised linear mixed effects models\nbroom.mixed : tidying methods for mixed models\neffects : for tabulating and graphing effects in linear models\nlmerTest: for quick p-values from mixed models\nparameters: various inferential methods for mixed models"
  },
  {
    "objectID": "assignments/multilevel_modeling_walkthrough/multilevel_modeling_walkthrough.html#footnotes",
    "href": "assignments/multilevel_modeling_walkthrough/multilevel_modeling_walkthrough.html#footnotes",
    "title": "Multilevel Modeling Walkthrough",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nImage sources:http://tophatsasquatch.com/2012-tmnt-classics-action-figures/https://www.dezeen.com/2016/02/01/barbie-dolls-fashionista-collection-mattel-new-body-types/https://www.wish.com/product/5da9bc544ab36314cfa7f70chttps://www.worldwideshoppingmall.co.uk/toys/jumbo-farm-animals.asphttps://www.overstock.com/Sports-Toys/NJ-Croce-Scooby-Doo-5pc.-Bendable-Figure-Set-with-Scooby-Doo-Shaggy-Daphne-Velma-and-Fred/28534567/product.htmlhttps://tvtropes.org/pmwiki/pmwiki.php/Toys/Furbyhttps://www.fun.com/toy-story-4-figure-4-pack.htmlhttps://www.johnlewis.com/lego-minifigures-71027-series-20-pack/p5079461↩︎"
  },
  {
    "objectID": "assignments/Multilevel Modeling Walkthrough/multilevel_modeling_walkthrough.html",
    "href": "assignments/Multilevel Modeling Walkthrough/multilevel_modeling_walkthrough.html",
    "title": "Multilevel Modeling Walkthrough",
    "section": "",
    "text": "New Packages!\n\n\n\n\n\nThese are the main packages we’re going to use in this block. It might make sense to install them now if you do not have them already\n\ntidyverse : for organising data\n\nlme4 : for fitting generalised linear mixed effects models\nbroom.mixed : tidying methods for mixed models\neffects : for tabulating and graphing effects in linear models\nlmerTest: for quick p-values from mixed models\nparameters: various inferential methods for mixed models"
  },
  {
    "objectID": "assignments/Multilevel Modeling Walkthrough/multilevel_modeling_walkthrough.html#footnotes",
    "href": "assignments/Multilevel Modeling Walkthrough/multilevel_modeling_walkthrough.html#footnotes",
    "title": "Multilevel Modeling Walkthrough",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nImage sources:http://tophatsasquatch.com/2012-tmnt-classics-action-figures/https://www.dezeen.com/2016/02/01/barbie-dolls-fashionista-collection-mattel-new-body-types/https://www.wish.com/product/5da9bc544ab36314cfa7f70chttps://www.worldwideshoppingmall.co.uk/toys/jumbo-farm-animals.asphttps://www.overstock.com/Sports-Toys/NJ-Croce-Scooby-Doo-5pc.-Bendable-Figure-Set-with-Scooby-Doo-Shaggy-Daphne-Velma-and-Fred/28534567/product.htmlhttps://tvtropes.org/pmwiki/pmwiki.php/Toys/Furbyhttps://www.fun.com/toy-story-4-figure-4-pack.htmlhttps://www.johnlewis.com/lego-minifigures-71027-series-20-pack/p5079461↩︎"
  },
  {
    "objectID": "assignments/Multilevel Modeling Walkthrough/MLM_Intro_Questions.html",
    "href": "assignments/Multilevel Modeling Walkthrough/MLM_Intro_Questions.html",
    "title": "Intro to MLM Exercise/Walkthrough",
    "section": "",
    "text": "New Packages!\n\n\n\n\n\nThese are the main packages we’re going to use in this block. It might make sense to install them now if you do not have them already\n\ntidyverse : for organising data\n\nlme4 : for fitting generalised linear mixed effects models\nbroom.mixed : tidying methods for mixed models\neffects : for tabulating and graphing effects in linear models\nlmerTest: for quick p-values from mixed models\nparameters: various inferential methods for mixed models"
  },
  {
    "objectID": "assignments/Multilevel Modeling Walkthrough/MLM_Intro_Questions.html#footnotes",
    "href": "assignments/Multilevel Modeling Walkthrough/MLM_Intro_Questions.html#footnotes",
    "title": "Intro to MLM Exercise/Walkthrough",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nImage sources:http://tophatsasquatch.com/2012-tmnt-classics-action-figures/https://www.dezeen.com/2016/02/01/barbie-dolls-fashionista-collection-mattel-new-body-types/https://www.wish.com/product/5da9bc544ab36314cfa7f70chttps://www.worldwideshoppingmall.co.uk/toys/jumbo-farm-animals.asphttps://www.overstock.com/Sports-Toys/NJ-Croce-Scooby-Doo-5pc.-Bendable-Figure-Set-with-Scooby-Doo-Shaggy-Daphne-Velma-and-Fred/28534567/product.htmlhttps://tvtropes.org/pmwiki/pmwiki.php/Toys/Furbyhttps://www.fun.com/toy-story-4-figure-4-pack.htmlhttps://www.johnlewis.com/lego-minifigures-71027-series-20-pack/p5079461↩︎"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html",
    "href": "assignments/multilevel_modeling2.html",
    "title": "Multilevel Modeling 2",
    "section": "",
    "text": "When to use them:\n\nNested designs\nRepeated measures\nLongitudinal data\nComplex designs\n\nWhy use them:\n\nCaptures variance occurring between groups and within groups\n\nWhat they are:\n\nLinear model with extra residuals"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#multilevel-models",
    "href": "assignments/multilevel_modeling2.html#multilevel-models",
    "title": "Multilevel Modeling 2",
    "section": "",
    "text": "When to use them:\n\nNested designs\nRepeated measures\nLongitudinal data\nComplex designs\n\nWhy use them:\n\nCaptures variance occurring between groups and within groups\n\nWhat they are:\n\nLinear model with extra residuals"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#today",
    "href": "assignments/multilevel_modeling2.html#today",
    "title": "Multilevel Modeling 2",
    "section": "Today",
    "text": "Today\n\nEverything you need to know to run and report a MLM\n\nOrganizing data for MLM analysis\nEstimation\nFit and interpret multilevel models\nVisualization\nEffect size\nReporting\nPower"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#packages",
    "href": "assignments/multilevel_modeling2.html#packages",
    "title": "Multilevel Modeling 2",
    "section": "Packages",
    "text": "Packages\n\n\nCode\nlibrary(tidyverse) # data wrangling\nlibrary(knitr) # nice tables\nlibrary(lme4) # fit mixed models\nlibrary(lmerTest) # mixed models\nlibrary(broom.mixed) # tidy output of mixed models\nlibrary(afex) # fit mixed models for lrt test\nlibrary(emmeans) # marginal means\nlibrary(ggeffects) # marginal means\nlibrary(ggrain) # rain plots\nlibrary(easystats) # nice ecosystem of packages\n\noptions(scipen=999) # get rid of sci notation\n\n\n\nFind the .qmd document here to follow along: https://github.com/suyoghc/PSY-504_Spring-2025/blob/main/Multilevel%20Modeling/mlm-02.qmd"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#todays-data",
    "href": "assignments/multilevel_modeling2.html#todays-data",
    "title": "Multilevel Modeling 2",
    "section": "Today’s data",
    "text": "Today’s data\n\nWhat did you say?\n\nPs (N = 31) listened to both clear (NS) and 6 channel vocoded speech (V6)\n\n(https://www.mrc-cbu.cam.ac.uk/personal/matt.davis/vocode/a1_6.wav)\n\nFixed factor: ?\nRandom factor: ?"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#todays-data-1",
    "href": "assignments/multilevel_modeling2.html#todays-data-1",
    "title": "Multilevel Modeling 2",
    "section": "Today’s data",
    "text": "Today’s data\n\n\nCode\neye  &lt;- read_csv(\"https://raw.githubusercontent.com/suyoghc/PSY-504_Spring-2025/refs/heads/main/Multilevel%20Modeling/data/vocoded_pupil.csv\") # data for class"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#data-organization",
    "href": "assignments/multilevel_modeling2.html#data-organization",
    "title": "Multilevel Modeling 2",
    "section": "Data organization",
    "text": "Data organization\n\nData Structure\n\nMLM analysis (in R) requires data in long format"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#data-organization-1",
    "href": "assignments/multilevel_modeling2.html#data-organization-1",
    "title": "Multilevel Modeling 2",
    "section": "Data organization",
    "text": "Data organization\n\nLevel 1: trial\nLevel 2: subject\n\n\n\n\n\n\nsubject\ntrial\nvocoded\nmean_pupil\n\n\n\n\nEYE15\n3\nV6\n0.0839555\n\n\nEYE15\n4\nV6\n0.0141083\n\n\nEYE15\n5\nV6\n0.0224967\n\n\nEYE15\n6\nV6\n0.0007424\n\n\nEYE15\n7\nV6\n0.0242540\n\n\nEYE15\n8\nV6\n0.0267617"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#centering",
    "href": "assignments/multilevel_modeling2.html#centering",
    "title": "Multilevel Modeling 2",
    "section": "Centering",
    "text": "Centering\n\n\n\nIn a single-level regression, centering ensures that the zero value for each predictor is meaningful before running the model\nIn MLM, if you have specific questions about within, between, and contextual effects, you need to center!"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#group--vs.-grand-mean-centering",
    "href": "assignments/multilevel_modeling2.html#group--vs.-grand-mean-centering",
    "title": "Multilevel Modeling 2",
    "section": "Group- vs. Grand-Mean Centering",
    "text": "Group- vs. Grand-Mean Centering\n\nGrand-mean centering: \\(x_{ij} - x\\)\n\nVariable represents each observation’s deviation from everyone’s norm, regardless of group\n\nGroup-mean centering: \\(x_{ij} - x_j\\)\n\nVariable represents each observation’s deviation from their group’s norm (removes group effect)"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#group--vs.-grand-mean-centering-1",
    "href": "assignments/multilevel_modeling2.html#group--vs.-grand-mean-centering-1",
    "title": "Multilevel Modeling 2",
    "section": "Group- vs. Grand-Mean Centering",
    "text": "Group- vs. Grand-Mean Centering\n\n\n\nLevel 1 predictors\n\nGrand-mean centering\n\nInclude means of level 2\n\nAllows us to directly test within-group effect\nCoefficient associated with the Level 2 group mean represents contextual effect\n\n\n\n\n\n\nGroup-mean centering\n\nLevel 1 coefficient will always be with within-group effect, regardless of whether the group means are included at Level 2 or not\nIf level 2 means included, coefficient represents the between-groups effect\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nCan apply to categorical predictors as well (see Yaremych, Preacher, & Hedeker, 2023)"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#centering-in-r",
    "href": "assignments/multilevel_modeling2.html#centering-in-r",
    "title": "Multilevel Modeling 2",
    "section": "Centering in R",
    "text": "Centering in R"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#maximum-likelihood",
    "href": "assignments/multilevel_modeling2.html#maximum-likelihood",
    "title": "Multilevel Modeling 2",
    "section": "Maximum Likelihood",
    "text": "Maximum Likelihood\n\n\n\n\nIn MLM we try to maximize the likelihood of the data\n\nNo OLS!"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#probability-vs.-likelihood",
    "href": "assignments/multilevel_modeling2.html#probability-vs.-likelihood",
    "title": "Multilevel Modeling 2",
    "section": "Probability vs. Likelihood",
    "text": "Probability vs. Likelihood\n\nProbability\n\n\nIf I assume a distribution with certain parameters (fixed), what is the probability I see a particular value in the data?\n\n\n\n\nPr⁡(𝑦&gt;0│𝜇=0,𝜎=1)=.50\nPr⁡(−1&lt;𝑦&lt;1│𝜇=0,𝜎=1)=.68\nPr⁡(0&lt;𝑦&lt;1│𝜇=0,𝜎=1)=.34\nPr⁡(𝑦&gt;2│𝜇=0,𝜎=1)=.02"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#likelihood",
    "href": "assignments/multilevel_modeling2.html#likelihood",
    "title": "Multilevel Modeling 2",
    "section": "Likelihood",
    "text": "Likelihood\n\n\n\n\\(L(𝜇,𝜎│𝑥)\\)\nHolding a sample of data constant, which parameter values are more likely?\n\nWhich values have higher likelihood?\n\nHere data is fixed and distribution can change"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#likelihood-1",
    "href": "assignments/multilevel_modeling2.html#likelihood-1",
    "title": "Multilevel Modeling 2",
    "section": "Likelihood",
    "text": "Likelihood\nInteractive: Understanding Maximum Likelihood Estimation: https://rpsychologist.com/likelihood/"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#likelihood-2",
    "href": "assignments/multilevel_modeling2.html#likelihood-2",
    "title": "Multilevel Modeling2",
    "section": "Likelihood",
    "text": "Likelihood"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#likelihood-3",
    "href": "assignments/multilevel_modeling2.html#likelihood-3",
    "title": "Multilevel Modeling2",
    "section": "Likelihood",
    "text": "Likelihood"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#likelihood-4",
    "href": "assignments/multilevel_modeling2.html#likelihood-4",
    "title": "Multilevel Modeling2",
    "section": "Likelihood",
    "text": "Likelihood"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#likelihood-5",
    "href": "assignments/multilevel_modeling2.html#likelihood-5",
    "title": "Multilevel Modeling2",
    "section": "Likelihood",
    "text": "Likelihood"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#likelihood-6",
    "href": "assignments/multilevel_modeling2.html#likelihood-6",
    "title": "Multilevel Modeling2",
    "section": "Likelihood",
    "text": "Likelihood\nInteractive: Understanding Maximum Likelihood Estimation: https://rpsychologist.com/likelihood/"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#log-likelihood",
    "href": "assignments/multilevel_modeling2.html#log-likelihood",
    "title": "Multilevel Modeling 2",
    "section": "Log likelihood",
    "text": "Log likelihood\n\nWith large samples, likelihood values ℒ(𝜇,𝜎│𝑥) get very small very fast\n\nTo make them easier to work with, we usually work with the log-likelihood\n\nMeasure of how well the model fits the data\nHigher values of \\(\\log L\\) are better\n\n\nDeviance = \\(-2logL\\)\n\n\\(-2logL\\) follows a \\(\\chi^2\\) distribution with \\(n (\\text{sample size}) - p (\\text{paramters}) - 1\\) degrees of freedom"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#chi2-distribution",
    "href": "assignments/multilevel_modeling2.html#chi2-distribution",
    "title": "Multilevel Modeling 2",
    "section": "\\(\\chi^2\\) distribution",
    "text": "\\(\\chi^2\\) distribution"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#comparing-nested-models",
    "href": "assignments/multilevel_modeling2.html#comparing-nested-models",
    "title": "Multilevel Modeling 2",
    "section": "Comparing nested models",
    "text": "Comparing nested models\n\nSuppose there are two models:\n\nReduced model includes predictors \\(x_1, \\ldots, x_q\\)\nFull model includes predictors \\(x_1, \\ldots, x_q, x_{q+1}, \\ldots, x_p\\)\n\nWe want to test the hypotheses:\n\n\\(H_0\\): smaller model is better\n\\(H_1\\): Larger model is better\n\nTo do so, we will use the drop-in-deviance test (also known as the nested likelihood ratio test)"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#drop-in-deviance-test",
    "href": "assignments/multilevel_modeling2.html#drop-in-deviance-test",
    "title": "Multilevel Modeling 2",
    "section": "Drop-In-Deviance Test",
    "text": "Drop-In-Deviance Test\n\nHypotheses:\n\n\\(H_0\\): smaller model is better\n\\(H_1\\): Larger model is better\n\nTest Statistic: \\[G = (-2 \\log L_{reduced}) - (-2 \\log L_{full})\\]\nP-value: \\(P(\\chi^2 &gt; G)\\):\n\nCalculated using a \\(\\chi^2\\) distribution\ndf = \\(df_1\\) - \\(df_2\\)"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#testing-deviance",
    "href": "assignments/multilevel_modeling2.html#testing-deviance",
    "title": "Multilevel Modeling 2",
    "section": "Testing deviance",
    "text": "Testing deviance\n\nWe can use the anova function to conduct this test\n\nAdd test = “Chisq” to conduct the drop-in-deviance test\n\nI like test_likelihoodratio from easystats\n\n\n\nCode\n#anova(model1, model2, test=\"chisq\")\n\n# test using easystats function\n\n#test_likelihoodratio(model1, model2)"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#model-fitting-ml-or-reml",
    "href": "assignments/multilevel_modeling2.html#model-fitting-ml-or-reml",
    "title": "Multilevel Modeling 2",
    "section": "Model fitting: ML or REML?",
    "text": "Model fitting: ML or REML?\n\nTwo flavors of maximum likelihood\n\nMaximum Likelihood (ML or FIML)\n\nJointly estimate the fixed effects and variance components using all the sample data\nCan be used to draw conclusions about fixed and random effects\nIssue:\n\nResults are biased because fixed effects are estimated without error"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#model-fitting-ml-or-reml-1",
    "href": "assignments/multilevel_modeling2.html#model-fitting-ml-or-reml-1",
    "title": "Multilevel Modeling 2",
    "section": "Model fitting: ML or REML",
    "text": "Model fitting: ML or REML\n\nRestricted Maximum Likelihood (REML)\n\nEstimates the variance components using the sample residuals not the sample data\nIt is conditional on the fixed effects, so it accounts for uncertainty in fixed effects estimates\n\nThis results in unbiased estimates of variance components\nAssociated with error/penalty"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#model-fitting-ml-or-reml-2",
    "href": "assignments/multilevel_modeling2.html#model-fitting-ml-or-reml-2",
    "title": "Multilevel Modeling 2",
    "section": "Model fitting: ML or REML?",
    "text": "Model fitting: ML or REML?\n\nResearch has not determined one method absolutely superior to the other\nREML (REML = TRUE; default in lmer) is preferable when:\n\nThe number of parameters is large\nPrimary objective is to obtain relaible estimates of the variance parameters\nFor REML, likelihood ratio tests can only be used to draw conclusions about variance components\n\nML (REML = FALSE) must be used if you want to compare nested fixed effects models using a likelihood ratio test (e.g., a drop-in-deviance test)"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#ml-or-reml",
    "href": "assignments/multilevel_modeling2.html#ml-or-reml",
    "title": "Multilevel Modeling 2",
    "section": "ML or REML?",
    "text": "ML or REML?\n\nWhat would we use if we wanted to compare the below models?\n\n\n\nCode\n#x= lmer(DV ~ IV1 + IV2 + (1|ID))\n\n#y= lmer(DV ~ IV1*IV2 + (1|ID))"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#ml-or-reml-1",
    "href": "assignments/multilevel_modeling2.html#ml-or-reml-1",
    "title": "Multilevel Modeling 2",
    "section": "ML or REML?",
    "text": "ML or REML?\n\nWhat would we use if we wanted to compare the below models?\n\n\n\nCode\n#x = lmer(DV ~ IV1 + IV2 + (1+IV2|ID))\n\n#y = lmer(DV ~ IV1+ IV2 + (1|ID))"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#modeling-approach",
    "href": "assignments/multilevel_modeling2.html#modeling-approach",
    "title": "Multilevel Modeling 2",
    "section": "Modeling approach",
    "text": "Modeling approach\n\nForward/backward approach\n\n\n# Forward approach - Start with null model (unconditional means model)\nnull_model &lt;- lmer(math ~ (1|schcode), data = mlm_data, REML = TRUE)\nsummary(null_model)\n\n# Calculate ICC\nicc_val &lt;- 36.95/(36.95 + 39.26)\ncat(\"ICC =\", round(icc_val, 4), \"\\n\")\ncat(\"This means about\", round(icc_val*100, 2), \"% of variance is at the school level\\n\")\n\n# Forward approach - Add fixed effect (ses)\nmodel1 &lt;- lmer(math ~ ses + (1|schcode), data = mlm_data, REML = FALSE)\n\n# Forward approach - Add random slope\nmodel2 &lt;- lmer(math ~ ses + (1+ses|schcode), data = mlm_data, REML = FALSE)\n\n# Check for singular fit in model2\nsummary(model2)\n\n# If convergence issues or singular fit, try removing correlation\nmodel3 &lt;- lmer(math ~ ses + (1|schcode) + (0+ses|schcode), data = mlm_data, REML = FALSE)\n# Alternative syntax: lmer(math ~ ses + (1+ses||schcode), data = mlm_data)\n\n# Model comparison with LRT\nanova(model1, model3)\n\n# Backward approach - Start with maximal model (if it converges)\nmax_model &lt;- lmer(math ~ ses + minority + (1+ses+minority|schcode), \n                  data = mlm_data, REML = FALSE, control = lmerControl(optimizer = \"bobyqa\"))\n\n# Simplified model (backward step)\nreduced_model &lt;- lmer(math ~ ses + minority + (1+ses|schcode), \n                     data = mlm_data, REML = FALSE)\n\n# Compare models\nanova(reduced_model, max_model)\n\n# Final model with REML for better variance estimates\nfinal_model &lt;- lmer(math ~ ses + minority + (1+ses|schcode), \n                   data = mlm_data, REML = TRUE)\nsummary(final_model)\n\nKeep it maximal1\n\nWhatever can vary, should vary\n\nDecreases Type 1 error"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#modeling-approach-1",
    "href": "assignments/multilevel_modeling2.html#modeling-approach-1",
    "title": "Multilevel Modeling 2",
    "section": "Modeling approach",
    "text": "Modeling approach\n\nFull (maximal) model\n\nOnly when there is convergence issues should you remove terms\n\nif non-convergence (pay attention to warning messages in summary output!):\n\nTry different optimizer (afex::all_fit())\n\nSort out random effects\n\nRemove correlations between slopes and intercepts\nRandom slopes\nRandom Intercepts\n\nSort out fixed effects (e.g., interaction)\nOnce you arrive at the final model present it using REML estimation"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#modeling-approach-2",
    "href": "assignments/multilevel_modeling2.html#modeling-approach-2",
    "title": "Multilevel Modeling 2",
    "section": "Modeling approach",
    "text": "Modeling approach\n\nIf your model is singular (check output!!!!)\n\nVariance might be close to 0\nPerfect correlations (1 or -1)\n\nDrop the parameter!"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#modeling-approach-3",
    "href": "assignments/multilevel_modeling2.html#modeling-approach-3",
    "title": "Multilevel Modeling 2",
    "section": "Modeling approach",
    "text": "Modeling approach\n\n\nCode\ndata &lt;- read.csv(\"https://raw.githubusercontent.com/suyoghc/PSY-504_Spring-2025/refs/heads/main/Multilevel%20Modeling/data/heck2011.csv\")\n\nsummary(lmer(math~ses + (1+ses|schcode), data=data))\n\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: math ~ ses + (1 + ses | schcode)\n   Data: data\n\nREML criterion at convergence: 48190.1\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.8578 -0.5553  0.1290  0.6437  5.7098 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n schcode  (Intercept)  3.2042  1.7900        \n          ses          0.7794  0.8828   -1.00\n Residual             62.5855  7.9111        \nNumber of obs: 6871, groups:  schcode, 419\n\nFixed effects:\n             Estimate Std. Error        df t value            Pr(&gt;|t|)    \n(Intercept)   57.6959     0.1315  378.6378  438.78 &lt;0.0000000000000002 ***\nses            3.9602     0.1408 1450.7730   28.12 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n    (Intr)\nses -0.284\noptimizer (nloptwrap) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n\n. . .\n\n\nCode\nlmer(math~ses + (1+ses||schcode), data=data) # removes correlation() with double pipes. Does not work with categorical variables"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#null-model-unconditional-means",
    "href": "assignments/multilevel_modeling2.html#null-model-unconditional-means",
    "title": "Multilevel Modeling 2",
    "section": "Null model (unconditional means)",
    "text": "Null model (unconditional means)\nGet ICC\n\nICC is a standardized way of expressing how much variance is due to clustering/group\n\nRanges from 0-1\n\nCan also be interpreted as correlation among observations within cluster/group!\nIf ICC is sufficiently low (i.e., \\(\\rho\\) &lt; .1), then you don’t have to use MLM! BUT YOU PROBABLY SHOULD 🙂"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#null-model-unconditional-means-1",
    "href": "assignments/multilevel_modeling2.html#null-model-unconditional-means-1",
    "title": "Multilevel Modeling 2",
    "section": "Null model (unconditional means)",
    "text": "Null model (unconditional means)\n\n\nCode\nlibrary(lme4) # pop linear modeling package\n\nnull_model &lt;- lmer(mean_pupil ~ (1|subject), data = eye, REML=TRUE)\n\nsummary(null_model)\n\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: mean_pupil ~ (1 | subject)\n   Data: eye\n\nREML criterion at convergence: -19811.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.1411 -0.5530 -0.0463  0.4822 10.8130 \n\nRandom effects:\n Groups   Name        Variance  Std.Dev.\n subject  (Intercept) 0.0001303 0.01142 \n Residual             0.0016840 0.04104 \nNumber of obs: 5609, groups:  subject, 31\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(&gt;|t|)  \n(Intercept)  0.005227   0.002124 29.457784   2.461   0.0199 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#calculating-icc",
    "href": "assignments/multilevel_modeling2.html#calculating-icc",
    "title": "Multilevel Modeling 2",
    "section": "Calculating ICC",
    "text": "Calculating ICC\n\nRun baseline (null) model\nGet intercept variance and residual variance\n\n\\[\\mathrm{ICC}=\\frac{\\text { between-group variability }}{\\text { between-group variability+within-group variability}}\\]\n\\[\nICC=\\frac{\\operatorname{Var}\\left(u_{0 j}\\right)}{\\operatorname{Var}\\left(u_{0 j}\\right)+\\operatorname{Var}\\left(e_{i j}\\right)}=\\frac{\\tau_{00}}{\\tau_{00}+\\sigma^{2}}\n\\]\n\n\nCode\n# easystats \n#adjusted icc just random effects\n#unadjusted fixed effects taken into account\nperformance::icc(null_model)\n\n\n# Intraclass Correlation Coefficient\n\n    Adjusted ICC: 0.072\n  Unadjusted ICC: 0.072"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#maximal-model-fixed-effect-random-intercepts-subject-and-slopes-vocoded-model",
    "href": "assignments/multilevel_modeling2.html#maximal-model-fixed-effect-random-intercepts-subject-and-slopes-vocoded-model",
    "title": "Multilevel Modeling 2",
    "section": "Maximal model: Fixed effect random intercepts (subject) and slopes (vocoded) model",
    "text": "Maximal model: Fixed effect random intercepts (subject) and slopes (vocoded) model\n\n\nCode\nmax_model &lt;- lmer(mean_pupil ~vocoded +(1+vocoded|subject), data = eye)\n\nsummary(max_model)\n\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: mean_pupil ~ vocoded + (1 + vocoded | subject)\n   Data: eye\n\nREML criterion at convergence: -19813.7\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.0296 -0.5509 -0.0467  0.4810 10.7164 \n\nRandom effects:\n Groups   Name        Variance   Std.Dev. Corr \n subject  (Intercept) 0.00013592 0.011658      \n          vocodedV6   0.00002816 0.005307 -0.19\n Residual             0.00167497 0.040926      \nNumber of obs: 5609, groups:  subject, 31\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(&gt;|t|)  \n(Intercept)  0.003643   0.002235 28.852288    1.63   0.1140  \nvocodedV6    0.003124   0.001453 30.471988    2.15   0.0396 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n          (Intr)\nvocodedV6 -0.306"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#fixed-effects",
    "href": "assignments/multilevel_modeling2.html#fixed-effects",
    "title": "Multilevel Modeling 2",
    "section": "Fixed effects",
    "text": "Fixed effects\n\nInterpretation same as lm\n\n\n\nCode\n#grab the fixed effects\nsummary(max_model)\n\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: mean_pupil ~ vocoded + (1 + vocoded | subject)\n   Data: eye\n\nREML criterion at convergence: -19813.7\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.0296 -0.5509 -0.0467  0.4810 10.7164 \n\nRandom effects:\n Groups   Name        Variance   Std.Dev. Corr \n subject  (Intercept) 0.00013592 0.011658      \n          vocodedV6   0.00002816 0.005307 -0.19\n Residual             0.00167497 0.040926      \nNumber of obs: 5609, groups:  subject, 31\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(&gt;|t|)  \n(Intercept)  0.003643   0.002235 28.852288    1.63   0.1140  \nvocodedV6    0.003124   0.001453 30.471988    2.15   0.0396 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n          (Intr)\nvocodedV6 -0.306"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#degrees-of-freedom-and-p-values",
    "href": "assignments/multilevel_modeling2.html#degrees-of-freedom-and-p-values",
    "title": "Multilevel Modeling 2",
    "section": "Degrees of freedom and p-values",
    "text": "Degrees of freedom and p-values\n\nDegrees of freedom (denominator) and p-values can be assessed with several methods:\n\nSatterthwaite (default when install lmerTest and then run lmer)\nAsymptotic (Inf) (default behavior lme4)\nKenward-Rogers"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#random-effectsvariance-components",
    "href": "assignments/multilevel_modeling2.html#random-effectsvariance-components",
    "title": "Multilevel Modeling 2",
    "section": "Random effects/variance components",
    "text": "Random effects/variance components\n\nTells us how much variability there is around the fixed intercept/slope\n\nHow much does the average pupil size change between participants\n\n\n\n\nCode\nsummary(max_model)\n\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: mean_pupil ~ vocoded + (1 + vocoded | subject)\n   Data: eye\n\nREML criterion at convergence: -19813.7\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-5.0296 -0.5509 -0.0467  0.4810 10.7164 \n\nRandom effects:\n Groups   Name        Variance   Std.Dev. Corr \n subject  (Intercept) 0.00013592 0.011658      \n          vocodedV6   0.00002816 0.005307 -0.19\n Residual             0.00167497 0.040926      \nNumber of obs: 5609, groups:  subject, 31\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(&gt;|t|)  \n(Intercept)  0.003643   0.002235 28.852288    1.63   0.1140  \nvocodedV6    0.003124   0.001453 30.471988    2.15   0.0396 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n          (Intr)\nvocodedV6 -0.306"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#random-effectsvariance-components-1",
    "href": "assignments/multilevel_modeling2.html#random-effectsvariance-components-1",
    "title": "Multilevel Modeling 2",
    "section": "Random effects/variance components",
    "text": "Random effects/variance components\n\nCorrelation between random intercepts and slopes\n\nNegative correlation\n\nHigher intercept (for normal speech) less of effect (lower slope)\n\n\n\n\n\nParameter1 |  Parameter2 |     r |        95% CI | t(29) |     p\n----------------------------------------------------------------\nvocodedV6  | (Intercept) | -0.10 | [-0.44, 0.26] | -0.57 | 0.576\n\nObservations: 31"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#visualize-random-effects",
    "href": "assignments/multilevel_modeling2.html#visualize-random-effects",
    "title": "Multilevel Modeling 2",
    "section": "Visualize Random Effects",
    "text": "Visualize Random Effects\n\n\nCode\n# use easystats to grab group variance\nrandom &lt;- estimate_grouplevel(max_model)\n\nplot(random) + theme_lucid()"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#model-comparisons",
    "href": "assignments/multilevel_modeling2.html#model-comparisons",
    "title": "Multilevel Modeling 2",
    "section": "Model comparisons",
    "text": "Model comparisons\n\nCan compare models using anova function or test_likelihoodratio from easystats\n\nWill be refit using ML if interested in fixed effects\n\n\n\n\nCode\n# Model comparison using anova\n# First refit models with ML for fixed effects comparison\nnull_model_ml &lt;- lmer(mean_pupil ~ (1|subject), data = eye, REML = FALSE)\nmax_model_ml &lt;- lmer(mean_pupil ~ vocoded + (1+vocoded|subject), data = eye, REML = FALSE)\n\n# Compare models using anova\nanova(null_model_ml, max_model_ml)\n\n\nData: eye\nModels:\nnull_model_ml: mean_pupil ~ (1 | subject)\nmax_model_ml: mean_pupil ~ vocoded + (1 + vocoded | subject)\n              npar    AIC    BIC logLik -2*log(L)  Chisq Df Pr(&gt;Chisq)   \nnull_model_ml    3 -19816 -19796 9911.1    -19822                        \nmax_model_ml     6 -19823 -19784 9917.7    -19835 13.269  3    0.00409 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#aic",
    "href": "assignments/multilevel_modeling2.html#aic",
    "title": "Multilevel Modeling 2",
    "section": "AIC",
    "text": "AIC\n\nAIC:\n\n\\[\nD + 2p\n\\]\n\nwhere d = deviance and p = # of parameters in model\nCan compare AICs2:\n\\[\n\\Delta_i = AIC_{i} - AIC_{min}\n\\]\nLess than 2: More parsimonious model is preferred\nBetween 4 and 7: some evidence for lower AIC model\nGreater than 10,: strong evidence for lower AIC"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#bic",
    "href": "assignments/multilevel_modeling2.html#bic",
    "title": "Multilevel Modeling 2",
    "section": "BIC",
    "text": "BIC\n\nBIC:\n\n\\[\nD + ln(n)*p\n\\]\n\nwhere d = deviance, p = # of parameters in model, n = sample size\nChange in BIC:\n\n\\(\\Delta{BIC}\\) &lt;= 2 (No difference)\n\\(\\Delta{BIC}\\) &gt; 3 (evidence for smaller BIC model)"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#aicbic-1",
    "href": "assignments/multilevel_modeling2.html#aicbic-1",
    "title": "Multilevel Modeling 2",
    "section": "AIC/BIC",
    "text": "AIC/BIC\n\n\nCode\nperformance::model_performance(max_model) %&gt;% # easystats\n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAIC\nAICc\nBIC\nR2_conditional\nR2_marginal\nICC\nRMSE\nSigma\n\n\n\n\n-19801.66\n-19801.65\n-19761.87\n0.0773469\n0.0013442\n0.076105\n0.0407697\n0.0409264"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#hypothesis-testing",
    "href": "assignments/multilevel_modeling2.html#hypothesis-testing",
    "title": "Multilevel Modeling 2",
    "section": "Hypothesis testing",
    "text": "Hypothesis testing\n\nMultiple options\n\nt/F tests with approximate degrees of freedom (Kenward-Rogers or Satterwaithe)\nParametric bootstrap\nLikelihood ratio test (LRT)\n\n\nCan be interpreted as main effects and interactions\nUse afex package to do that"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#hypothesis-testing---afex",
    "href": "assignments/multilevel_modeling2.html#hypothesis-testing---afex",
    "title": "Multilevel Modeling 2",
    "section": "Hypothesis testing - afex",
    "text": "Hypothesis testing - afex\n\n\nCode\nlibrary(afex) # load afex in \n\nm &lt;- mixed(mean_pupil ~ 1 + vocoded +  (1+vocoded|subject), data =eye, method = \"LRT\") # fit lmer using afex\n\nnice(m) %&gt;%\n  kable()\n\n\n\n\n\nEffect\ndf\nChisq\np.value\n\n\n\n\nvocoded\n1\n4.47 *\n.034"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#using-emmeans",
    "href": "assignments/multilevel_modeling2.html#using-emmeans",
    "title": "Multilevel Modeling 2",
    "section": "Using emmeans",
    "text": "Using emmeans\n\nGet means and contrasts\n\n\n\nCode\nlibrary(emmeans) # get marginal means \n\nemmeans(max_model, specs = \"vocoded\") %&gt;% \n  kable() # grabs means/SEs for each level of vocode \n\n\n\n\n\nvocoded\nemmean\nSE\ndf\nasymp.LCL\nasymp.UCL\n\n\n\n\nNS\n0.0036427\n0.0022348\nInf\n-0.0007374\n0.0080229\n\n\nV6\n0.0067668\n0.0022618\nInf\n0.0023337\n0.0111999\n\n\n\n\n\nCode\npairs(emmeans(max_model, specs = \"vocoded\")) %&gt;%\n  confint() %&gt;%\n  kable()\n\n\n\n\n\ncontrast\nestimate\nSE\ndf\nasymp.LCL\nasymp.UCL\n\n\n\n\nNS - V6\n-0.0031241\n0.0014532\nInf\n-0.0059723\n-0.0002759\n\n\n\n\n\nCode\n# use this to get pariwise compairsons between levels of factors"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#check-assumptions",
    "href": "assignments/multilevel_modeling2.html#check-assumptions",
    "title": "Multilevel Modeling 2",
    "section": "Check assumptions",
    "text": "Check assumptions\n\n\n\nLinearity\nNormality\n\nLevel 1 residuals are normally distributed around zero\nLevel 2 residuals are multivariate-normal with a mean of zero\n\nHomoskedacticity\n\nLevel 1/Level 2 predictors and residuals are homoskedastic\n\n\n\n\nCollinearity\nOutliers"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#assumptions-1",
    "href": "assignments/multilevel_modeling2.html#assumptions-1",
    "title": "Multilevel Modeling 2",
    "section": "Assumptions",
    "text": "Assumptions\n\n\nCode\nlibrary(easystats) # performance package\n\ncheck_model(max_model)"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#visualization",
    "href": "assignments/multilevel_modeling2.html#visualization",
    "title": "Multilevel Modeling 2",
    "section": "Visualization",
    "text": "Visualization"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#ggeffects",
    "href": "assignments/multilevel_modeling2.html#ggeffects",
    "title": "Multilevel Modeling 2",
    "section": "ggeffects",
    "text": "ggeffects\n\n\nCode\nggemmeans(max_model, terms=c(\"vocoded\")) %&gt;% plot()"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#effect-size",
    "href": "assignments/multilevel_modeling2.html#effect-size",
    "title": "Multilevel Modeling 2",
    "section": "Effect size",
    "text": "Effect size\n\nReport pseudo-\\(R^2\\) for marginal (fixed) and conditional model (full model) (Nakagawa et al. 2017)\n\n\\[\nR^2_{LMM(c)} = \\frac{\\sigma_f^2\\text{fixed} + \\sigma_a^2\\text{random}}{\\sigma_f^2\\text{fixed} + \\sigma_a^2\\text{random} + \\sigma_e^2\\text{residual}}\n\\]\n\\[\nR^2_{\\text{LMM}(m)} = \\frac{\\sigma_f^2\\text{fixed}}{\\sigma_f^2\\text{fixed} + \\sigma_a^2\\text{random} + \\sigma_e^2\\text{residual}}\n\\]\n\nReport semi-partial \\(R^2\\) for each predictor variable\n\n\\(R^2_\\beta\\)\n\npartR2 package in R does this for you"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#effect-size-1",
    "href": "assignments/multilevel_modeling2.html#effect-size-1",
    "title": "Multilevel Modeling 2",
    "section": "Effect size",
    "text": "Effect size\n\n\nCode\n#get r2 for model with performance from easystats\n\nperformance::r2(max_model) \n\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.077\n     Marginal R2: 0.001\n\n\n\n\nCode\n# get semi-part\nlibrary(partR2)\n# does not work with random slopes for some reason :/\n#R2_3 &lt;- partR2(max_model,data=eye, \n  #partvars = c(\"vocoded\"),\n  #R2_type = \"marginal\", nboot = 10, CI = 0.95\n#)"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#effect-size-2",
    "href": "assignments/multilevel_modeling2.html#effect-size-2",
    "title": "Multilevel Modeling 2",
    "section": "Effect size",
    "text": "Effect size\n\nCohen’s \\(d\\) for treatment effects/categorical predictions3\n\n\\[\nd = \\frac{\\text{Effect}}{\\sqrt{\\sigma^2_\\text{Intercept} + \\sigma^2_\\text{slope} + \\sigma^2_\\text{residual}}}\n\\]\n\n\nCode\nlibrary(emmeans)\n\n# Then later in your code\nemmeans(max_model, ~ vocoded) %&gt;% \n  eff_size(sigma=.04, edf=30)\n\n\n contrast effect.size     SE  df asymp.LCL asymp.UCL\n NS - V6      -0.0781 0.0377 Inf    -0.152  -0.00421\n\nsigma used for effect sizes: 0.04 \nDegrees-of-freedom method: inherited from asymptotic when re-gridding \nConfidence level used: 0.95"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#describing-a-mlm-analysis---structure",
    "href": "assignments/multilevel_modeling2.html#describing-a-mlm-analysis---structure",
    "title": "Multilevel Modeling 2",
    "section": "Describing a MLM analysis - Structure",
    "text": "Describing a MLM analysis - Structure\n\nWhat was the nested data structure (e.g., how many levels; what were the units at each level?)\n\nLevel 1: Individual students (n = 1,427) Level 2: Clusters (classrooms/schools) identified by clu_id (n = 222)\n• How many units were in each level, on average? \nOn average, there were 6.43 students per cluster\n• What was the range of the number of lower-level units in each group/cluster?\nThe range of students per cluster was 2 to 18 The outcome variable was RTW_SpringK (reading/writing ability in Spring of Kindergarten)"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#describing-a-mlm-analysis---model",
    "href": "assignments/multilevel_modeling2.html#describing-a-mlm-analysis---model",
    "title": "Multilevel Modeling 2",
    "section": "Describing a MLM analysis - Model",
    "text": "Describing a MLM analysis - Model\n\n\n\nWhat equation can best represent your model?\n\n\\[\nY_{ij} = \\beta_0 + \\beta_1 X_{ij} + \\beta_2 Z_j + u_{0j} + u_{1j}X_{ij} + e_{ij}\n\\]\nWhere \\(Y_{ij}\\) is the reading/writing score, \\(X_{ij}\\) is the vocabulary score, and \\(Z_j\\) is the treatment indicator.\n\nWhat estimation method was used (e.g., ML, REML)?\n\nREML\n\nIf there were convergence issues, how was this addressed?\n\nNo convergence issues were encountered in the final model\n\nWhat software (and version) was used (when using R, what packages as well)?\n\nPackage lme4 in R was used for model fitting, with lmerTest for p-values\n\n\nIf degrees of freedom were used, what kind?\n\nSatterthwaite included in the lmerTest package\n\nWhat type of models were estimated (i.e., unconditional, random intercept, random slope, max)?\n\nThree types of models were estimated: 1. Unconditional means model (null model with only random intercepts) 2. Random intercept model with fixed effects for TRT and PPVT_FallK 3. Random intercept and slope model (adding random slopes for PPVT_FallK)\n\nWhat variables were centered and what kind of centering was used?\n\nppvtc1 appears to be group-mean centered PPVT scores ppvtc2 appears to be grand-mean centered PPVT scores ctrt_e1 and ctrt_e2 suggest centered treatment effect variables\nWe used both group-mean and grand-mean centering.\n\nWhat model assumptions were checked and what were the results?\n\nNormality of residuals - Residuals showed approximately normal distribution with no substantial deviations Homoscedasticity - Plots of residuals versus fitted values indicated relatively constant variance Linearity - The relationship between predictors and the outcome appeared reasonably linear Independence - The multilevel structure accounted for the clustering in the data\nNo severe violations of model assumptions were detected that would invalidate the inferences from the model."
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#describing-a-mlm-analysis---results",
    "href": "assignments/multilevel_modeling2.html#describing-a-mlm-analysis---results",
    "title": "Multilevel Modeling 2",
    "section": "Describing a MLM analysis - Results",
    "text": "Describing a MLM analysis - Results\n\n\n\nWhat was the ICC of the outcome variable?\n\nThe ICC of the reading/writing outcome variable was 0.23, indicating that 23% of the total variance in reading/writing scores was attributable to between-cluster differences, while 77% was due to within-cluster (individual student) differences.\n\nAre fixed effects and variance components reported?\n\nYes, both fixed effects and variance components were reported. The fixed effects included the intercept (β₀), treatment effect (β₂), and vocabulary score effect (β₁). Variance components included random intercept variance (1.74), random slope variance for PPVT_FallK (0.002), the correlation between random effects (-0.26), and residual variance (10.02).\n\nWhat inferential statistics were used (e.g., t-statistics, LRTs)?\n\nLRTs were used to compare nested models. Additionally, t-statistics were reported for individual fixed effects parameters with p-values.\n\nHow precise were the results (report the standard errors and/or confidence intervals)?\n\nThe results included standard errors for fixed effects: treatment effect (SE = 0.19) and vocabulary score effect (SE = 0.01). Confidence intervals were reported for the treatment effect, with the 95% CI [0.44, 1.18] indicating the plausible range of the treatment effect.\n\n\nWere model comparisons performed (e.g., AIC, BIC, if using an LRT,report the χ2, degrees of freedom, and p value)?\n\nYes, model comparisons were performed using likelihood ratio tests. The comparison between the null model and the random intercept model showed χ² = 261.89, df = 2, p &lt; 0.001. The comparison between the random intercept model and random intercept & slope model showed χ² = 9.88, df = 2, p = 0.007. AIC and BIC values were also reported for all models.\n\nWere effect sizes reported for overall model and individual predictors (e.g., Cohen’s d, \\(R^2\\) )?\n\nYes, effect sizes were reported. For the overall model, marginal R² was 0.29 (variance explained by fixed effects) and conditional R² was 0.45 (variance explained by the entire model). Cohen’s d for the treatment effect was 0.24, indicating a small-to-medium effect size."
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#write-up",
    "href": "assignments/multilevel_modeling2.html#write-up",
    "title": "Multilevel Modeling 2",
    "section": "Write-up",
    "text": "Write-up\n\n\nCode\nlibrary(modelsummary)\nmodelsummary(max_model, stars = TRUE)\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)\n                  0.004\n                \n                \n                  \n                  (0.002)\n                \n                \n                  vocodedV6\n                  0.003*\n                \n                \n                  \n                  (0.001)\n                \n                \n                  SD (Intercept subject)\n                  0.012\n                \n                \n                  SD (vocodedV6 subject)\n                  0.005\n                \n                \n                  Cor (Intercept~vocodedV6 subject)\n                  -0.195\n                \n                \n                  SD (Observations)\n                  0.041\n                \n                \n                  Num.Obs.\n                  5609\n                \n                \n                  R2 Marg.\n                  0.001\n                \n                \n                  R2 Cond.\n                  0.077\n                \n                \n                  AIC\n                  -19801.7\n                \n                \n                  BIC\n                  -19761.9\n                \n                \n                  ICC\n                  0.1\n                \n                \n                  RMSE\n                  0.04\n                \n        \n      \n    \n\n\n\nCode\n##| eval: false\n#report::report(max_model) # easystats report function\n\n\nWe fitted a linear mixed model (estimated using REML and nloptwrap optimizer) to predict mean_pupil with vocoded (formula: mean_pupil ~ vocoded). The model included vocoded as random effects (formula: ~1 + vocoded | subject). The model’s total explanatory power is weak (conditional R2 = 0.08) and the part related to the fixed effects alone (marginal R2) is of 1.34e-03. The model’s intercept, corresponding to vocoded = NS, is at 3.64e-03 (95% CI [-7.38e-04, 8.02e-03], t(5603) = 1.63, p = 0.103). Within this model:\n\nThe effect of vocoded [V6] is statistically significant and positive (beta = 3.12e-03, 95% CI [2.75e-04, 5.97e-03], t(5603) = 2.15, p = 0.032; Std. beta = 0.07, 95% CI [6.49e-03, 0.14])\n\nStandardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using a Wald t-distribution approximation."
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#table",
    "href": "assignments/multilevel_modeling2.html#table",
    "title": "Multilevel Modeling 2",
    "section": "Table",
    "text": "Table\n\n\nCode\nmodelsummary::modelsummary(list(\"max model\" = max_model), output=\"html\") # modelsummary package\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                max model\n              \n        \n        \n        \n                \n                  (Intercept)\n                  0.004\n                \n                \n                  \n                  (0.002)\n                \n                \n                  vocodedV6\n                  0.003\n                \n                \n                  \n                  (0.001)\n                \n                \n                  SD (Intercept subject)\n                  0.012\n                \n                \n                  SD (vocodedV6 subject)\n                  0.005\n                \n                \n                  Cor (Intercept~vocodedV6 subject)\n                  −0.195\n                \n                \n                  SD (Observations)\n                  0.041\n                \n                \n                  Num.Obs.\n                  5609\n                \n                \n                  R2 Marg.\n                  0.001\n                \n                \n                  R2 Cond.\n                  0.077\n                \n                \n                  AIC\n                  −19801.7\n                \n                \n                  BIC\n                  −19761.9\n                \n                \n                  ICC\n                  0.1\n                \n                \n                  RMSE\n                  0.04"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#power",
    "href": "assignments/multilevel_modeling2.html#power",
    "title": "Multilevel Modeling 2",
    "section": "Power",
    "text": "Power\n\nSimulation-based power analyses\n\nSimulate new data\n\nfaux (https://debruine.github.io/faux/articles/sim_mixed.html)\n\nUse pilot data (what I would do)\n\nmixedpower(https://link.springer.com/article/10.3758/s13428-021-01546-0)\nsimr (https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210X.12504)"
  },
  {
    "objectID": "assignments/multilevel_modeling2.html#footnotes",
    "href": "assignments/multilevel_modeling2.html#footnotes",
    "title": "Multilevel Modeling 2",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBarr, D. J., Levy, R., Scheepers, C., & Tily, H. J. (2013). Random effects structure for confirmatory hypothesis testing: Keep it maximal. Journal of memory and language, 68(3), 10.1016/j.jml.2012.11.001. https://doi.org/10.1016/j.jml.2012.11.001↩︎\nBURNHAM, ANDERSON, & HUYVAERT (2011)↩︎\nBrysbaert, M., & Debeer, D. (2023, September 12). How to run linear mixed effects analysis for pairwise comparisons? A tutorial and a proposal for the calculation of standardized effect sizes. https://doi.org/10.31234/osf.io/esnku↩︎"
  },
  {
    "objectID": "assignments/bayesian_modeling.html",
    "href": "assignments/bayesian_modeling.html",
    "title": "Bayesian Modeling",
    "section": "",
    "text": "Here is a worksheet and assignment that combines Bayes (brms) with tidyverse tools. The focus is on the essentials when it comes to simple linear regression with brms.\nPlease read and run through this worksheet and answer the conceptual questions that are interleaved within them. At the end of each part, is a coding exercise based on the material you’ve read until then."
  },
  {
    "objectID": "assignments/bayesian_modeling.html#packages-and-data",
    "href": "assignments/bayesian_modeling.html#packages-and-data",
    "title": "Bayesian Modeling",
    "section": "Packages and data",
    "text": "Packages and data\nLoad the primary packages.\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggside)\nlibrary(brms)\nlibrary(broom)\nlibrary(broom.mixed)\n\n\nWe’ll use the penguins data set from the palmerpenguins package.\n\n\nCode\ndata(penguins, package = \"palmerpenguins\")\n\n# Any type of looking at data is a part of EDA \nglimpse(penguins)\n\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\nCode\nhead(penguins)\n\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nYou might divide the data set by the three levels of species.\n\n\nCode\npenguins %&gt;% \n  count(species)\n\n\n# A tibble: 3 × 2\n  species       n\n  &lt;fct&gt;     &lt;int&gt;\n1 Adelie      152\n2 Chinstrap    68\n3 Gentoo      124\n\n\nTo start, we’ll make a subset of the data called chinstrap.\n\n\nCode\nchinstrap &lt;- penguins %&gt;% \n  filter(species == \"Chinstrap\")\n\nglimpse(chinstrap)\n\n\nRows: 68\nColumns: 8\n$ species           &lt;fct&gt; Chinstrap, Chinstrap, Chinstrap, Chinstrap, Chinstra…\n$ island            &lt;fct&gt; Dream, Dream, Dream, Dream, Dream, Dream, Dream, Dre…\n$ bill_length_mm    &lt;dbl&gt; 46.5, 50.0, 51.3, 45.4, 52.7, 45.2, 46.1, 51.3, 46.0…\n$ bill_depth_mm     &lt;dbl&gt; 17.9, 19.5, 19.2, 18.7, 19.8, 17.8, 18.2, 18.2, 18.9…\n$ flipper_length_mm &lt;int&gt; 192, 196, 193, 188, 197, 198, 178, 197, 195, 198, 19…\n$ body_mass_g       &lt;int&gt; 3500, 3900, 3650, 3525, 3725, 3950, 3250, 3750, 4150…\n$ sex               &lt;fct&gt; female, male, male, female, male, female, female, ma…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\nWe’ve done from a full data set with \\(N = 344\\) rows, to a subset with \\(n = 68\\) rows. (“$” signs hold LaTex snippets)"
  },
  {
    "objectID": "assignments/bayesian_modeling.html#more-exploratory-data-analysis-eda",
    "href": "assignments/bayesian_modeling.html#more-exploratory-data-analysis-eda",
    "title": "Bayesian Modeling",
    "section": "More Exploratory data analysis (EDA)",
    "text": "More Exploratory data analysis (EDA)\nOur focal variables will be body_mass_g and bill_length_mm. Here they are in a scatter plot.\n\n\nCode\nchinstrap %&gt;% \n  ggplot(aes(x = body_mass_g, y = bill_length_mm)) +\n  geom_point() +\n  stat_smooth(method = \"lm\", formula = 'y ~ x', se = FALSE)\n\n\n\n\n\n\n\n\n\nWe can augment the plot with some nice functions from the ggside package.\n\n\nCode\nchinstrap %&gt;% \n  ggplot(aes(x = body_mass_g, y = bill_length_mm)) +\n  geom_point() +\n  stat_smooth(method = \"lm\", formula = 'y ~ x', se = FALSE) +\n  # from ggside\n  geom_xsidehistogram(bins = 30) +\n  geom_ysidehistogram(bins = 30) +\n  scale_xsidey_continuous(breaks = NULL) +\n  scale_ysidex_continuous(breaks = NULL) +\n  theme(ggside.panel.scale = 0.25)\n\n\n\n\n\n\n\n\n\nIt’s a good idea to get a sense of the sample statistics. Here are the means and SD’s for the two variables.\n\n\nCode\nchinstrap %&gt;% \n  summarise(body_mass_g_mean = mean(body_mass_g),\n            body_mass_g_sd = sd(body_mass_g),\n            bill_length_mm_mean = mean(bill_length_mm),\n            bill_length_mm_sd = sd(bill_length_mm)) \n\n\n# A tibble: 1 × 4\n  body_mass_g_mean body_mass_g_sd bill_length_mm_mean bill_length_mm_sd\n             &lt;dbl&gt;          &lt;dbl&gt;               &lt;dbl&gt;             &lt;dbl&gt;\n1            3733.           384.                48.8              3.34\n\n\nAnd you know that more efficient way to compute sample statistics for multiple variables is to first convert the data into the long format with pivot_longer(). Then you use a group_by() line before the main event in summarise().\n\n\nCode\nchinstrap %&gt;% \n  pivot_longer(cols = c(body_mass_g, bill_length_mm)) %&gt;% \n  group_by(name) %&gt;% \n  summarise(mean = mean(value),\n            sd = sd(value),\n            # count the missing data (if any)\n            n_missing = sum(is.na(value))) \n\n\n# A tibble: 2 × 4\n  name             mean     sd n_missing\n  &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;     &lt;int&gt;\n1 bill_length_mm   48.8   3.34         0\n2 body_mass_g    3733.  384.           0\n\n\n\nQuestion 1.1: What do the marginal histograms added by ggside tell you about the distribution of body_mass_g and bill_length_mm individually?\nbody_mass_g appears to have a somewhat bimodal distribution, suggesting there might be two subgroups of chinstrap penguins based on body mass (possibly corresponding to male and female penguins?)\nbill_length_mm seems to be more normally distributed, with a slight right skew."
  },
  {
    "objectID": "assignments/bayesian_modeling.html#ols",
    "href": "assignments/bayesian_modeling.html#ols",
    "title": "Bayesian Modeling",
    "section": "OLS",
    "text": "OLS\nWe’ll fit the model\n\\[\n\\begin{align}\n\\text{bill_length_mm}_i & = \\beta_0 + \\beta_1 \\text{body_mass_g}_i + \\epsilon_i \\\\\n\\epsilon_i & \\sim \\operatorname{Normal}(0, \\sigma_\\epsilon)\n\\end{align}\n\\]\nwhere bill_length_mm is the dependent variable or a response variable. The sole predictor is body_mass_g. Both variables have \\(i\\) subscripts, which indicate they vary across the \\(i\\) rows in the data set. For now, you might think if \\(i\\) as standing for “index.” The last term in the first line, \\(\\epsilon\\), is often called the error, or noise term. In the second line, we see we’re making the conventional assumption the “errors” are normally distributed around the regression line.\nAn alternative and equivalent way to write that equation is\n\\[\n\\begin{align}\n\\text{bill_length_mm}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i & = \\beta_0 + \\beta_1 \\text{body_mass_g}_i,\n\\end{align}\n\\]\nwhich is meant to convey we are modeling bill_length_mm as normally distributed, with a conditional mean. You don’t tend to see equations written this way in the OLS paradigm. However, this style of notation will serve us better when we start modeling our data with other distributions.\nThis notation grows on you\nFitting the model with the base R lm() function, which uses the OLS algorithm.\n\n\nCode\n# fit\nfit1.ols &lt;- lm(\n  data = chinstrap,\n  bill_length_mm ~ 1 + body_mass_g\n)\n\n# summarize the results\nsummary(fit1.ols)\n\n\n\nCall:\nlm(formula = bill_length_mm ~ 1 + body_mass_g, data = chinstrap)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.8399 -2.2370  0.3247  1.8385  9.3138 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 3.217e+01  3.443e+00   9.344 1.07e-13 ***\nbody_mass_g 4.463e-03  9.176e-04   4.863 7.48e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.887 on 66 degrees of freedom\nMultiple R-squared:  0.2638,    Adjusted R-squared:  0.2527 \nF-statistic: 23.65 on 1 and 66 DF,  p-value: 7.48e-06\n\n\nThe point estimates are in scientific notation. We can pull them with the coef() function.\n\n\nCode\ncoef(fit1.ols)\n\n\n (Intercept)  body_mass_g \n32.174192865  0.004462694 \n\n\nWe can compute fitted values, or predictions, with the predict() function. Here’s the default behavior.\n\n\nCode\npredict(fit1.ols)\n\n\n       1        2        3        4        5        6        7        8 \n47.79362 49.57870 48.46303 47.90519 48.79773 49.80183 46.67795 48.90930 \n       9       10       11       12       13       14       15       16 \n50.69437 48.68616 49.13243 49.02086 48.68616 50.24810 48.12832 50.24810 \n      17       18       19       20       21       22       23       24 \n46.90108 48.68616 47.57049 51.81005 48.23989 47.34735 45.11601 49.13243 \n      25       26       27       28       29       30       31       32 \n46.90108 50.69437 47.34735 49.13243 48.68616 52.47945 46.45481 51.36378 \n      33       34       35       36       37       38       39       40 \n47.12422 50.47124 48.23989 49.57870 49.35556 53.59512 44.22347 52.25632 \n      41       42       43       44       45       46       47       48 \n49.80183 48.46303 48.01676 47.79362 48.57459 52.03318 47.34735 51.36378 \n      49       50       51       52       53       54       55       56 \n46.67795 48.57459 47.01265 49.80183 48.23989 50.24810 47.12422 47.57049 \n      57       58       59       60       61       62       63       64 \n46.67795 50.24810 49.13243 47.90519 49.80183 48.46303 48.46303 50.02497 \n      65       66       67       68 \n47.34735 49.02086 50.47124 49.02086 \n\n\nWe get one prediction, one fitted value, for each case in the data set. We can express the uncertainty around those predictions with confidence intervals.\n\n\nCode\npredict(fit1.ols,\n        interval = \"confidence\") %&gt;% \n  # just the top 6\n  head()\n\n\n       fit      lwr      upr\n1 47.79362 46.97456 48.61268\n2 49.57870 48.81580 50.34160\n3 48.46303 47.74771 49.17834\n4 47.90519 47.10905 48.70133\n5 48.79773 48.09864 49.49682\n6 49.80183 48.99783 50.60584\n\n\nWe might also ask for a standard error for each prediction.\n\n\nCode\npredict(fit1.ols,\n        se.fit = TRUE) %&gt;% \n  data.frame()\n\n\n        fit    se.fit df residual.scale\n1  47.79362 0.4102359 66       2.886728\n2  49.57870 0.3821060 66       2.886728\n3  48.46303 0.3582736 66       2.886728\n4  47.90519 0.3987564 66       2.886728\n5  48.79773 0.3501459 66       2.886728\n6  49.80183 0.4026961 66       2.886728\n7  46.67795 0.5648454 66       2.886728\n8  48.90930 0.3504110 66       2.886728\n9  50.69437 0.5185569 66       2.886728\n10 48.68616 0.3513814 66       2.886728\n11 49.13243 0.3554108 66       2.886728\n12 49.02086 0.3521734 66       2.886728\n13 48.68616 0.3513814 66       2.886728\n14 50.24810 0.4550963 66       2.886728\n15 48.12832 0.3789333 66       2.886728\n16 50.24810 0.4550963 66       2.886728\n17 46.90108 0.5296025 66       2.886728\n18 48.68616 0.3513814 66       2.886728\n19 47.57049 0.4359183 66       2.886728\n20 51.81005 0.7050167 66       2.886728\n21 48.23989 0.3707575 66       2.886728\n22 47.34735 0.4647215 66       2.886728\n23 45.11601 0.8407923 66       2.886728\n24 49.13243 0.3554108 66       2.886728\n25 46.90108 0.5296025 66       2.886728\n26 50.69437 0.5185569 66       2.886728\n27 47.34735 0.4647215 66       2.886728\n28 49.13243 0.3554108 66       2.886728\n29 48.68616 0.3513814 66       2.886728\n30 52.47945 0.8273195 66       2.886728\n31 46.45481 0.6015246 66       2.886728\n32 51.36378 0.6270243 66       2.886728\n33 47.12422 0.4961023 66       2.886728\n34 50.47124 0.4856973 66       2.886728\n35 48.23989 0.3707575 66       2.886728\n36 49.57870 0.3821060 66       2.886728\n37 49.35556 0.3661365 66       2.886728\n38 53.59512 1.0397147 66       2.886728\n39 44.22347 1.0105441 66       2.886728\n40 52.25632 0.7859885 66       2.886728\n41 49.80183 0.4026961 66       2.886728\n42 48.46303 0.3582736 66       2.886728\n43 48.01676 0.3882941 66       2.886728\n44 47.79362 0.4102359 66       2.886728\n45 48.57459 0.3541019 66       2.886728\n46 52.03318 0.7451900 66       2.886728\n47 47.34735 0.4647215 66       2.886728\n48 51.36378 0.6270243 66       2.886728\n49 46.67795 0.5648454 66       2.886728\n50 48.57459 0.3541019 66       2.886728\n51 47.01265 0.5126128 66       2.886728\n52 49.80183 0.4026961 66       2.886728\n53 48.23989 0.3707575 66       2.886728\n54 50.24810 0.4550963 66       2.886728\n55 47.12422 0.4961023 66       2.886728\n56 47.57049 0.4359183 66       2.886728\n57 46.67795 0.5648454 66       2.886728\n58 50.24810 0.4550963 66       2.886728\n59 49.13243 0.3554108 66       2.886728\n60 47.90519 0.3987564 66       2.886728\n61 49.80183 0.4026961 66       2.886728\n62 48.46303 0.3582736 66       2.886728\n63 48.46303 0.3582736 66       2.886728\n64 50.02497 0.4272392 66       2.886728\n65 47.34735 0.4647215 66       2.886728\n66 49.02086 0.3521734 66       2.886728\n67 50.47124 0.4856973 66       2.886728\n68 49.02086 0.3521734 66       2.886728\n\n\nInstead of relying on predictions from the values in the data, we might instead define a sequence of values from the predictor variable. We’ll call those nd.\n\n\nCode\nnd &lt;- tibble(body_mass_g = seq(from = min(chinstrap$body_mass_g),\n                               to = max(chinstrap$body_mass_g),\n                               length.out = 50))\n\nglimpse(nd)\n\n\nRows: 50\nColumns: 1\n$ body_mass_g &lt;dbl&gt; 2700.000, 2742.857, 2785.714, 2828.571, 2871.429, 2914.286…\n\n\nWe can insert our nd data into the newdata argument.\n\n\nCode\npredict(fit1.ols,\n        interval = \"confidence\",\n        newdata = nd) %&gt;% \n  # just the top 6\n  head()\n\n\n       fit      lwr      upr\n1 44.22347 42.20585 46.24108\n2 44.41473 42.47057 46.35888\n3 44.60598 42.73489 46.47708\n4 44.79724 42.99874 46.59574\n5 44.98850 43.26207 46.71493\n6 45.17976 43.52482 46.83469\n\n\nNow we wrangle those predictions a bit and pump the results right into ggplot().\n\n\nCode\npredict(fit1.ols,\n        interval = \"confidence\",\n        newdata = nd) %&gt;% \n  data.frame() %&gt;% \n  bind_cols(nd) %&gt;% \n  \n  ggplot(aes(x = body_mass_g)) +\n  # 95% confidence interval ribbon\n  geom_ribbon(aes(ymin = lwr, ymax = upr),\n              alpha = 1/3) +\n  # point estimate line\n  geom_line(aes(y = fit)) +\n  geom_point(data = chinstrap,\n             aes(y = bill_length_mm))\n\n\n\n\n\n\n\n\n\nIf we wanted to, we could look at the residuals with help from the residuals() function.\n\n\nCode\nresiduals(fit1.ols)\n\n\n         1          2          3          4          5          6          7 \n-1.2936220  0.4213003  2.8369738 -2.5051894  3.9022718 -4.6018344 -0.5779485 \n         8          9         10         11         12         13         14 \n 2.3907044 -4.6943732  2.6138391 -2.5324303  2.6791371 -1.6861609  1.7518962 \n        15         16         17         18         19         20         21 \n-2.2283241  0.2518962  3.3989168  9.3138391 -1.1704873 -2.6100467 -5.8398915 \n        22         23         24         25         26         27         28 \n 1.1526474 -1.9160056  1.4675697 -0.2010832  1.3056268  3.1526474  0.3675697 \n        29         30         31         32         33         34         35 \n-2.2861609  0.3205492 -5.5548138  2.8362227 -4.6242179  0.5287615  1.4601085 \n        36         37         38         39         40         41         42 \n-2.0786997 -1.7555650 -1.5951243  2.6765332  1.2436839 -0.8018344 -2.2630262 \n        43         44         45         46         47         48         49 \n 2.8832432 -2.2936220  2.3254065 -1.2331814  2.7526474 -2.3637773  4.8220515 \n        50         51         52         53         54         55         56 \n 1.2254065  1.0873494  1.5981656 -2.5398915  0.4518962 -4.6242179  4.6295127 \n        57         58         59         60         61         62         63 \n-1.4779485 -0.9481038  1.0675697 -2.3051894  2.0981656 -1.6630262 -2.7630262 \n        64         65         66         67         68 \n 5.7750309 -3.8473526  0.5791371  0.3287615  1.1791371 \n\n\nHere we might put them in a tibble and display them in a plot.\n\n\nCode\n# put them in a tibble\ntibble(r = residuals(fit1.ols)) %&gt;% \n  # plot!\n  ggplot(aes(x = r)) +\n  geom_histogram(binwidth = 1)\n\n\n\n\n\n\n\n\n\n\nQuestion 1.2: Can you predict what the mean value, and standard deviations will be? Why? Calculate it. Compare this against outputs in summary(fit1.ols) and explain. Map the values you find to the latex equations before.\nThe mean should be close to 0, and the SD should be close to the residual standard error.\n\n\nCode\n# Calculate mean and SD of residuals\nmean(residuals(fit1.ols))  \n\n\n[1] 1.600027e-16\n\n\nCode\nsd(residuals(fit1.ols))    \n\n\n[1] 2.865104\n\n\nCode\nsummary(fit1.ols)$sigma   \n\n\n[1] 2.886728\n\n\nThe residual standard error (2.887) matches the calculated value of 2.887.\n\\[\n\\begin{align}\n\\text{bill_length_mm}_i & = \\beta_0 + \\beta_1 \\text{body_mass_g}_i + \\epsilon_i \\\\\n\\epsilon_i & \\sim \\operatorname{Normal}(0, \\sigma_\\epsilon)\n\\end{align}\n\\]\nIn this equation, \\(\\epsilon_i\\) is the error term, which is assumed to be normally distributed with mean 0 and standard deviation \\(\\sigma_\\epsilon\\). 2.887 is the standard deviation of the residuals, which are the differences between the observed values of bill_length_mm and the predicted values from the model."
  },
  {
    "objectID": "assignments/bayesian_modeling.html#bayes-with-default-settings",
    "href": "assignments/bayesian_modeling.html#bayes-with-default-settings",
    "title": "Bayesian Modeling",
    "section": "Bayes with default settings",
    "text": "Bayes with default settings\nWe’ll be fitting our Bayesian models with the brms package. The primary function is brm().\nbrm() can work a lot like the OLS-based lm() function. For example, here’s how to fit a Bayesian version of our OLS model fit1.ols.\n\n\nCode\nfit1.b &lt;- brm(\n  data = chinstrap,\n  bill_length_mm ~ 1 + body_mass_g\n)\n\n\nNotice what’s happening in the console, below. We’ll get into the details of what just happened later. For now, appreciate we just fit our first Bayesian model, and it wasn’t all that hard.\nSummarize the model.\n\n\nCode\nsummary(fit1.b)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: bill_length_mm ~ 1 + body_mass_g \n   Data: chinstrap (Number of observations: 68) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      32.24      3.56    25.32    39.38 1.00     4789     3248\nbody_mass_g     0.00      0.00     0.00     0.01 1.00     4883     3111\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     2.92      0.26     2.47     3.48 1.00     1897     1916\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\nQuestion 1.3: Contrast the language of in the brm() output from the in the lm() output. Ignore ‘Rhat,’ ‘Bulk_ESS,’ and ‘Tail_ESS’ for now.\nThe brm() output uses the term “family” to describe the distribution of the response variable, while the lm() output uses the term “residual standard error.”\nThe brm() output also provides information about the posterior distribution of the model parameters, including the mean, standard deviation, and 95% credible intervals. The lm() output provides the coefficients, standard errors, t-values, and p-values for the model parameters.\nAdditionally, the ‘lm()’ output uses p-values for null hypothesis significance testing, while the ‘brm()’ output provides probability intervals directly from the posterior distribution\nWe can get a quick and dirty plot of the fitted line with the conditional_effects() function.\n\n\nCode\nconditional_effects(fit1.b)\n\n\n\n\n\n\n\n\n\nCode\n# %&gt;% \n#   plot(points = TRUE)"
  },
  {
    "objectID": "assignments/bayesian_modeling.html#coefficients-and-coefficient-plots",
    "href": "assignments/bayesian_modeling.html#coefficients-and-coefficient-plots",
    "title": "Bayesian Modeling",
    "section": "Coefficients and coefficient plots",
    "text": "Coefficients and coefficient plots\nWe might want to compare the coefficient summaries from the OLS model to those from the Bayesian model. Here’s the frequentist summary:\n\n\nCode\ncbind(coef(fit1.ols),              # point estimates\n      sqrt(diag(vcov(fit1.ols))),  # standard errors\n      confint(fit1.ols))           # 95% CIs\n\n\n                                             2.5 %       97.5 %\n(Intercept) 32.174192865 3.4433622902 25.299298235 39.049087495\nbody_mass_g  0.004462694 0.0009176106  0.002630625  0.006294763\n\n\nWe can compute a focused summary of the Bayesian model with the fixef() function.\n\n\nCode\nfixef(fit1.b)\n\n\n                Estimate    Est.Error         Q2.5        Q97.5\nIntercept   32.243201279 3.5578978933 25.324542044 39.384829020\nbody_mass_g  0.004452674 0.0009491811  0.002544748  0.006237691\n\n\nIn this case, the results are very similar.\nWe can also pull this information from our OLS model with the broom::tidy() function.\n\n\nCode\ntidy(fit1.ols, conf.int = TRUE)\n\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept) 32.2      3.44          9.34 1.07e-13 25.3      39.0    \n2 body_mass_g  0.00446  0.000918      4.86 7.48e- 6  0.00263   0.00629\n\n\nIf you would like to use the tidy() function with your brms models, it will have to be the version of tidy() from the broom.mixed package.\n\n\nCode\ntidy(fit1.b)\n\n\n# A tibble: 3 × 8\n  effect   component group    term         estimate std.error conf.low conf.high\n  &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 fixed    cond      &lt;NA&gt;     (Intercept)  32.2      3.56     25.3      39.4    \n2 fixed    cond      &lt;NA&gt;     body_mass_g   0.00445  0.000949  0.00254   0.00624\n3 ran_pars cond      Residual sd__Observa…  2.92     0.264     2.47      3.48   \n\n\nHere’s how to wrangle and combine these two results into a single data frame. Then we’ll make a coefficient plot.\n\n\nCode\nbind_rows(\n  tidy(fit1.ols, conf.int = TRUE) %&gt;% select(term, estimate, contains(\"conf\")),\n  tidy(fit1.b) %&gt;% select(term, estimate, contains(\"conf\")) %&gt;% filter(term != \"sd__Observation\")\n) %&gt;% \n  mutate(method = rep(c(\"lm()\", \"brm()\"), each = 2)) %&gt;% \n  \n  ggplot(aes(x = estimate, xmin = conf.low, xmax = conf.high, y = method)) +\n  geom_pointrange() +\n  scale_x_continuous(\"parameter space\", expand = expansion(mult = 0.2)) +\n  scale_y_discrete(expand = expansion(mult = 5)) +\n  facet_wrap(~ term, scales = \"free_x\")\n\n\n\n\n\n\n\n\n\nAt a superficial level for simple conventional regression type models, the results from a Bayesian brm() model will be very similar to those from an OLS lm() model. This will not always be case, and even in this example there are many differences once we look below the surface."
  },
  {
    "objectID": "assignments/bayesian_modeling.html#more-questionsexercise",
    "href": "assignments/bayesian_modeling.html#more-questionsexercise",
    "title": "Bayesian Modeling",
    "section": "More Questions/Exercise",
    "text": "More Questions/Exercise\nGo back to the full penguins data set. This time, make a subset of the data called gentoo, which is only the cases for which species == \"Gentoo\".\n\n\nCode\ndata(penguins, package = \"palmerpenguins\")\n\ngentoo &lt;- penguins %&gt;% \n  filter(species == \"Gentoo\")\n\n\nCan you fit the same OLS model to these data?\n\n\nCode\nfit_gentoo_ols &lt;- lm(\n  data = gentoo,\n  bill_length_mm ~ 1 + body_mass_g\n)\nsummary(fit_gentoo_ols)\n\n\n\nCall:\nlm(formula = bill_length_mm ~ 1 + body_mass_g, data = gentoo)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.8802 -1.5075 -0.0575  1.3118  8.1107 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 26.739549   2.106594  12.693   &lt;2e-16 ***\nbody_mass_g  0.004091   0.000413   9.905   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.3 on 121 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.4478,    Adjusted R-squared:  0.4432 \nF-statistic: 98.12 on 1 and 121 DF,  p-value: &lt; 2.2e-16\n\n\nHow about plotting the results with predict()?\n\n\nCode\nnd_gentoo &lt;- tibble(body_mass_g = seq(from = min(gentoo$body_mass_g, na.rm = TRUE),\n                               to = max(gentoo$body_mass_g, na.rm = TRUE),\n                               length.out = 50))\n\ngentoo_pred &lt;- predict(fit_gentoo_ols,\n        interval = \"confidence\",\n        newdata = nd_gentoo) %&gt;% \n  data.frame() %&gt;% \n  bind_cols(nd_gentoo)\n\npred_plot &lt;- gentoo_pred %&gt;%\n  ggplot(aes(x = body_mass_g)) +\n  geom_ribbon(aes(ymin = lwr, ymax = upr),\n              alpha = 1/3) +\n  geom_line(aes(y = fit)) +\n  geom_point(data = gentoo,\n             aes(y = bill_length_mm)) +\n  labs(title = \"OLS Model for Gentoo Penguins\",\n       x = \"Body Mass (g)\",\n       y = \"Bill Length (mm)\")\nprint(pred_plot)\n\n\n\n\n\n\n\n\n\nCan you fit the same default Bayesian brm() model to these data?\n\n\nCode\nfit_gentoo_b &lt;- brm(\n  data = gentoo,\n  bill_length_mm ~ 1 + body_mass_g,\n  iter = 2000,\n  chains = 2\n)\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 2.5e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.25 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.038 seconds (Warm-up)\nChain 1:                0.028 seconds (Sampling)\nChain 1:                0.066 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 2e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.036 seconds (Warm-up)\nChain 2:                0.027 seconds (Sampling)\nChain 2:                0.063 seconds (Total)\nChain 2: \n\n\nCode\nsummary(fit_gentoo_b)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: bill_length_mm ~ 1 + body_mass_g \n   Data: gentoo (Number of observations: 123) \n  Draws: 2 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 2000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      26.74      2.16    22.50    30.93 1.00     2234     1503\nbody_mass_g     0.00      0.00     0.00     0.00 1.00     2254     1558\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     2.32      0.15     2.05     2.61 1.01      706      675\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nHow about plotting the results with conditional_effects()?\n\n\nCode\nce &lt;- conditional_effects(fit_gentoo_b)\nplot(ce, points = TRUE)\n\n\n\n\n\n\n\n\n\nCan you make a coefficient plot comparing the new OLS and Bayesian beta coefficients?\n\n\nCode\nbind_rows(\n  # Get OLS coefficients with confidence intervals\n  tidy(fit_gentoo_ols, conf.int = TRUE) %&gt;% \n    select(term, estimate, conf.low, conf.high) %&gt;%\n    mutate(method = \"OLS (lm())\"),\n  \n  # Get Bayesian coefficients with credible intervals\n  tidy(fit_gentoo_b) %&gt;% \n    select(term, estimate, conf.low, conf.high) %&gt;% \n    filter(term != \"sd__Observation\") %&gt;%\n    mutate(method = \"Bayesian (brm())\")\n) %&gt;%\n  mutate(method = factor(method, levels = c(\"OLS (lm())\", \"Bayesian (brm())\"))) %&gt;%\n  \n  ggplot(aes(x = estimate, xmin = conf.low, xmax = conf.high, y = method, color = method)) +\n  geom_pointrange(position = position_dodge(width = 0.5), size = 1) +\n  scale_color_manual(values = c(\"OLS (lm())\" = \"darkblue\", \"Bayesian (brm())\" = \"darkred\")) +\n  facet_wrap(~ term, scales = \"free_x\", labeller = label_parsed) +\n  labs(\n    title = \"Comparison of OLS and Bayesian Coefficients for Gentoo Penguins\",\n    subtitle = \"Point estimates with 95% confidence/credible intervals\",\n    x = \"Parameter Estimate\",\n    y = NULL\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"bottom\",\n    strip.text = element_text(size = 12, face = \"bold\"),\n    panel.grid.minor = element_blank(),\n    panel.spacing = unit(1, \"cm\")\n  )"
  },
  {
    "objectID": "assignments/bayesian_modeling.html#exploring-model-results",
    "href": "assignments/bayesian_modeling.html#exploring-model-results",
    "title": "Bayesian Modeling",
    "section": "Exploring model results",
    "text": "Exploring model results\nWe can extract the posterior draws from our Bayesian models with the as_draws_df() function.\n\n\nCode\nas_draws_df(fit1.b)\n\n\n# A draws_df: 1000 iterations, 4 chains, and 6 variables\n   b_Intercept b_body_mass_g sigma Intercept lprior lp__\n1           35        0.0036   2.9        48   -4.3 -173\n2           33        0.0043   3.1        49   -4.3 -172\n3           32        0.0043   3.1        48   -4.4 -172\n4           29        0.0051   2.7        49   -4.3 -172\n5           33        0.0043   3.2        49   -4.4 -172\n6           33        0.0041   3.1        48   -4.4 -173\n7           33        0.0044   3.3        49   -4.4 -172\n8           34        0.0041   2.9        49   -4.3 -172\n9           28        0.0057   2.9        49   -4.3 -172\n10          37        0.0033   3.0        49   -4.3 -172\n# ... with 3990 more draws\n# ... hidden reserved variables {'.chain', '.iteration', '.draw'}\n\n\nNote the meta data. We can get a sense of the full posterior distributions of the \\(\\beta\\) parameters with plots.\n\n\nCode\n# wrangle\nas_draws_df(fit1.b) %&gt;% \n  pivot_longer(starts_with(\"b_\")) %&gt;% \n  \n  # plot!\n  ggplot(aes(x = value)) + \n  # geom_density(fill = \"grey20\") +\n  geom_histogram(bins = 40) +\n  facet_wrap(~ name, scales = \"free\")\n\n\n\n\n\n\n\n\n\nWe might summarize those posterior distributions with basic descriptive statistics, like their means, SD’s, and inner 95-percentile range.\n\n\nCode\nas_draws_df(fit1.b) %&gt;% \n  pivot_longer(starts_with(\"b_\")) %&gt;% \n  group_by(name) %&gt;% \n  summarise(mean = mean(value),\n            sd = sd(value),\n            ll = quantile(value, probs = 0.025),\n            ul = quantile(value, probs = 0.975))\n\n\n# A tibble: 2 × 5\n  name              mean       sd       ll       ul\n  &lt;chr&gt;            &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 b_Intercept   32.2     3.56     25.3     39.4    \n2 b_body_mass_g  0.00445 0.000949  0.00254  0.00624\n\n\nNotice how these values match up exactly with those from fixef().\n\n\nCode\nfixef(fit1.b)\n\n\n                Estimate    Est.Error         Q2.5        Q97.5\nIntercept   32.243201279 3.5578978933 25.324542044 39.384829020\nbody_mass_g  0.004452674 0.0009491811  0.002544748  0.006237691\n\n\nThus,\n\nThe Bayesian posterior mean is analogous to the frequentist point estimate.\nThe Bayesian posterior SD is analogous to the frequentist standard error.\nThe Bayesian posterior percentile-based 95% (credible) interval is analogous to the frequentist 95% confidence interval.\n\nThese are not exactly the same, mind you. But they serve similar functions.\nWe can also get a sense of these distributions with the plot() function.\n\n\nCode\nplot(fit1.b)\n\n\n\n\n\n\n\n\n\nIgnore the trace plots on the right for a moment. And let’s consider the pairs() plot.\n\n\nCode\npairs(fit1.b)\n\n\n\n\n\n\n\n\n\nCode\n# we can adjust some of the settings with the off_diag_args argument\npairs(fit1.b, off_diag_args = list(size = 1/4, alpha = 1/4))\n\n\n\n\n\n\n\n\n\n\nQuestion 2.1 : In the parlance of Probability, do you know what is the term by which the distributions in the diagonal of the above plot are known as? And the distributions in the off-diagonal?\nThe distributions in the diagonal of the pairs() plot are known as marginal distributions. A marginal distribution shows the probability distribution of a single variable while averaging or integrating over all other variables in the joint distribution.\nThe off-diagnal plots are known as joint distributions. A joint distribution shows the probability distribution of two or more random variables simultaneously.\nNotice how the two \\(\\beta\\) parameters seem to have a strong negative correlation. We can quantify that correlation with the vcov() function.\n\n\nCode\nvcov(fit1.b)                      # variance/covariance metric\n\n\n               Intercept   body_mass_g\nIntercept   12.658637419 -3.360992e-03\nbody_mass_g -0.003360992  9.009448e-07\n\n\nCode\nvcov(fit1.b, correlation = TRUE)  # correlation metric\n\n\n             Intercept body_mass_g\nIntercept    1.0000000  -0.9952334\nbody_mass_g -0.9952334   1.0000000\n\n\nThis correlation/covariance among the parameters is not unique to Bayesian models. Here’s the vcov() output for the OLS model.\n\n\nCode\nvcov(fit1.ols)  # variance/covariance metric\n\n\n             (Intercept)   body_mass_g\n(Intercept) 11.856743861 -3.143295e-03\nbody_mass_g -0.003143295  8.420092e-07\n\n\nI’m not aware of an easy way to get that output in a correlation metric for our OLS model. Here’s how to compute the correlation by hand.\n\n\nCode\ncov_xy &lt;- vcov(fit1.ols)[2, 1]  # covariance between the intercept and slope\nvar_x  &lt;- vcov(fit1.ols)[1, 1]  # variance for the intercept\nvar_y  &lt;- vcov(fit1.ols)[2, 2]  # variance for the slope\n\n# convert the covariance into a correlation\ncov_xy / (sqrt(var_x) * sqrt(var_y))\n\n\n[1] -0.9948188\n\n\nThat code follows the definition of a covariance, which can be expressed as\n\\[\n\\text{Cov}(x, y) = \\rho \\sigma_x \\sigma_y,\n\\]\nwhere \\(\\sigma_x\\) is the standard deviation for x, \\(\\sigma_y\\) is the standard deviation for y, and \\(\\rho\\) is their correlation. And thus, you can convert a covariance into a correlation with the formula\n\\[\n\\rho = \\frac{\\sigma_{xy}}{\\sigma_x \\sigma_y},\n\\]\nwhere \\(\\sigma_{xy}\\) is the covariance of x and y."
  },
  {
    "objectID": "assignments/bayesian_modeling.html#draws",
    "href": "assignments/bayesian_modeling.html#draws",
    "title": "Bayesian Modeling",
    "section": "Draws",
    "text": "Draws\nLet’s save the as_draws_df() output for our model as an object called draws.\n\n\nCode\ndraws &lt;- as_draws_df(fit1.b)\nglimpse(draws)\n\n\nRows: 4,000\nColumns: 9\n$ b_Intercept   &lt;dbl&gt; 34.78616, 33.00230, 32.33875, 29.35202, 32.80183, 33.048…\n$ b_body_mass_g &lt;dbl&gt; 0.003616358, 0.004321150, 0.004296630, 0.005135446, 0.00…\n$ sigma         &lt;dbl&gt; 2.864046, 3.086035, 3.144576, 2.717955, 3.223488, 3.1235…\n$ Intercept     &lt;dbl&gt; 48.28635, 49.13353, 48.37845, 48.52309, 48.98730, 48.168…\n$ lprior        &lt;dbl&gt; -4.327721, -4.315584, -4.387382, -4.266907, -4.357681, -…\n$ lp__          &lt;dbl&gt; -172.5145, -171.5111, -172.1725, -171.7198, -171.8164, -…\n$ .chain        &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ .iteration    &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ .draw         &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n\n\nFor each parameter in the model, we have 4,000 draws from the posterior.\n\nQuestion 2.2: How does this concept relate to representing uncertainty? Can you anticipate how predictions are made based upon these 4000 draws and the linear regression formula?\nEach draw represents a plausible set of parameter values (β₀, β₁, σ) that is consistent with both our prior beliefs and the actual data. The variation across these draws directly quantifies our uncertainty about the “true” parameters.The more draws we have, the more we can capture the variability in the posterior distribution of the parameters.\nPredictions are made by plugging the draws into the linear regression formula. For each draw, we can compute a predicted value of bill_length_mm given a specific value of body_mass_g. This results in a distribution of predicted values, which reflects the uncertainty in our predictions.\n\\[\\widehat{\\text{bill_length_mm}}_i = \\beta_0 + \\beta_1 \\text{body_mass_g}_i.\\]\nLet’s break the 4000 draws down with our draws object.\n\n\nCode\n# adjust the parameter names \ndraws &lt;- draws %&gt;% \n  mutate(beta0 = b_Intercept,\n         beta1 = b_body_mass_g)\n\n# Note: go through this one line at a time\ndraws %&gt;% \n  select(.draw, beta0, beta1) %&gt;% \n  mutate(body_mass_g = mean(chinstrap$body_mass_g)) %&gt;% \n  mutate(y_hat = beta0 + beta1 * body_mass_g) %&gt;% \n  \n  ggplot(aes(x = y_hat)) +\n  geom_histogram(bins = 40) +\n  labs(title = \"Bayesians have posterior distributions\",\n       x = expression(hat(italic(y))*'|'*italic(x)==3733.1)) +\n  coord_cartesian(xlim = c(47, 51))\n\n\n\n\n\n\n\n\n\nHere’s what that is for the OLS model.\n\n\nCode\npredict(fit1.ols,\n        newdata = tibble(body_mass_g = mean(chinstrap$body_mass_g)),\n        interval = \"confidence\") %&gt;% \n  data.frame() %&gt;% \n  \n  ggplot(aes(x = fit, xmin = lwr, xmax = upr, y = 0)) +\n  geom_pointrange() +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(title = \"Frequentists have point estmates and 95% CI's\",\n       x = expression(hat(italic(y))*'|'*italic(x)==3733.1)) +\n  coord_cartesian(xlim = c(47, 51))\n\n\n\n\n\n\n\n\n\nAnother handy way to present a Bayesian posterior is as a density with a point-interval summary below.\n\n\nCode\nlibrary(ggdist) #for stat_half_eye and mean_qi\ndraws %&gt;% \n  mutate(body_mass_g = mean(chinstrap$body_mass_g)) %&gt;% \n  mutate(y_hat = beta0 + beta1 * body_mass_g) %&gt;% \n  \n  ggplot(aes(x = y_hat)) +\n  stat_halfeye(point_interval = mean_qi, .width = .95) +\n  # scale_y_continuous(NULL, breaks = NULL) +\n  labs(title = \"Bayesians have posterior distributions\",\n       x = expression(hat(italic(y))*'|'*italic(x)==3733.1)) +\n  coord_cartesian(xlim = c(47, 51))\n\n\n\n\n\n\n\n\n\nThe dot at the base of the plot is the posterior mean, and the horizontal line marks the 95% percentile-based interval. If you’d like to mark the median instead, set point_interval = median_qi. If you’re like a different kind of horizontal interval, adjust the .width argument.\n\n\nCode\ndraws %&gt;% \n  mutate(body_mass_g = mean(chinstrap$body_mass_g)) %&gt;% \n  mutate(y_hat = beta0 + beta1 * body_mass_g) %&gt;% \n  \n  ggplot(aes(x = y_hat)) +\n  # note the changes to this line\n  stat_halfeye(point_interval = median_qi, .width = c(.5, .99)) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(title = \"Bayesians have posterior distributions\",\n       subtitle = \"The dot marks the median.\\nThe thicker line marks the 50% interval, and\\nthe thinner line marks the 99% interval.\",\n       x = expression(hat(italic(y))*'|'*italic(x)==3733.1)) +\n  coord_cartesian(xlim = c(47, 51))"
  },
  {
    "objectID": "assignments/bayesian_modeling.html#about-those-means-sds-and-intervals.",
    "href": "assignments/bayesian_modeling.html#about-those-means-sds-and-intervals.",
    "title": "Bayesian Modeling",
    "section": "About those means, SD’s, and intervals.",
    "text": "About those means, SD’s, and intervals.\nYou can describe a Bayesian posterior in a lot of different ways. Earlier we said the posterior mean is the Bayesian point estimate. This isn’t strictly true. Means are very popular, but you can summarize a posterior by its mean, median, or mode.\nLet’s see what this looks like in practice. First, we compute and save our statistics for each of our model parameters.\n\n\nCode\npoints &lt;- draws %&gt;% \n  rename(`beta[0]` = beta0,\n         `beta[1]` = beta1) %&gt;% \n  pivot_longer(cols = c(`beta[0]`, `beta[1]`, sigma), \n               names_to = \"parameter\") %&gt;% \n  group_by(parameter) %&gt;% \n  summarise(mean = mean(value),\n            median = median(value),\n            mode = Mode(value)) %&gt;% \n  pivot_longer(starts_with(\"m\"), names_to = \"statistic\")\n\n# what?\npoints\n\n\n# A tibble: 9 × 3\n  parameter statistic    value\n  &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;\n1 beta[0]   mean      32.2    \n2 beta[0]   median    32.2    \n3 beta[0]   mode      32.5    \n4 beta[1]   mean       0.00445\n5 beta[1]   median     0.00445\n6 beta[1]   mode       0.00434\n7 sigma     mean       2.92   \n8 sigma     median     2.90   \n9 sigma     mode       2.85   \n\n\nNow plot.\n\n\nCode\ndraws %&gt;% \n  rename(`beta[0]` = beta0,\n         `beta[1]` = beta1) %&gt;% \n  pivot_longer(cols = c(`beta[0]`, `beta[1]`, sigma), \n               names_to = \"parameter\") %&gt;% \n  \n  ggplot(aes(x = value)) +\n  geom_density() +\n  geom_vline(data = points,\n             aes(xintercept = value, color = statistic),\n             size = 3/4) +\n  scale_color_viridis_d(option = \"A\", end = .8) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(\"parameter space\") +\n  facet_wrap(~ parameter, labeller = label_parsed, scales = \"free\", ncol = 1) +\n  theme(strip.text = element_text(size = 14))\n\n\n\n\n\n\n\n\n\n\nQuestion 2.3: Discuss the skew in \\(\\sigma\\).Why it might arise, etc.?\nThe skew in \\(\\sigma\\) is because that the standard deviation is a measure of variability, and it is always non-negative. In the context of Bayesian modeling, the posterior distribution of \\(\\sigma\\) is often skewed due to the influence of the data and the prior distribution.\nIn this case, the skewness of the posterior distribution of \\(\\sigma\\) suggests that there is a higher probability of observing smaller values of \\(\\sigma\\), which indicates that the model is fitting the data well. The skewness of the posterior distribution of \\(\\sigma\\) can also be influenced by the choice of prior distribution. If a prior distribution is more informative about the expected variability in the response variable, this can lead to a more (right)skewed posterior distribution.\n\n\nMean, median, and mode.\n\nThe mean is the brms default summary, and McElreath (2015, 2020) defaulted to the mean in his texts.\nThe median is also available for many brms functions, and it’s what Gelman et al (2020) recommend.\nThe mode can be attractive for very skewed distributions, and it’s what Kruschke (2015) used in his text.\n\nWith many brms functions, you can request the median by setting robust = TRUE. For example:\n\n\nCode\nfixef(fit1.b)                 # means\n\n\n                Estimate    Est.Error         Q2.5        Q97.5\nIntercept   32.243201279 3.5578978933 25.324542044 39.384829020\nbody_mass_g  0.004452674 0.0009491811  0.002544748  0.006237691\n\n\nCode\nfixef(fit1.b, robust = TRUE)  # medians\n\n\n                Estimate    Est.Error         Q2.5        Q97.5\nIntercept   32.249557421 3.5028012144 25.324542044 39.384829020\nbody_mass_g  0.004454945 0.0009365675  0.002544748  0.006237691\n\n\n\n\nQuestion 2.4: Given the skew in sigma and what you know about summary statistics, what might be the implication of using just the mean, median, or mode of posteriors to make a prediction?\nUsing just the mean, median, or mode of posteriors to make a prediction can lead to different interpretations of the data, especially in the presence of skewness. The mean is sensitive to extreme values and can be influenced by outliers, which may not accurately represent the central tendency of the data. The median, on the other hand, is more robust to outliers and provides a better measure of central tendency in skewed distributions. The mode represents the most frequently occurring value in the distribution, which may not be representative of the overall distribution if the distribution is multimodal.\n\nSD’s and MAD SD’s.\nEarlier we said the posterior SD is the Bayesian standard error. This isn’t strictly true. You can also use the median absolute deviation (MAD SD). If we let \\(M\\) stand for the median of some variable \\(y\\), which varies across \\(i\\) cases, we can define the MAD SD as\n\\[\\textit{MAD SD} = 1.4826 \\times \\operatorname{median}_{i = 1}^n |y_i - M|,\\]\nwhere \\(1.4826\\) is a constant that scales the MAD SD into a standard-deviation metric. Here’s what this looks like in practice.\n\n\nCode\n# go through this line by line\ndraws %&gt;% \n  select(beta0) %&gt;% \n  mutate(mdn = median(beta0)) %&gt;% \n  mutate(`|yi - mdn|` = abs(beta0 - mdn)) %&gt;% \n  summarise(MAD_SD = 1.4826 * median(`|yi - mdn|`))\n\n\n# A tibble: 1 × 1\n  MAD_SD\n   &lt;dbl&gt;\n1   3.50\n\n\nBase R also has a mad() function.\n\n\nCode\n?mad\n\n\nHelp on topic 'mad' was found in the following packages:\n\n  Package               Library\n  posterior             /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library\n  stats                 /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library\n\n\nUsing the first match ...\n\n\nCode\ndraws %&gt;% \n  summarise(MAD_SD = mad(beta0))\n\n\n# A tibble: 1 × 1\n  MAD_SD\n   &lt;dbl&gt;\n1   3.50\n\n\nYou can request the MAD SD from many brms functions by setting robust = TRUE.\n\n\nCode\nfixef(fit1.b)                 # SD\n\n\n                Estimate    Est.Error         Q2.5        Q97.5\nIntercept   32.243201279 3.5578978933 25.324542044 39.384829020\nbody_mass_g  0.004452674 0.0009491811  0.002544748  0.006237691\n\n\nCode\nfixef(fit1.b, robust = TRUE)  # MAD SD\n\n\n                Estimate    Est.Error         Q2.5        Q97.5\nIntercept   32.249557421 3.5028012144 25.324542044 39.384829020\nbody_mass_g  0.004454945 0.0009365675  0.002544748  0.006237691\n\n\n\nTo my eye, many authors (e.g., Kruschke, McElreath) just use the SD.\nGelman et al (see Section 5.3) recommend the MAD SD.\n\n\n\nBayesian intervals.\nBayesians describe the widths of their posteriors with intervals. I’ve seen these variously described as confidence intervals, credible intervals, probability intervals, and even uncertainty intervals. My recommendation is just pick a term, and clearly tell your audience what you mean (e.g., at the end of a Method section in a journal article).\nTo my eye, the most popular interval is a 95% percentile-based interval. 95% is conventional, perhaps due to the popularity of the 95% frequentist confidence interval, which is related to the 0.05 alpha level used for the conventional \\(p\\)-value cutoff. However, you can use other percentiles. Some common alternatives are 99%, 89%, 80%, and 50%.\nAlso, Bayesian intervals aren’t always percentile based. An alternative is the highest posterior density interval (HPDI), which has mathematical properties some find desirable.\nbrms only supports percentile-based intervals, but it does allow for a variety of different ranges via the prob argument. For example, here’s how to request 80% intervals in summary().\n\n\nCode\nsummary(fit1.b, prob = .80)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: bill_length_mm ~ 1 + body_mass_g \n   Data: chinstrap (Number of observations: 68) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-80% CI u-80% CI Rhat Bulk_ESS Tail_ESS\nIntercept      32.24      3.56    27.72    36.91 1.00     4789     3248\nbody_mass_g     0.00      0.00     0.00     0.01 1.00     4883     3111\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-80% CI u-80% CI Rhat Bulk_ESS Tail_ESS\nsigma     2.92      0.26     2.60     3.27 1.00     1897     1916\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nRegarding interval widths:\n\n95% Intervals are widely used.\nMcElreat likes 89% intervals, and uses them as a default in his rethinking package.\nSome of the bayesplot, ggdist, and tidybayes functions return 80% intervals.\nSome of the ggdist, and tidybayes functions return 66% or 50% intervals.\nI’ve heard Gelman report his fondness for 50% intervals on his blog (https://statmodeling.stat.columbia.edu/2016/11/05/why-i-prefer-50-to-95-intervals/).\n\nRegarding interval types:\n\nPercentile-based intervals are widely used in the Stan ecosystem, and are supported in texts like Gelman et al.\nKruschke has consistently advocates for HPDI’s in his articles, and in his text.\n\n\n\n\nPosterior summaries with tidybayes.\nMatthew Kay’s tidybayes package (https://mjskay.github.io/tidybayes/) offers an array of convenience functions for summarizing posterior distributions with points and intervals. See the Point summaries and intervals section of Kay’s Extracting and visualizing tidy draws from brms models vignette (https://mjskay.github.io/tidybayes/articles/tidy-brms.html#point-summaries-and-intervals) for a detailed breakdown. In short, the family of functions use the naming scheme [median|mean|mode]_[qi|hdi]. Here are a few examples.\n\n\nCode\ndraws %&gt;% mean_qi(beta0)                        # mean and 95% percentile interval\n\n\n# A tibble: 1 × 6\n  beta0 .lower .upper .width .point .interval\n  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1  32.2   25.3   39.4   0.95 mean   qi       \n\n\nCode\ndraws %&gt;% median_qi(beta0, .width = .80)        # median and 80% percentile interval\n\n\n# A tibble: 1 × 6\n  beta0 .lower .upper .width .point .interval\n  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1  32.2   27.7   36.9    0.8 median qi       \n\n\nCode\ndraws %&gt;% mode_hdi(beta0, .width = c(.5, .95))  # mode, with 95 and 50% HPDI's\n\n\n# A tibble: 2 × 6\n  beta0 .lower .upper .width .point .interval\n  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1  32.5   29.5   34.2   0.5  mode   hdi      \n2  32.5   25.6   39.6   0.95 mode   hdi      \n\n\nAs an aside, the Mode() function we used a while back was also from tidybayes.\n\n\nSpaghetti plots.\nRemember how we said the draw was something like 4,000 separate equations for our Bayesian model? Let’s see that again.\n\n\nCode\ndraws %&gt;% \n  select(.draw, beta0, beta1) %&gt;% \n  mutate(body_mass_g = mean(chinstrap$body_mass_g)) %&gt;% \n  # here's the equation\n  mutate(y_hat = beta0 + beta1 * body_mass_g) %&gt;% \n  # subset the top 6\n  head()\n\n\n# A tibble: 6 × 5\n  .draw beta0   beta1 body_mass_g y_hat\n  &lt;int&gt; &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1     1  34.8 0.00362       3733.  48.3\n2     2  33.0 0.00432       3733.  49.1\n3     3  32.3 0.00430       3733.  48.4\n4     4  29.4 0.00514       3733.  48.5\n5     5  32.8 0.00434       3733.  49.0\n6     6  33.0 0.00405       3733.  48.2\n\n\nOne way we might emphasize the 4,000 equations is with a spaghetti plot. When we display the fitted line for bill_length_mm over the range of body_mass_g values, we can display a single line for each posterior draw. Here’s what that can look like.\n\n\nCode\nrange(chinstrap$body_mass_g)\n\n\n[1] 2700 4800\n\n\nCode\n# Note: go through this one line at a time\ndraws %&gt;% \n  select(.draw, beta0, beta1) %&gt;% \n  expand_grid(body_mass_g = range(chinstrap$body_mass_g)) %&gt;% \n  mutate(y_hat = beta0 + beta1 * body_mass_g) %&gt;% \n  \n  # plot!\n  ggplot(aes(x = body_mass_g, y = y_hat, group = .draw)) +\n  geom_line(linewidth = 1/10, alpha = 1/10)\n\n\n\n\n\n\n\n\n\nIt might be easier to see what’s going on with a random subset of, say, 10 of the posterior draws.\n\n\nCode\nset.seed(10)\n\ndraws %&gt;% \n  # take a random sample of 10 rows\n  slice_sample(n = 10) %&gt;% \n  select(.draw, beta0, beta1) %&gt;% \n  expand_grid(body_mass_g = range(chinstrap$body_mass_g)) %&gt;% \n  mutate(y_hat = beta0 + beta1 * body_mass_g) %&gt;% \n  \n  ggplot(aes(x = body_mass_g, y = y_hat, group = .draw)) +\n  geom_line(linewidth = 1/2, alpha = 1/2)\n\n\n\n\n\n\n\n\n\nWhile we’re at it, let’s take 20 draws and do a little color coding.\n\n\nCode\nset.seed(20)\n\ndraws %&gt;% \n  # take a random sample of 20 rows\n  slice_sample(n = 20) %&gt;% \n  select(.draw, beta0, beta1) %&gt;% \n  expand_grid(body_mass_g = range(chinstrap$body_mass_g)) %&gt;% \n  mutate(y_hat = beta0 + beta1 * body_mass_g) %&gt;% \n  \n  ggplot(aes(x = body_mass_g, y = y_hat, group = .draw, color = beta0)) +\n  geom_line() +\n  scale_color_viridis_c(expression(beta[0]~(the~intercept)), end = .9)\n\n\n\n\n\n\n\n\n\nDo you remember how we said \\(\\beta_0\\) and \\(\\beta_1\\) had a strong negative correlation? Notice how the lines computed by lower \\(\\beta_0\\) values also tend to have higher slopes. This will happen all the time with conventional regression models.\n\n\nQuestion 2.5: We have done all this without yet specifying a prior. What do you think is going on?\nThe model is using the dataset to inform the posterior distributions of the parameters. The model is able to learn from the data and adjust the parameter estimates accordingly, even without specifying a prior. This is because the likelihood function, which describes the relationship between the data and the parameters, is strong enough to dominate the prior distribution."
  },
  {
    "objectID": "assignments/bayesian_modeling.html#questionexercise",
    "href": "assignments/bayesian_modeling.html#questionexercise",
    "title": "Bayesian Modeling",
    "section": "Question/Exercise:",
    "text": "Question/Exercise:\nIn the last part, we made a subset of the penguins data called gentoo, which was only the cases for which species == \"Gentoo\". Do that again and refit the Bayesian model to those data. Can you then remake some of the figures in this file with the new version of the model?\n\n\nCode\n# fit the model\ngentoo_model &lt;- brm(\n  data = gentoo,\n  bill_length_mm ~ 1 + body_mass_g,\n  iter = 2000,\n  chains = 2\n)\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 3.2e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.32 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.035 seconds (Warm-up)\nChain 1:                0.025 seconds (Sampling)\nChain 1:                0.06 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.038 seconds (Warm-up)\nChain 2:                0.026 seconds (Sampling)\nChain 2:                0.064 seconds (Total)\nChain 2: \n\n\nCode\nsummary(gentoo_model)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: bill_length_mm ~ 1 + body_mass_g \n   Data: gentoo (Number of observations: 123) \n  Draws: 2 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 2000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      26.84      2.18    22.56    31.18 1.00     2075     1396\nbody_mass_g     0.00      0.00     0.00     0.00 1.00     2118     1356\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     2.32      0.15     2.04     2.64 1.00      818      781\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\nCode\n#posterior draws\n\ngentoo_draws &lt;- as_draws_df(gentoo_model)\n\ngentoo_draws &lt;- gentoo_draws %&gt;% \n  mutate(beta0 = b_Intercept,\n         beta1 = b_body_mass_g)\n\nposterior_plot &lt;- gentoo_draws %&gt;% \n  pivot_longer(starts_with(\"b_\")) %&gt;% \n  \n  ggplot(aes(x = value)) + \n  geom_histogram(bins = 40) +\n  facet_wrap(~ name, scales = \"free\") +\n  labs(title = \"Posterior Distributions for Gentoo Model Parameters\",\n       x = \"Parameter Value\",\n       y = \"Count\")\nprint(posterior_plot)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# pposterior prediction at mean body mass\nmean_prediction &lt;- gentoo_draws %&gt;% \n  mutate(body_mass_g = mean(gentoo$body_mass_g, na.rm = TRUE)) %&gt;% \n  mutate(y_hat = beta0 + beta1 * body_mass_g) %&gt;% \n  \n  ggplot(aes(x = y_hat)) +\n  stat_halfeye(point_interval = mean_qi, .width = .95) +\n  labs(title = \"Posterior Predictive Distribution for Gentoo Penguins\",\n       subtitle = paste(\"Bill length prediction at mean body mass =\", \n                        round(mean(gentoo$body_mass_g, na.rm = TRUE), 1), \"g\"),\n       x = \"Predicted Bill Length (mm)\",\n       y = \"Density\")\nprint(mean_prediction)"
  },
  {
    "objectID": "assignments/bayesian_modeling.html#references",
    "href": "assignments/bayesian_modeling.html#references",
    "title": "Bayesian Modeling",
    "section": "References",
    "text": "References\nGelman, A., Hill, J., & Vehtari, A. (2020). Regression and other stories. Cambridge University Press. https://doi.org/10.1017/9781139161879\nKruschke, J. K. (2015). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press. https://sites.google.com/site/doingbayesiandataanalysis/\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (Second Edition). CRC Press. https://xcelab.net/rm/statistical-rethinking/\nMcElreath, R. (2015). Statistical rethinking: A Bayesian course with examples in R and Stan. CRC press. https://xcelab.net/rm/statistical-rethinking/"
  },
  {
    "objectID": "assignments/bayesian_modeling.html#session-information",
    "href": "assignments/bayesian_modeling.html#session-information",
    "title": "Bayesian Modeling",
    "section": "Session information",
    "text": "Session information\n\n\nCode\nsessionInfo()\n\n\nR version 4.4.3 (2025-02-28)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sequoia 15.3.2\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] ggdist_3.3.2        broom.mixed_0.2.9.6 broom_1.0.7        \n [4] brms_2.22.0         Rcpp_1.0.13-1       ggside_0.3.1       \n [7] lubridate_1.9.3     forcats_1.0.0       stringr_1.5.1      \n[10] dplyr_1.1.4         purrr_1.0.2         readr_2.1.5        \n[13] tidyr_1.3.1         tibble_3.2.1        ggplot2_3.5.1      \n[16] tidyverse_2.0.0    \n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.2.1     viridisLite_0.4.2    farver_2.1.2        \n [4] loo_2.8.0            fastmap_1.2.0        tensorA_0.36.2.1    \n [7] digest_0.6.37        estimability_1.5.1   timechange_0.3.0    \n[10] lifecycle_1.0.4      StanHeaders_2.32.10  processx_3.8.4      \n[13] magrittr_2.0.3       posterior_1.6.0      compiler_4.4.3      \n[16] rlang_1.1.4          tools_4.4.3          utf8_1.2.4          \n[19] yaml_2.3.10          knitr_1.49           labeling_0.4.3      \n[22] bridgesampling_1.1-2 htmlwidgets_1.6.4    curl_6.0.0          \n[25] pkgbuild_1.4.5       plyr_1.8.9           abind_1.4-8         \n[28] withr_3.0.2          grid_4.4.3           stats4_4.4.3        \n[31] fansi_1.0.6          xtable_1.8-4         colorspace_2.1-1    \n[34] future_1.34.0        inline_0.3.21        emmeans_1.10.5      \n[37] globals_0.16.3       scales_1.3.0         cli_3.6.3           \n[40] mvtnorm_1.3-2        rmarkdown_2.29       generics_0.1.3      \n[43] RcppParallel_5.1.10  rstudioapi_0.17.1    reshape2_1.4.4      \n[46] tzdb_0.4.0           rstan_2.32.6         splines_4.4.3       \n[49] bayesplot_1.11.1     parallel_4.4.3       matrixStats_1.5.0   \n[52] vctrs_0.6.5          V8_6.0.1             Matrix_1.7-2        \n[55] jsonlite_1.8.9       callr_3.7.6          hms_1.1.3           \n[58] listenv_0.9.1        glue_1.8.0           parallelly_1.42.0   \n[61] ps_1.8.1             codetools_0.2-20     distributional_0.5.0\n[64] stringi_1.8.4        gtable_0.3.6         QuickJSR_1.5.1      \n[67] palmerpenguins_0.1.1 munsell_0.5.1        pillar_1.9.0        \n[70] furrr_0.3.1          htmltools_0.5.8.1    Brobdingnag_1.2-9   \n[73] R6_2.5.1             evaluate_1.0.1       lattice_0.22-6      \n[76] backports_1.5.0      rstantools_2.4.0     coda_0.19-4.1       \n[79] gridExtra_2.3        nlme_3.1-167         checkmate_2.3.2     \n[82] mgcv_1.9-1           xfun_0.49            pkgconfig_2.0.3"
  },
  {
    "objectID": "assignments/bayesian_modeling_part2.html",
    "href": "assignments/bayesian_modeling_part2.html",
    "title": "Bayesian Modeling Part 2",
    "section": "",
    "text": "For Lab 1, you had explored the data and looked at models built via lm() and via brms(using default priors). You had also drawn posterior samples after fitting the model.\nFor Lab 2, we continue with the Palmer Penguins. And we will look more at distributions and priors.\nAgain, there will be conceptual questions to answer as you work through this example, and exercises."
  },
  {
    "objectID": "assignments/bayesian_modeling_part2.html#setup-packages-and-data",
    "href": "assignments/bayesian_modeling_part2.html#setup-packages-and-data",
    "title": "Bayesian Modeling Part 2",
    "section": "Setup: Packages and data",
    "text": "Setup: Packages and data\nWe load the primary packages.\n\n\nCode\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(tidybayes)\nlibrary(ggdist)\n\n\nWe want the same data set up as in the last lab.\n\n\nCode\n# load the penguins data\ndata(penguins, package = \"palmerpenguins\")\n\n# subset the data\nchinstrap &lt;- penguins %&gt;% \n  filter(species == \"Chinstrap\")\n\nglimpse(chinstrap)\n\n\nRows: 68\nColumns: 8\n$ species           &lt;fct&gt; Chinstrap, Chinstrap, Chinstrap, Chinstrap, Chinstra…\n$ island            &lt;fct&gt; Dream, Dream, Dream, Dream, Dream, Dream, Dream, Dre…\n$ bill_length_mm    &lt;dbl&gt; 46.5, 50.0, 51.3, 45.4, 52.7, 45.2, 46.1, 51.3, 46.0…\n$ bill_depth_mm     &lt;dbl&gt; 17.9, 19.5, 19.2, 18.7, 19.8, 17.8, 18.2, 18.2, 18.9…\n$ flipper_length_mm &lt;int&gt; 192, 196, 193, 188, 197, 198, 178, 197, 195, 198, 19…\n$ body_mass_g       &lt;int&gt; 3500, 3900, 3650, 3525, 3725, 3950, 3250, 3750, 4150…\n$ sex               &lt;fct&gt; female, male, male, female, male, female, female, ma…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…"
  },
  {
    "objectID": "assignments/bayesian_modeling_part2.html#models",
    "href": "assignments/bayesian_modeling_part2.html#models",
    "title": "Bayesian Modeling Part 2",
    "section": "Models",
    "text": "Models\nOnce again, we’ll fit the model\n\\[\n\\begin{align}\n\\text{bill_length_mm}_i & = \\beta_0 + \\beta_1 \\text{body_mass_g}_i + \\epsilon_i \\\\\n\\epsilon_i & \\sim \\operatorname{Normal}(0, \\sigma_\\epsilon) ,\n\\end{align}\n\\]\nwith both lm() and brm().\n\n\nCode\n# OLS\nfit1.ols &lt;- lm(\n  data = chinstrap,\n  bill_length_mm ~ 1 + body_mass_g\n)\n\n# Bayes\nfit1.b &lt;- brm(\n  data = chinstrap,\n  bill_length_mm ~ 1 + body_mass_g\n)"
  },
  {
    "objectID": "assignments/bayesian_modeling_part2.html#bayesians-have-many-kinds-of-distributions",
    "href": "assignments/bayesian_modeling_part2.html#bayesians-have-many-kinds-of-distributions",
    "title": "Bayesian Modeling Part 2",
    "section": "Bayesians have many kinds of distributions",
    "text": "Bayesians have many kinds of distributions\nIn Bayesian statistics, we have at least 6 distributions to keep track of. Those are:\n\nthe likelihood distributions\nthe prior parameter distribution (aka priors)\nthe prior predictive distributions\nthe posterior parameter distributions (aka posteriors)\nthe posterior-predictive distribution\n\nIn many respect, it’s distributions ‘all the way down,’ with Bayesians. This can be indeed be difficult to keep track of at first. But since this is true for any class of Bayesian models (not just regression), you’ll hopefully get used to it.\n\n\nQUESTION 1: How would you represent these 6 distributions mathematically, using \\(P_0\\)’\\(P\\), \\(D\\), \\(|\\), and \\(\\theta\\) ?\n\n\n\n\n\n\nTip\n\n\n\nHint 1: Many of these terms were in the Bayes Rule.\n\n\n\n\nAnswer:\n\nLikelihood distribution: \\(P(D|\\theta)\\), which is the probability of observing data \\(D\\) given parameters \\(\\theta\\).\nPriors: \\(P_0(\\theta)\\), which is the prior belief about parameters before seeing what the data looks like.\nPrior predictive distribution: \\(P_0(D) = \\int P(D|\\theta)P_0(\\theta)d\\theta\\), which is the expected distribution of the data given only prior beliefs.\nPosteriors: \\(P(\\theta|D) = \\frac{P(D|\\theta)P_0(\\theta)}{P(D)}\\), which is the updated beliefs about parameters after seeing data.\nPosterior predictive distribution: \\(P(D_{\\text{new}}|D) = \\int P(D_{\\text{new}}|\\theta)P(\\theta|D)d\\theta\\), which is the expected distribution of new data given the posteriors.\n\nWe also have some other distributions that follow from these. For example, - the distributions of the model expectations (i.e., the predicted means)\n\n\nLikelihood distributions.\nWe are approaching Bayesian statistics from a likelihood-based perspective. That is, we situate regression models within the greater context of a likelihood function. (There are ways to do non-parametric Bayesian statistics, which don’t focus on likelihoods. We won’t get into that right now.)\nSo far, we have been using the conventional Gaussian likelihood. If we have some variable \\(y\\), we can express it as normally distributed by\n\\[\n\\operatorname{Normal}(y \\mid \\mu, \\sigma) = \\frac{1}{\\sqrt{2 \\pi \\sigma}} \\exp \\left( \\frac{1}{2} \\left( \\frac{y - \\mu}{\\sigma}\\right)^2\\right),\n\\]\nwhere \\(\\mu\\) is the mean and \\(\\sigma\\) is the standard deviation. With this likelihood,\n\n\\(\\mu \\in \\mathbb R\\)\n\nthe mean can be any real number, ranging from \\(-\\infty\\) to \\(\\infty\\)\n\n\\(\\sigma \\in \\mathbb R_{&gt; 0}\\)\n\nthe standard deviation can take on any real number greater than zero.\n\n\nIt’s also the assumption\n\n\\(y \\in \\mathbb R\\)\n\nthe focal variable \\(y\\) can be any real number, ranging from \\(-\\infty\\) to \\(\\infty\\).\n\n\nOne of the ways we wrote our model formula back in the first file was\n\\[\n\\begin{align}\n\\text{bill_length_mm}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i & = \\beta_0 + \\beta_1 \\text{body_mass_g}_i,\n\\end{align}\n\\]\nand further in the discussion, we updated that equation with the posterior means for our three parameters to\n\\[\n\\begin{align}\n\\text{bill_length_mm}_i & \\sim \\operatorname{Normal}(\\mu_i, 2.92) \\\\\n\\mu_i & = 32.2 + 0.004 \\text{body_mass_g}_i.\n\\end{align}\n\\]\nBefore we get into this, though, let’s back up and consider an intercept-only model of the form\n\\[\n\\begin{align}\n\\text{bill_length_mm}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i & = \\beta_0 ,\n\\end{align}\n\\]\nwhere there is no predictor variable. Here’s how to fit the model with brm().\n\n\nCode\n# Bayes\nfit0.b &lt;- brm(\n  data = chinstrap,\n  bill_length_mm ~ 1\n)\n\n\nLet’s look at the model summary.\n\n\nCode\nsummary(fit0.b)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: bill_length_mm ~ 1 \n   Data: chinstrap (Number of observations: 68) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    48.85      0.40    48.07    49.66 1.00     3120     2204\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     3.38      0.30     2.86     4.02 1.00     3723     2493\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe intercept parameter \\(\\beta_0\\) is a stand-in for \\(\\mu\\). The \\(\\sigma\\) parameter is just \\(\\sigma\\). Here they are in a plot.\n\n\nCode\ndraws &lt;- as_draws_df(fit0.b) \n\ndraws %&gt;% \n  rename(`beta[0]==mu` = b_Intercept) %&gt;% \n  pivot_longer(`beta[0]==mu`:sigma, names_to = \"parameter\") %&gt;% \n  \n  ggplot(aes(x = value)) +\n  stat_halfeye(.width = .95, normalize = \"panels\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(\"parameter space\") +\n  facet_wrap(~ parameter, scales = \"free\", labeller = label_parsed)\n\n\n\n\n\n\n\n\n\nHere are the posterior means for those two parameters.\n\n\nCode\nmu &lt;- mean(draws$b_Intercept)\nsigma &lt;- mean(draws$sigma)\n\nmu; sigma\n\n\n[1] 48.84778\n\n\n[1] 3.382025\n\n\nWe can use dnorm() to compute the shape of \\(\\operatorname{Normal}(48.8, 3.4)\\).\n\n\nCode\ntibble(y = seq(from = 30, to = 70, by = 0.1)) %&gt;% \n  mutate(density = dnorm(x = y, mean = mu, sd = sigma)) %&gt;% \n  \n  ggplot(aes(x = y, y = density)) +\n  geom_line() +\n  xlab(\"bill_length_mm\")\n\n\n\n\n\n\n\n\n\nWe can compare this to the sample distribution of the bill_length_mm data:\n\n\nCode\nchinstrap %&gt;% \n  ggplot(aes(x = bill_length_mm)) +\n  geom_histogram(aes(y = after_stat(density)),\n                 binwidth = 2.5) +\n  geom_line(data = tibble(bill_length_mm = seq(from = 30, to = 70, by = 0.1)),\n            aes(y = dnorm(x = bill_length_mm, mean = mu, sd = sigma)),\n            color = \"red\")\n\n\n\n\n\n\n\n\n\nIt’s not a great fit, but not horrible either.\nNow let’s see what this means for our univariable model fit1.b. First, let’s learn about the posterior_summary() function, which we’ll use to save a few posterior means.\n\n\nCode\nposterior_summary(fit1.b)\n\n\n                   Estimate    Est.Error          Q2.5         Q97.5\nb_Intercept    3.224855e+01 3.5365707440  2.524656e+01  3.922084e+01\nb_body_mass_g  4.445011e-03 0.0009445837  2.573688e-03  6.312568e-03\nsigma          2.931173e+00 0.2585496351  2.493345e+00  3.497498e+00\nIntercept      4.884217e+01 0.3547404207  4.813254e+01  4.953614e+01\nlprior        -4.299932e+00 0.0712959260 -4.460611e+00 -4.183230e+00\nlp__          -1.723192e+02 1.2988160755 -1.755965e+02 -1.708863e+02\n\n\nCode\nb0    &lt;- posterior_summary(fit1.b)[1, 1]\nb1    &lt;- posterior_summary(fit1.b)[2, 1]\nsigma &lt;- posterior_summary(fit1.b)[3, 1]\n\n\nNow we plot.\n\n\nCode\ncrossing(body_mass_g    = seq(from = 2500, to = 5000, length.out = 200),\n         bill_length_mm = seq(from = 35, to = 60, length.out = 200))  %&gt;% \n  mutate(density = dnorm(x = bill_length_mm, \n                         mean = b0 + b1 * body_mass_g,\n                         sd = sigma)) %&gt;% \n  \n  ggplot(aes(x = body_mass_g, y = bill_length_mm)) +\n  geom_raster(aes(fill = density),\n              interpolate = TRUE) +\n  geom_point(data = chinstrap,\n             shape = 21, color = \"white\", fill = \"black\", stroke = 1/4) +\n  scale_fill_viridis_c(option = \"A\", begin = .15, limits = c(0, NA)) +\n  coord_cartesian(xlim = range(chinstrap$body_mass_g),\n                  ylim = range(chinstrap$bill_length_mm))\n\n\n\n\n\n\n\n\n\nOur univariable model fit1.b can be viewed as something like a 3-dimensional Gaussian hill.\n\n\nPrior distributions & Prior predictive distributions.\nLet’s hold off on this for a bit.\n\n\nParameter distributions.\nUp above, we plotted the posterior distributions for our intercept-only fit0.b model. Here they are again.\n\n\nCode\ndraws %&gt;% \n  rename(`beta[0]==mu` = b_Intercept) %&gt;% \n  pivot_longer(`beta[0]==mu`:sigma, names_to = \"parameter\") %&gt;% \n  \n  ggplot(aes(x = value)) +\n  stat_halfeye(.width = .99, normalize = \"panels\",\n               # customize some of the aesthetics\n               fill = \"lightskyblue1\", color = \"royalblue\", \n               point_color = \"darkorchid4\", point_size = 4, shape = 15) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(title = \"fit0.b\",\n       subtitle = \"This time we used 99% intervals, and got silly with the colors.\",\n       x = \"parameter space\") +\n  facet_wrap(~ parameter, scales = \"free\", labeller = label_parsed)\n\n\n\n\n\n\n\n\n\nWe might practice making a similar plot for our univariable model fit1.b.\n\n\nCode\nas_draws_df(fit1.b) %&gt;% \n  rename(`beta[0]` = b_Intercept,\n         `beta[1]` = b_body_mass_g) %&gt;% \n  pivot_longer(cols = c(`beta[0]`, `beta[1]`, sigma), \n               names_to = \"parameter\") %&gt;% \n  \n  ggplot(aes(x = value)) +\n  stat_histinterval(.width = .95, normalize = \"panels\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  labs(title = \"fit1.b\",\n       subtitle = \"Using good old 95% intervals, but switching to histograms\",\n       x = \"parameter space\") +\n  facet_wrap(~ parameter, scales = \"free\", labeller = label_parsed)\n\n\n\n\n\n\n\n\n\nSome authors, like John Kruschke, have a strong preference for plotting their posteriors with histograms, rather than density plots."
  },
  {
    "objectID": "assignments/bayesian_modeling_part2.html#distributions-of-the-model-expectations.",
    "href": "assignments/bayesian_modeling_part2.html#distributions-of-the-model-expectations.",
    "title": "Bayesian Modeling Part 2",
    "section": "Distributions of the model expectations.",
    "text": "Distributions of the model expectations.\nTake another look at the conditional_effects() plot from earlier.\n\n\nCode\nconditional_effects(fit1.b) %&gt;% \n  plot(points = TRUE)\n\n\n\n\n\n\n\n\n\nThe blue line is the posterior mean, for the \\(\\mu_i\\), the model-based mean for bill_length_mm, given the value for the predictor body_mass_g. The semitransparent gray ribbon marks the percentile-based interval for the conditional mean.\nWe can make a similar plot with the fitted() function. First we’ll need a predictor grid, we’ll call nd.\n\n\nCode\nnd &lt;- tibble(body_mass_g = seq(\n  from = min(chinstrap$body_mass_g),\n  to = max(chinstrap$body_mass_g),\n  length.out = 100))\n\nglimpse(nd)\n\n\nRows: 100\nColumns: 1\n$ body_mass_g &lt;dbl&gt; 2700.000, 2721.212, 2742.424, 2763.636, 2784.848, 2806.061…\n\n\nNow pump nd into the fitted() function.\n\n\nCode\nfitted(fit1.b, newdata = nd) %&gt;% \n  # subset the first 6 rows\n  head()\n\n\n     Estimate Est.Error     Q2.5    Q97.5\n[1,] 44.25008 1.0312701 42.19616 46.29570\n[2,] 44.34437 1.0124791 42.32587 46.34688\n[3,] 44.43866 0.9937368 42.45473 46.40796\n[4,] 44.53295 0.9750459 42.58034 46.46165\n[5,] 44.62724 0.9564095 42.71773 46.51539\n[6,] 44.72153 0.9378309 42.85141 46.56892\n\n\nNow plot.\n\n\nCode\nfitted(fit1.b, newdata = nd) %&gt;% \n  data.frame() %&gt;% \n  bind_cols(nd) %&gt;% \n  ggplot(aes(x = body_mass_g)) +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              alpha = 1/3) +\n  geom_line(aes(y = Estimate)) +\n  # add the data\n  geom_point(data = chinstrap,\n             aes(y = bill_length_mm))\n\n\n\n\n\n\n\n\n\nLook what happens if we augment the probs argument in fitted().\n\n\nCode\nfitted(fit1.b, \n       newdata = nd,\n       probs = c(.025, .975, .25, .75)) %&gt;% \n  data.frame() %&gt;% \n  bind_cols(nd) %&gt;% \n  ggplot(aes(x = body_mass_g)) +\n  # 95% range\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              alpha = 1/4) +\n  # 50% range\n  geom_ribbon(aes(ymin = Q25, ymax = Q75),\n              alpha = 1/4) +\n  geom_line(aes(y = Estimate)) +\n  geom_point(data = chinstrap,\n             aes(y = bill_length_mm))\n\n\n\n\n\n\n\n\n\nNow look what happens if we set summary = FALSE.\n\n\nCode\nfitted(fit1.b, \n       newdata = nd,\n       summary = FALSE) %&gt;% \n  str()\n\n\n num [1:4000, 1:100] 46.2 43.6 43.9 44.1 43.5 ...\n\n\nWe get full 4,000 draw posterior distributions for each of the 100 levels of the predictor body_mass_g. Now look at what happens if we wrangle that output a little, and plot with aid from stat_lineribbon() from the ggdist package.\n\n\nCode\nfitted(fit1.b, \n       newdata = nd,\n       summary = F) %&gt;% \n  data.frame() %&gt;% \n  set_names(pull(nd, body_mass_g)) %&gt;% \n  mutate(draw = 1:n()) %&gt;% \n  pivot_longer(-draw) %&gt;% \n  mutate(body_mass_g = as.double(name)) %&gt;%\n  \n  ggplot(aes(x = body_mass_g, y = value)) +\n  stat_lineribbon() +\n  scale_fill_brewer() +\n  coord_cartesian(ylim = range(chinstrap$bill_length_mm)) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\nLook what happens when we request more intervals in the .width argument.\n\n\nCode\nfitted(fit1.b, \n       newdata = nd,\n       summary = F) %&gt;% \n  data.frame() %&gt;% \n  set_names(pull(nd, body_mass_g)) %&gt;% \n  mutate(draw = 1:n()) %&gt;% \n  pivot_longer(-draw) %&gt;% \n  mutate(body_mass_g = as.double(name)) %&gt;%\n  \n  ggplot(aes(x = body_mass_g, y = value)) +\n  # make more ribbons\n  stat_lineribbon(.width = c(.1, .2, .3, .4, .5, .6, .7, .8, .9),\n                  # remove the line\n                  linewidth = 0) +\n  scale_fill_brewer() +\n  coord_cartesian(ylim = range(chinstrap$bill_length_mm)) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\nThe conditional mean, \\(\\mu_i\\), has its own distribution. We can take this visualization approach even further to make a color gradient.\n\n\nCode\nfitted(fit1.b, \n       newdata = nd,\n       summary = F) %&gt;% \n  data.frame() %&gt;% \n  set_names(pull(nd, body_mass_g)) %&gt;% \n  mutate(draw = 1:n()) %&gt;% \n  pivot_longer(-draw) %&gt;% \n  mutate(body_mass_g = as.double(name)) %&gt;%\n  \n  ggplot(aes(x = body_mass_g, y = value, fill = after_stat(.width))) +\n  # make more ribbons\n  stat_lineribbon(.width = ppoints(50)) +\n  scale_fill_distiller(limits = 0:1) +\n  coord_cartesian(ylim = range(chinstrap$bill_length_mm)) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\nFor technical details on this visualization approach, go here: https://mjskay.github.io/ggdist/articles/lineribbon.html#lineribbon-gradients.\nThe ggdist package even has an experimental visualization approach that’s based on density gradients, rather than interval-width gradients. Since this is experimental, I’m not going to go into the details. But if you’re curious and adventurous, you can learn more here: https://mjskay.github.io/ggdist/articles/lineribbon.html#lineribbon-density-gradients.\n\nPosterior-predictive distributions.\nThe last section showed the posterior distributions for the model expectations (i.e., the conditional means). In the context of the Gaussian distribution, that’s \\(\\mu\\), or \\(\\mu_i\\) in the case of the univariable model fit1.b. But the whole Gaussian distribution includes \\(\\mu\\) and \\(\\sigma\\).\nThis is where the predict() function comes in. First, we compare the fitted() output to predict().\n\n\nCode\nfitted(fit1.b, newdata = nd) %&gt;% \n  # subset the first 6 rows\n  head()\n\n\n     Estimate Est.Error     Q2.5    Q97.5\n[1,] 44.25008 1.0312701 42.19616 46.29570\n[2,] 44.34437 1.0124791 42.32587 46.34688\n[3,] 44.43866 0.9937368 42.45473 46.40796\n[4,] 44.53295 0.9750459 42.58034 46.46165\n[5,] 44.62724 0.9564095 42.71773 46.51539\n[6,] 44.72153 0.9378309 42.85141 46.56892\n\n\nCode\npredict(fit1.b, newdata = nd) %&gt;% \n  # subset the first 6 rows\n  head()\n\n\n     Estimate Est.Error     Q2.5    Q97.5\n[1,] 44.30379  3.061648 38.29225 50.24035\n[2,] 44.29916  3.131161 38.10922 50.41003\n[3,] 44.47545  3.090529 38.33836 50.67989\n[4,] 44.58551  3.105407 38.36860 50.68317\n[5,] 44.63171  3.081789 38.48993 50.58438\n[6,] 44.77296  3.118955 38.61518 50.80150\n\n\nThe posterior means (Estimate) are about the same, but the SD’s (Est.Error) are much larger in the predict() output, and the widths of the 95% intervals are too. Let’s make a plot.\n\n\nCode\npredict(fit1.b, newdata = nd) %&gt;% \n  data.frame() %&gt;% \n  bind_cols(nd) %&gt;% \n  ggplot(aes(x = body_mass_g)) +\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              alpha = 1/3) +\n  geom_line(aes(y = Estimate)) +\n  # add the data\n  geom_point(data = chinstrap,\n             aes(y = bill_length_mm)) +\n  coord_cartesian(ylim = range(chinstrap$bill_length_mm))\n\n\n\n\n\n\n\n\n\nThe gray band is the 95% interval for the entire posterior predictive distribution, not just the mean. In a good model, about 95% of the data points should be within those bands.\nDiscuss how the jagged lines have to do with the uncertainty in \\(\\sigma\\).\nIf we wanted to, we could integrate the fitted()-based conditional posterior mean, with the predict()-based posterior-predictive distribution.\n\n\nCode\n# save the fitted() results\nf &lt;- fitted(fit1.b, newdata = nd) %&gt;% \n  data.frame() %&gt;% \n  bind_cols(nd) \n\npredict(fit1.b, newdata = nd) %&gt;% \n  data.frame() %&gt;% \n  bind_cols(nd) %&gt;% \n  \n  ggplot(aes(x = body_mass_g)) +\n  # 95% posterior-predictive range\n  geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5),\n              alpha = 1/4) +\n  # 95% conditional mean range\n  geom_ribbon(data = f,\n              aes(ymin = Q2.5, ymax = Q97.5),\n              alpha = 1/4) +\n  # posterior mean of the conditional mean\n  geom_line(data = f,\n            aes(y = Estimate)) +\n  # original data\n  geom_point(data = chinstrap,\n             aes(y = bill_length_mm)) +\n  coord_cartesian(ylim = range(chinstrap$bill_length_mm))\n\n\n\n\n\n\n\n\n\nIt’s the posterior predictive distribution that we use to predict new data points. For example, here’s what happens if we use predict() without the newdata argument.\n\n\nCode\npredict(fit1.b) %&gt;% \n  head()\n\n\n     Estimate Est.Error     Q2.5    Q97.5\n[1,] 47.80444  2.949190 42.15464 53.57805\n[2,] 49.54131  2.979606 43.86749 55.37979\n[3,] 48.49301  2.983325 42.57801 54.38934\n[4,] 48.00647  2.979760 42.04473 53.84145\n[5,] 48.81826  2.983537 42.94551 54.66447\n[6,] 49.81791  2.941450 44.10726 55.63958\n\n\nWe get posterior predictive summaries for each of the original data points. Here’s what happens if we set summary = FALSE.\n\n\nCode\npredict(fit1.b, summary = FALSE) %&gt;% \n  str()\n\n\n num [1:4000, 1:68] 48.7 44.9 47 47.6 49.3 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : NULL\n  ..$ : NULL\n\n\nThis time, we got 4,000 posterior draws for each. We can reduce that output with the ndraws argument.\n\n\nCode\npredict(fit1.b, summary = FALSE, ndraws = 6) %&gt;% \n  str()\n\n\n num [1:6, 1:68] 49.1 44.4 54.4 43.3 46.2 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : NULL\n  ..$ : NULL\n\n\nNow wrangle and plot.\n\n\nCode\nset.seed(1)\n\npredict(fit1.b, summary = FALSE, ndraws = 6) %&gt;% \n  data.frame() %&gt;% \n  mutate(draw = 1:n()) %&gt;% \n  pivot_longer(-draw) %&gt;% \n  mutate(row = str_remove(name, \"X\") %&gt;% as.double()) %&gt;% \n  left_join(chinstrap %&gt;% \n              mutate(row = 1:n()),\n            by = join_by(row)) %&gt;% \n  \n  ggplot(aes(x = body_mass_g, y = value)) + \n  geom_point() +\n  ylab(\"bill_length_mm\") +\n  facet_wrap(~ draw, labeller = label_both)\n\n\n\n\n\n\n\n\n\nWith predict(), we can use the entire posterior-predictive distribution to simulate new data based on the values of our predictor variable(s). To give you a better sense of what’s happening under the hood, here’s an as_draws_df() based alternative.\n\n\nCode\nset.seed(1)\n\n# walk this code through\nas_draws_df(fit1.b) %&gt;% \n  rename(beta0 = b_Intercept,\n         beta1 = b_body_mass_g) %&gt;% \n  select(.draw, beta0, beta1, sigma) %&gt;% \n  slice_sample(n = 6) %&gt;% \n  expand_grid(chinstrap %&gt;% select(body_mass_g)) %&gt;% \n  mutate(bill_length_mm = rnorm(n = n(),\n                                mean = beta0 + beta1 * body_mass_g,\n                                sd = sigma)) %&gt;% \n  \n  ggplot(aes(x = body_mass_g, y = bill_length_mm)) + \n  geom_point() +\n  facet_wrap(~ .draw, labeller = label_both)\n\n\n\n\n\n\n\n\n\nNow take a look at what happens when we plot the densities of several simulated draws.\n\n\nCode\nset.seed(1)\n\nas_draws_df(fit1.b) %&gt;% \n  rename(beta0 = b_Intercept,\n         beta1 = b_body_mass_g) %&gt;% \n  select(.draw, beta0, beta1, sigma) %&gt;% \n  slice_sample(n = 50) %&gt;%  # increase the number of random draws\n  expand_grid(chinstrap %&gt;% select(body_mass_g)) %&gt;% \n  mutate(bill_length_mm = rnorm(n = n(),\n                                mean = beta0 + beta1 * body_mass_g,\n                                sd = sigma)) %&gt;% \n  \n  ggplot(aes(x = bill_length_mm, group = .draw)) + \n  geom_density(size = 1/4, color = alpha(\"black\", 1/2)) +\n  coord_cartesian(xlim = range(chinstrap$bill_length_mm) + c(-2, 2))\n\n\n\n\n\n\n\n\n\nThe similarities and differences among the individual density lines give you a sense of the (un)certainty of the posterior-predictive distribution.\nThis may be a good time for you to work on Exercise 1 (see end of the document)\n#Part 4: Beginning to look at priors"
  },
  {
    "objectID": "assignments/bayesian_modeling_part2.html#bayes-rule",
    "href": "assignments/bayesian_modeling_part2.html#bayes-rule",
    "title": "Bayesian Modeling Part 2",
    "section": "Bayes’ rule",
    "text": "Bayes’ rule\nBayes’ theorem will allow us to determine the plausibility of various values of our parameter(s) of interest, \\(\\theta\\), given the data \\(d\\), which we can express formally as \\(\\Pr(\\theta \\mid d)\\). Bayes’ rule takes on the form\n\\[\n\\Pr(\\theta \\mid d) = \\frac{\\Pr(d \\mid \\theta) \\Pr(\\theta)}{\\Pr(d)}.\n\\]\nwhere\n\n\\(\\Pr(d \\mid \\theta)\\) is the likelihood,\n\\(\\Pr(\\theta)\\) is the prior,\n\\(\\Pr(d)\\) is the average probability of the data, and\n\\(\\Pr(\\theta \\mid d)\\) is the posterior.\n\nWe can express this in words as\n\\[\n\\text{Posterior} = \\frac{\\text{Probability of the data} \\times \\text{Prior}}{\\text{Average probability of the data}}.\n\\]\nThe denominator \\(\\Pr(d)\\) is a normalizing constant, and dividing by this constant is what converts the posterior \\(\\Pr(\\theta \\mid d)\\) into a probability metric."
  },
  {
    "objectID": "assignments/bayesian_modeling_part2.html#default-priors",
    "href": "assignments/bayesian_modeling_part2.html#default-priors",
    "title": "Bayesian Modeling Part 2",
    "section": "Default priors",
    "text": "Default priors\nTo set your priors with brms, the brm() function has a prior argument. If you don’t explicitly use the prior argument, brm() will use default priors. This is what happened with our fit1.b model from above. We used default priors. If you’d like to see what those priors are, execute fit1.b$prior.\n\n\nCode\n# maybe show str(fit1.b)\nfit1.b$prior\n\n\n                   prior     class        coef group resp dpar nlpar lb ub\n                  (flat)         b                                        \n                  (flat)         b body_mass_g                            \n student_t(3, 49.5, 3.6) Intercept                                        \n    student_t(3, 0, 3.6)     sigma                                    0   \n       source\n      default\n (vectorized)\n      default\n      default\n\n\nThus, a fuller expression of our model is\n\\[\n\\begin{align}\n\\text{bill_length_mm}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i & = \\beta_0 + \\beta_1 \\text{body_mass_g}_i \\\\\n\\beta_0 & \\sim \\operatorname{Student-t}(3, 49.5, 3.6) \\\\\n\\beta_1 & \\sim \\operatorname{Uniform}(-\\infty, \\infty) \\\\\n\\sigma & \\sim \\operatorname{Student-t}^+(3, 0, 3.6).\n\\end{align}\n\\]\nIf we had wanted to see the brm() defaults before fitting the model, we could have used the get_prior() function.\n\n\nCode\nget_prior(\n  data = chinstrap,\n  bill_length_mm ~ 1 + body_mass_g\n)\n\n\n                   prior     class        coef group resp dpar nlpar lb ub\n                  (flat)         b                                        \n                  (flat)         b body_mass_g                            \n student_t(3, 49.5, 3.6) Intercept                                        \n    student_t(3, 0, 3.6)     sigma                                    0   \n       source\n      default\n (vectorized)\n      default\n      default\n\n\nIf you recall, the normal distribution is a member of the Student-t family, where the \\(\\nu\\) (aka degrees of freedom or normality parameter) is set to \\(\\infty\\). To give you a sense, here are the densities of three members of the Student-t family, with varying \\(\\nu\\) values.\n\n\nCode\ncrossing(theta = seq(from = -4.5, to = 4.5, length.out = 200),\n         nu = c(3, 10, Inf)) %&gt;% \n  mutate(density = dt(x = theta, df = nu)) %&gt;% \n  \n  ggplot(aes(x = theta, y = density, color = factor(nu))) +\n  geom_line(linewidth = 1) +\n  scale_color_viridis_d(expression(nu), option = \"A\", end = .7) +\n  labs(title = \"3 members of the Student-t family\",\n       x = expression(theta)) +\n  coord_cartesian(xlim = c(-4, 4))\n\n\n\n\n\n\n\n\n\nThus, Student-t distributions have thicker tails when they have smaller \\(\\nu\\) parameters. In the case where \\(\\nu = 3\\), the tails are pretty thick, which means they are more tolerant of more extreme values. And thus priors with small-\\(\\nu\\) parameters will be weaker (i.e., more permissive) than their Gaussian counterparts.\nWe can visualize functions from ggdist to visualize the default brm() priors. We’ll start with the student_t(3, 49.5, 3.6) \\(\\beta_0\\) prior, and also take the opportunity to compare that with a slightly stronger normal(49.5, 3.6) alternative.\n\n\nCode\nc(prior(student_t(3, 49.5, 3.6)),\n  prior(normal(49.5, 3.6))) %&gt;% \n  parse_dist() %&gt;% \n  \n  ggplot(aes(xdist = .dist_obj, y = prior)) + \n  stat_halfeye() +\n  labs(x = expression(italic(p)(beta[0])),\n       y = NULL) +\n  coord_cartesian(xlim = c(25, 75))\n\n\n\n\n\n\n\n\n\nSee how that \\(n = 3\\) parameter in the default prior let do much thicker tails than it’s Gaussian counterpart. We can make the same kind of plot for our default \\(\\sigma\\) prior and its half-Gaussian counterpart.\n\n\nCode\nc(prior(student_t(3, 0, 3.6), lb = 0),  # note our use of the lb = 0 argument\n  prior(normal(0, 3.6), lb = 0)) %&gt;% \n  parse_dist() %&gt;% \n  \n  ggplot(aes(xdist = .dist_obj, y = prior)) + \n  stat_halfeye(point_interval = mean_qi, .width = c(.90, .99)) +\n  labs(x = expression(italic(p)(sigma)),\n       y = NULL) +\n  coord_cartesian(xlim = c(0, 30))\n\n\n\n\n\n\n\n\n\nHere’s how we could have explicitly set our priors by hand.\n\n\nCode\nfit2.b &lt;- brm(\n  data = chinstrap,\n  bill_length_mm ~ 1 + body_mass_g,\n  prior = prior(student_t(3, 49.5, 3.6), class = Intercept) +\n    prior(student_t(3, 0, 3.6), class = sigma, lb = 0)\n)\n\n\nCompare the results.\n\n\nCode\nsummary(fit1.b)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: bill_length_mm ~ 1 + body_mass_g \n   Data: chinstrap (Number of observations: 68) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      32.25      3.54    25.25    39.22 1.00     5177     2895\nbody_mass_g     0.00      0.00     0.00     0.01 1.00     5396     3070\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     2.93      0.26     2.49     3.50 1.00     1986     1833\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCode\nsummary(fit2.b)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: bill_length_mm ~ 1 + body_mass_g \n   Data: chinstrap (Number of observations: 68) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      32.18      3.58    25.13    39.24 1.00     4678     3182\nbody_mass_g     0.00      0.00     0.00     0.01 1.00     4724     3103\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     2.93      0.27     2.46     3.49 1.00     2005     1960\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1)."
  },
  {
    "objectID": "assignments/bayesian_modeling_part2.html#question-2-are-the-priors-the-same-what-do-you-think-is-going-on",
    "href": "assignments/bayesian_modeling_part2.html#question-2-are-the-priors-the-same-what-do-you-think-is-going-on",
    "title": "Bayesian Modeling Part 2",
    "section": "QUESTION 2 Are the priors the same? What do you think is going on?",
    "text": "QUESTION 2 Are the priors the same? What do you think is going on?\n\nAnswer:\nLooking at the summaries of fit1.b and fit2.b, the priors are essentially the same. This is because in fit1.b, we used the default priors built into brms, while in fit2.b, we explicitly specified the same priors that brms uses by default for the intercept and sigma parameters. However, we didn’t specify a prior for the slope (b_body_mass_g) in fit2.b, so it’s still using the default flat prior for that parameter.\nIf you want to learn more about the default prior settings for brms, read through the set_prior section of the brms reference manual (https://CRAN.R-project.org/package=brms/brms.pdf)."
  },
  {
    "objectID": "assignments/bayesian_modeling_part2.html#references",
    "href": "assignments/bayesian_modeling_part2.html#references",
    "title": "Bayesian Modeling Part 2",
    "section": "References",
    "text": "References\nKruschke, J. K. (2015). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press. https://sites.google.com/site/doingbayesiandataanalysis/"
  },
  {
    "objectID": "assignments/bayesian_modeling_part2.html#session-information",
    "href": "assignments/bayesian_modeling_part2.html#session-information",
    "title": "Bayesian Modeling Part 2",
    "section": "Session information",
    "text": "Session information"
  },
  {
    "objectID": "assignments/bayes_priors_predictive_checks.html",
    "href": "assignments/bayes_priors_predictive_checks.html",
    "title": "bayes_priors_predictive_checks.qmd",
    "section": "",
    "text": "During the first Bayes Lab you considered exploratory data analysis, compared default brms with lm(), and extracted posteriors after fitting models. You summarized posterior distributions and also generated a distribution of predictions using these posterior draws.\n\nDuring the second Bayes lab, you looked at the different types of distributions that are relevant for Bayesian analysis, including priors.\nDuring today’s lab, you will go into prior predictive checks and some HMC diagnostics. While we look at the simple linear modeling case, this workflow is relevant for all Bayesian models."
  },
  {
    "objectID": "assignments/bayes_priors_predictive_checks.html#setup-packages-and-data",
    "href": "assignments/bayes_priors_predictive_checks.html#setup-packages-and-data",
    "title": "bayes_priors_predictive_checks.qmd",
    "section": "Setup: Packages and data",
    "text": "Setup: Packages and data\nLoad the primary packages.\n\n\nCode\nlibrary(tidyverse)\nlibrary(brms)\nlibrary(tidybayes)\n# library(truncnorm)  # if needed\n\n\nThis time we’ll be taking data from the moderndive package. We want the evals data set.\n\n\nCode\ndata(evals, package = \"moderndive\")\n\n\nThe evals data were originally in the paper by Hamermesh and Parker (2005; https://doi.org/10.1016/j.econedurev.2004.07.013). You can learn more about the data like this:\n\n\nCode\n?moderndive::evals\n\n\nYou can learn even more information about the data from https://www.openintro.org/data/index.php?data=evals.\nAnyway, we need to subset the data.\n\n\nCode\nevals94 &lt;- evals %&gt;% \n  group_by(prof_ID) %&gt;% \n  slice(1) %&gt;% \n  ungroup()\n\nglimpse(evals94)\n\n\nRows: 94\nColumns: 14\n$ ID           &lt;int&gt; 1, 5, 8, 10, 18, 24, 31, 36, 43, 50, 60, 63, 68, 75, 79, …\n$ prof_ID      &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17…\n$ score        &lt;dbl&gt; 4.7, 4.6, 4.1, 4.5, 4.8, 4.4, 4.4, 3.4, 4.8, 4.0, 3.6, 4.…\n$ age          &lt;int&gt; 36, 59, 51, 40, 31, 62, 33, 51, 33, 47, 35, 37, 42, 49, 3…\n$ bty_avg      &lt;dbl&gt; 5.000, 3.000, 3.333, 3.167, 7.333, 5.500, 4.167, 4.000, 4…\n$ gender       &lt;fct&gt; female, male, male, female, female, male, female, female,…\n$ ethnicity    &lt;fct&gt; minority, not minority, not minority, not minority, not m…\n$ language     &lt;fct&gt; english, english, english, english, english, english, eng…\n$ rank         &lt;fct&gt; tenure track, tenured, tenured, tenured, tenure track, te…\n$ pic_outfit   &lt;fct&gt; not formal, not formal, not formal, not formal, not forma…\n$ pic_color    &lt;fct&gt; color, color, color, color, color, color, color, color, c…\n$ cls_did_eval &lt;int&gt; 24, 17, 55, 40, 42, 182, 33, 25, 48, 16, 18, 30, 28, 30, …\n$ cls_students &lt;int&gt; 43, 20, 55, 46, 48, 282, 41, 41, 60, 19, 25, 34, 40, 36, …\n$ cls_level    &lt;fct&gt; upper, upper, upper, upper, upper, upper, upper, upper, u…"
  },
  {
    "objectID": "assignments/bayes_priors_predictive_checks.html#intercept-only-model",
    "href": "assignments/bayes_priors_predictive_checks.html#intercept-only-model",
    "title": "bayes_priors_predictive_checks.qmd",
    "section": "Intercept-only model",
    "text": "Intercept-only model\nLet’s start by fitting an intercept-only model\n\\[\n\\begin{align}\n\\text{bty_avg}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i & = \\beta_0 \\\\\n\\beta_0 & \\sim \\text{???} \\\\\n\\sigma & \\sim \\text{???},\n\\end{align}\n\\]\nwhere \\(\\beta_0\\) is the same as the unconditional population mean, and the population standard deviation is \\(\\sigma\\). Our next task will be choosing our priors.\n\nQuestion 1: Why have we left some of the specification above unfilled / with questions marks at this point?\nWe’ve left the priors for \\(\\beta_0\\) and \\(\\sigma\\) unspecified because we are in the process of exploring and evaluating different possible priors. This allows us to visually inspect and assess their plausibility and fit using prior predictive checks before committing to them.\n\n\nVisualize possible prior distributions.\nIn this exercise, we’ll choose the priors together. Let’s start with prior on \\(\\beta_0\\). Below are a few candidate distributions visualized with ggdist and friends.\n\n\nCode\nc(\n  prior(normal(5.5, 1)),\n  prior(normal(8, 2)),\n  prior(normal(5.5, 2))\n) %&gt;% \n  parse_dist() %&gt;% \n\n  ggplot(aes(xdist = .dist_obj, y = prior)) + \n  stat_halfeye(point_interval = mean_qi, .width = c(.5, .95)) +\n  geom_vline(xintercept = c(1, 10), color = \"red\") +\n  labs(subtitle = \"The red lines mark the lower and upper boundaries.\",\n       x = expression(italic(p)(beta[0])),\n      y = NULL)\n\n\n\n\n\n\n\n\n\nThe red lines in the figures (shown at x=1 and x=10) represent the lower and upper boundaries for the beauty ratings scale used in the study. With the simple intercept model, setting a prior on the intercept parameter is the same as setting a prior on the expected mean in observation space.\nNow let’s visualize a few potential priors for \\(\\sigma\\).\n\n\nCode\nc(\n  prior(exponential(1)), \n  prior(normal(0, 1), lb = 0), \n  prior(normal(2, 0.3), lb = 0)\n) %&gt;% \n  parse_dist() %&gt;% \n  \n  ggplot(aes(xdist = .dist_obj, y = prior)) + \n  stat_halfeye(point_interval = mean_qi, .width = c(.5, .95)) +\n  xlab(expression(italic(p)(sigma))) +\n  ylab(NULL)\n\n\n\n\n\n\n\n\n\n\nQuestion 2: Given that \\(\\sigma\\) refers to the standard deviation, are these three priors theoretically possible? If yes, give an example of a theoretically impossible prior for \\(\\sigma\\).\nYes, all three priors above are theoretically possible because they only assign positive values, which is required for standard deviation parameters. For example, a prior that assigns negative values to \\(\\sigma\\) would be theoretically impossible, as standard deviations cannot be negative like this one prior(normal(0, 1)).\n\n\n\nPrior-predictive checks (by hand).\nNote: It’s possible we’ll need the truncnorm::rtruncnorm() function in this section. Once we have candidate priors for both \\(\\beta_0\\) and \\(\\sigma\\), we can simulate values from those priors and plot the implied distributions.\n\n\nCode\n# how many distributions do you want?\nn &lt;- 50\n\n# do you want to make the simulation reproducible?\n# set.seed(1)\n\n# simulate values from the priors\ntibble(iter = 1:n,\n       # choose the hyperparameter values with the class\n       beta0 = rnorm(n = n, mean = 5.5, sd = 1),\n       sigma = rexp(n = n, rate = 1 / 1)) %&gt;% \n  expand_grid(bty_avg = seq(from = -2, to = 13, by = 0.025)) %&gt;% \n  mutate(density = dnorm(x = bty_avg, mean = beta0, sd = sigma)) %&gt;% \n  \n  # plot!\n  ggplot(aes(x = bty_avg, y = density, group = iter)) +\n  geom_line(linewidth = 1/3, alpha = 1/2) +\n  geom_vline(xintercept = c(1, 10), color = \"red\") +\n  coord_cartesian(xlim = c(-1, 12),\n                  ylim = c(0, 3)) +\n  labs(subtitle = expression(\"Prior predictive distributions based on \"*italic(p)(beta[0])~and~italic(p)(sigma)))\n\n\n\n\n\n\n\n\n\nThe simulated values constitute predictions that are made using our prior beliefs (a prior is set for beta0 and another for sigma) When you check if these predictions (prior predictive) make sense or not, it is called the prior predictive check. The point of the prior predictive check is to iterate on specifying the priors until the prior predictive is sensible/satisfactory.\n(Again, the red boundaries denote that the only possible bty_avg values are between 1 and 10.)\n\nQuestion 3: Can Explain what the section of the previous command, before ggplot is doing?\n[[Answer: The previous command is simulating values from the prior distributions of \\(\\beta_0\\) and \\(\\sigma\\). It generates 50 samples of \\(\\beta_0\\) from a normal distribution with a mean of 5.5 and a standard deviation of 1, and 50 samples of \\(\\sigma\\) from an exponential distribution with a rate of 1. The expand_grid() function creates a grid of values for bty_avg ranging from -2 to 13, and the mutate() function calculates the density of the normal distribution for each combination of bty_avg, beta0, and sigma. Finally, the resulting data frame is used to create a plot with ggplot(), showing the prior predictive distributions based on the specified priors. ]]\n\n\nQuestion 4: The prior predictive above is for one combination of our candidate priors. Why don’t you also try the \\(\\beta_0\\) prior centered at 8, along with the \\(\\sigma\\) prior centered at 2? What do you observe? Among these two , which would you pick? And why? (Optional: try others too if you’d like)\nI’d observe that the prior predictive distribution is slightly more spread-out (i.e., flatter) curves due to the larger variance in the intercept and sigma. Compared to beta0 ~ Normal(5.5, 1), this prior is less conservative and assumes more prior belief that beauty ratings are higher and more variable.\nI’d pick beta0 ~ Normal(5.5, 1) and sigma ~ Exponential(1) because it centers the prior mean within the plausible range of the rating scale, and produces predictions mostly within the 1–10 range, which reflects more uncertainty but avoids implausibly extreme values.\n\n\nCode\n# how many distributions do you want?\nn &lt;- 50\n\n# do you want to make the simulation reproducible?\n# set.seed(1)\n\n# simulate values from the priors\ntibble(iter = 1:n,\n       # choose the hyperparameter values with the class\n       beta0 = rnorm(n = n, mean = 8, sd = 2),\n       sigma = rnorm(n = n, mean = 2, sd = 0.3)) %&gt;% \n  expand_grid(bty_avg = seq(from = -2, to = 13, by = 0.025)) %&gt;% \n  mutate(density = dnorm(x = bty_avg, mean = beta0, sd = sigma)) %&gt;% \n  \n  # plot!\n  ggplot(aes(x = bty_avg, y = density, group = iter)) +\n  geom_line(linewidth = 1/3, alpha = 1/2) +\n  geom_vline(xintercept = c(1, 10), color = \"red\") +\n  coord_cartesian(xlim = c(-1, 12),\n                  ylim = c(0, 3)) +\n  labs(subtitle = expression(\"Prior predictive distributions based on \"*italic(p)(beta[0])~and~italic(p)(sigma)))\n\n\n\n\n\n\n\n\n\n\n\n\nFit the model that you prefer\nWe should practice writing out our model equation with our priors of choice:\n\\[\n\\begin{align}\n\\text{bty_avg}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i & = \\beta_0 \\\\\n\\beta_0 & \\sim \\text{&lt;put distribution here&gt;} \\\\\n\\sigma & \\sim \\text{&lt;put distribution here&gt;}.\n\\end{align}\n\\]\nLet’s fit a model with our priors of choice.\n\n\nCode\nfit9.b = brm(\n  data = evals94,\n  family = gaussian,\n  bty_avg ~ 1,\n  # make sure we're settled on our priors \n  # we don't need to use these; they're placeholders\n  prior = prior(normal(5.5, 1), class = Intercept) +\n    prior(exponential(1), class = sigma)\n)\n\n\nCheck the model summary.\n\n\nCode\nsummary(fit9.b)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: bty_avg ~ 1 \n   Data: evals94 (Number of observations: 94) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     4.62      0.16     4.29     4.95 1.00     3369     2502\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.60      0.12     1.39     1.86 1.00     3160     2415\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nNow we might do a posterior predictive check to see how well our model describes the data.\n\n\nCode\nset.seed(1)\npp_check(fit9.b, ndraws = 100) +\n  ggtitle(\"posterior predictive check\")\n\n\n\n\n\n\n\n\n\nCode\nset.seed(2)\npp_check(fit9.b, ndraws = 8,\n         type = \"hist\", binwidth = 0.5) +\n  # yes, we can add our red lines to our pp-check\n  geom_vline(xintercept = c(1, 10), color = \"red\")  +\n  ggtitle(\"posterior predictive check\")\n\n\n\n\n\n\n\n\n\nOur simple Gaussian model doesn’t do a great job respecting the lower and upper boundaries, but this is about as good as it gets when you’re in Gaussian land. On the whole, the model did a pretty okay reproducing the gross features of the distribution of the sample data.\n\nQuestion 5: To ensure you’ve understood things well, can you write below about the difference between the prior predictive check and the posterior predictive check? How do they differ in their objectives?\nPrior predictive check simulates data from the model using only the priors, before seeing the data. Posterior predictive check simulates data from the model after conditioning on the observed data (i.e., using the posterior)."
  },
  {
    "objectID": "assignments/bayes_priors_predictive_checks.html#prior-predictive-checks-by-sample_prior-only",
    "href": "assignments/bayes_priors_predictive_checks.html#prior-predictive-checks-by-sample_prior-only",
    "title": "bayes_priors_predictive_checks.qmd",
    "section": "Prior-predictive checks (by sample_prior = \"only\")",
    "text": "Prior-predictive checks (by sample_prior = \"only\")\nWe can also sample from the prior predictive distribution from brm() itself. To do so, we use the sample_prior argument, which has the following options:\n\n\"no\", which is the default, and does not sample from the prior;\n\"yes\",, which will sample from both the prior and the posterior; and\n\"only\", which will only sample from the prior.\n\nLet’s set sample_prior = \"only\".\n\n\nCode\n# check to see if we want to use other priors\n\nfit10.b = brm(\n  data = evals94,\n  family = gaussian,\n  bty_avg ~ 1,\n  prior = prior(normal(5.5, 1), class = Intercept) +\n    prior(exponential(1), class = sigma),\n  # here's the magic\n  sample_prior = \"only\",\n  # we can set our seed, too!\n  seed = 1\n)\n\n\nDid you notice how we used the seed argument? This makes the results reproducible.\nNow the summary() function only returns summaries for the priors, NOT the posterior.\n\n\nCode\nsummary(fit10.b)  # this summarizes the prior\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: bty_avg ~ 1 \n   Data: evals94 (Number of observations: 94) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     5.50      1.00     3.61     7.49 1.00     1876     1938\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.00      1.01     0.03     3.63 1.00     1957     1424\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe as_draws_df() function also returns draws from the prior.\n\n\nCode\nas_draws_df(fit10.b) %&gt;% \n  head()\n\n\n# A draws_df: 6 iterations, 1 chains, and 5 variables\n  b_Intercept sigma Intercept lprior lp__\n1         5.5  0.80       5.5   -1.7 -1.9\n2         4.8  1.36       4.8   -2.5 -2.2\n3         4.4  0.46       4.4   -2.0 -2.8\n4         6.1  0.72       6.1   -1.8 -2.1\n5         5.5  1.13       5.5   -2.0 -1.9\n6         5.7  1.51       5.7   -2.4 -2.0\n# ... hidden reserved variables {'.chain', '.iteration', '.draw'}\n\n\nHere’s how we might use that as_draws_df() output to make a similar plot to the one we made before.\n\n\nCode\n# how many distributions do you want?\nn &lt;- 50\n\n# do you want to make the results reproducible?\n# set.seed(1)\n\nas_draws_df(fit10.b) %&gt;% \n  \n  # subset\n  slice_sample(n = n) %&gt;% \n  expand_grid(bty_avg = seq(from = -2, to = 13, by = 0.025)) %&gt;% \n  # notice we're defining the mean by b_Intercept\n  mutate(density = dnorm(x = bty_avg, mean = b_Intercept, sd = sigma)) %&gt;% \n  \n  ggplot(aes(x = bty_avg, y = density, \n             # notice we're grouping by .draw\n             group = .draw)) +\n  geom_line(linewidth = 1/3, alpha = 1/2) +\n  geom_vline(xintercept = c(1, 10), color = \"red\") +\n  coord_cartesian(xlim = c(-1, 12),\n                  ylim = c(0, 3)) +\n  labs(subtitle = expression(\"Prior predictive distributions based on \"*italic(p)(beta[0])~and~italic(p)(sigma)))\n\n\n\n\n\n\n\n\n\nWe can also use functions like pp_check() to compare the prior to the sample data.\n\n\nCode\nset.seed(1)\npp_check(fit10.b, ndraws = 100) +\n  coord_cartesian(xlim = c(-1, 12),\n                  ylim = c(0, 3)) +\n  ggtitle(\"prior predictive check\")\n\n\n\n\n\n\n\n\n\nCode\nset.seed(2)\npp_check(fit10.b, ndraws = 8,\n         type = \"hist\", binwidth = 0.5) +\n  # yes, we can add our red lines to our pp-check\n  geom_vline(xintercept = c(1, 10), color = \"red\") +\n  ggtitle(\"prior predictive check\")"
  },
  {
    "objectID": "assignments/bayes_priors_predictive_checks.html#univariable-predictor-model",
    "href": "assignments/bayes_priors_predictive_checks.html#univariable-predictor-model",
    "title": "bayes_priors_predictive_checks.qmd",
    "section": "Univariable predictor model",
    "text": "Univariable predictor model\nNow we’ll add gender as the sole predictor in the model,\n\\[\n\\begin{align}\n\\text{bty_avg}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i & = \\beta_0 + \\beta_1 \\text{gender}_i \\\\\n\\beta_0 & \\sim \\text{???} \\\\\n\\beta_1 & \\sim \\text{???} \\\\\n\\sigma & \\sim \\text{???}.\n\\end{align}\n\\]\nLet’s try these same set of \\(\\beta_0\\) priors\n\n\nCode\n# change as needed\n\nc(\n  prior(normal(5.5, 1)),\n  prior(normal(7, 0.5)),\n  prior(normal(5.5, 2))\n) %&gt;% \n  parse_dist() %&gt;% \n  \n  ggplot(aes(xdist = .dist_obj, y = prior)) + \n  stat_halfeye(point_interval = mean_qi, .width = c(.5, .95)) +\n  geom_vline(xintercept = c(1, 10), color = \"red\") +\n  labs(subtitle = \"The red lines mark the lower and upper bondaries.\",\n       x = expression(italic(p)(beta[0])),\n      y = NULL)\n\n\n\n\n\n\n\n\n\nNow we update our by-hand prior predictive simulation to accomodate \\(\\beta_0\\) and \\(\\beta_1\\).\n\n\nCode\nn &lt;- 50\n\nset.seed(1)\n\ntibble(iter = 1:n,\n       beta0 = rnorm(n = n, mean = 5.5, sd = 1),\n       # notice our new line\n       beta1 = rnorm(n = n, mean = 0, sd = 1),\n       sigma = rexp(n = n, rate = 1 / 1)) %&gt;% \n  # we have a new expand_grid() line\n  # make sure everyone understands this coding scheme\n  expand_grid(gendermale = 0:1) %&gt;% \n  expand_grid(bty_avg = seq(from = -2, to = 13, by = 0.025)) %&gt;% \n  # notice the updated mean formula\n  mutate(density = dnorm(x = bty_avg, \n                         mean = beta0 + beta1 * gendermale, \n                         sd = sigma)) %&gt;% \n  \n  # plot!\n  ggplot(aes(x = bty_avg, y = density, group = iter)) +\n  geom_line(linewidth = 1/3, alpha = 1/2) +\n  geom_vline(xintercept = c(1, 10), color = \"red\") +\n  coord_cartesian(xlim = c(-1, 12),\n                  ylim = c(0, 3)) +\n  labs(subtitle = expression(\"Prior predictive distributions based on \"*italic(p)(beta[0])~ and~italic(p)(beta[1])~and~italic(p)(sigma))) +\n  facet_wrap(~ gendermale, labeller = label_both)\n\n\n\n\n\n\n\n\n\nBefore we fit the model, let’s practice the sample_prior = \"only\" approach.\n\n\nCode\n# check to see if we want to use other priors\n\nfit11.b = brm(\n  data = evals94,\n  family = gaussian,\n  # notice the 0 + Intercept syntax\n  bty_avg ~ 0 + Intercept + gender,\n  prior = prior(normal(5.5, 1), class = b, coef = Intercept) +\n    prior(normal(0, 1), class = b, coef = gendermale) +\n    prior(exponential(1), class = sigma),\n  # here's the magic\n  sample_prior = \"only\",\n  seed = 2\n)\n\n\nCheck the prior summary.\n\n\nCode\nsummary(fit11.b)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: bty_avg ~ 0 + Intercept + gender \n   Data: evals94 (Number of observations: 94) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      5.46      1.00     3.53     7.43 1.00     3147     2820\ngendermale     0.02      0.99    -1.91     1.99 1.00     4109     2946\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.02      0.98     0.03     3.66 1.00     3149     1903\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCompare the prior with the data with pp_check().\n\n\nCode\nset.seed(1)\npp_check(fit11.b, \n         type = \"dens_overlay_grouped\",\n         group = \"gender\",\n         ndraws = 100) +\n  coord_cartesian(xlim = c(-1, 12),\n                  ylim = c(0, 3)) +\n  ggtitle(\"prior predictive check\")\n\n\n\n\n\n\n\n\n\nCode\nset.seed(2)\npp_check(fit11.b, ndraws = 5,\n         type = \"freqpoly_grouped\", group = \"gender\") +\n  # yes, we can add our red lines to our pp-check\n  geom_vline(xintercept = c(1, 10), color = \"red\") +\n  ggtitle(\"prior predictive check\")\n\n\n\n\n\n\n\n\n\nThere isn’t a great grouped histogram option for pp_check(), so we experimented with type = \"freqpoly_grouped\" instead.\nIf we wanted, we could also use the predict() function to simulate bty_avg values from the priors.\n\n\nCode\n# walk through this slowly\n\nset.seed(1)\n\npredict(fit11.b,\n        summary = FALSE,\n        ndraws = 5) %&gt;% \n  str()\n\n\n num [1:5, 1:94] 5.37 7.82 3.87 4.89 6.06 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : NULL\n  ..$ : NULL\n\n\n\n\nCode\n# customize the predictor grid, as desired\nnd &lt;- tibble(gender = rep(c(\"female\", \"male\"), each = 50)) %&gt;% \n  # this will make it easier to connect the nd data to the predict() output\n  mutate(row = 1:n())\n\nset.seed(1)\n\npredict(fit11.b,\n        newdata = nd,\n        summary = FALSE,\n        ndraws = 5) %&gt;% \n  data.frame() %&gt;% \n  mutate(draw = 1:n()) %&gt;% \n  pivot_longer(-draw) %&gt;% \n  mutate(row = str_remove(name, \"X\") %&gt;% as.double()) %&gt;% \n  left_join(nd, by = \"row\") %&gt;% \n  \n  ggplot(aes(x = value)) +\n  geom_histogram(binwidth = 0.5, boundary = 1) +\n  geom_vline(xintercept = c(1, 10), color = \"red\") +\n  facet_grid(draw ~ gender, labeller = label_both)\n\n\n\n\n\n\n\n\n\nOnce we’ve settled on our priors, we should once again practice writing out the full model equation:\n\\[\n\\begin{align}\n\\text{bty_avg}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i & = \\beta_0 + \\beta_1 \\text{gender}_i \\\\\n\\beta_0 & \\sim \\text{&lt;put distribution here&gt;} \\\\\n\\beta_1 & \\sim \\text{&lt;put distribution here&gt;} \\\\\n\\sigma & \\sim \\text{&lt;put distribution here&gt;}.\n\\end{align}\n\\]\nOkay, let’s fit the real model.\n\n\nCode\n# check to see if we want to use other priors\n\nfit12.b = brm(\n  data = evals94,\n  family = gaussian,\n  bty_avg ~ 0 + Intercept + gender,\n  prior = prior(normal(5.5, 1), class = b, coef = Intercept) +\n    prior(normal(0, 1), class = b, coef = gendermale) +\n    prior(exponential(1), class = sigma),\n  \n  # yes, you can set your seed for your posteriors, too\n  # this makes the results reproducible\n  seed = 3\n)\n\n\nCheck the model summary.\n\n\nCode\nsummary(fit12.b)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: bty_avg ~ 0 + Intercept + gender \n   Data: evals94 (Number of observations: 94) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      4.93      0.24     4.48     5.40 1.00     2134     2345\ngendermale    -0.54      0.31    -1.14     0.06 1.00     2050     2218\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.59      0.12     1.38     1.83 1.00     2431     2371\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nHow does the posterior-predictive check look?\n\n\nCode\nset.seed(1)\npp_check(fit12.b, \n         type = \"dens_overlay_grouped\",\n         group = \"gender\",\n         ndraws = 100) +\n  coord_cartesian(xlim = c(-1, 12)) +\n  ggtitle(\"posterior predictive check\")\n\n\n\n\n\n\n\n\n\nCode\nset.seed(2)\npp_check(fit12.b, ndraws = 5,\n         type = \"freqpoly_grouped\", group = \"gender\") +\n  # yes, we can add our red lines to our pp-check\n  geom_vline(xintercept = c(1, 10), color = \"red\") +\n  ggtitle(\"prior predictive check\")\n\n\n\n\n\n\n\n\n\n\nQuestion 6: Does the posterior predictive check look satsifactory to you?\nYes, overall it looks good. The posterior predictive distribution aligns fairly well with the actual data, capturing the overall shape and spread of the bty_avg scores within each gender group. The densities stay mostly within the valid bounds (1–10), and the model seems to reproduce the main characteristics of the data.\n\n\n\n\n\n\nNote\n\n\n\nFor more on prior predictive checks, see McElreath (from Chapter 4), and Solomon Kurz’s brms/tidverse implementations as well.\nFor a comprehensive guide to set priors for a given situation, look at reccomendations made by the Stan team https://github.com/stan-dev/stan/wiki/prior-choice-recommendations\nThey generally recommend against uniform priors on \\(\\beta\\) and \\(\\sigma\\) parameters. This is based on a general principle that you should not use a prior that places an artificial boundary on a parameter.\nE.g. \\(\\sigma\\) parameters have natural lower boundaries at zero, but they don’t have upper boundaries. Thus, a uniform prior adds an unnatural upper boundary. A better prior would be something that is weakly informative"
  },
  {
    "objectID": "assignments/bayes_priors_predictive_checks.html#references",
    "href": "assignments/bayes_priors_predictive_checks.html#references",
    "title": "bayes_priors_predictive_checks.qmd",
    "section": "References",
    "text": "References\nHamermesh, D. S., & Parker, A. (2005). Beauty in the classroom: Instructors’ pulchritude and putative pedagogical productivity. Economics of Education Review, 24(4), 369-376. https://doi.org/10.1016/j.econedurev.2004.07.013\nKurz, A. S. (2023). Statistical Rethinking with brms, ggplot2, and the tidyverse: Second Edition (version 0.4.0). https://bookdown.org/content/4857/\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (Second Edition). CRC Press. https://xcelab.net/rm/statistical-rethinking/"
  },
  {
    "objectID": "assignments/bayes_priors_predictive_checks.html#session-information",
    "href": "assignments/bayes_priors_predictive_checks.html#session-information",
    "title": "bayes_priors_predictive_checks.qmd",
    "section": "Session information",
    "text": "Session information\n\n\nCode\nsessionInfo()\n\n\nR version 4.4.3 (2025-02-28)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sequoia 15.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] tidybayes_3.0.7 brms_2.22.0     Rcpp_1.0.13-1   lubridate_1.9.3\n [5] forcats_1.0.0   stringr_1.5.1   dplyr_1.1.4     purrr_1.0.2    \n [9] readr_2.1.5     tidyr_1.3.1     tibble_3.2.1    ggplot2_3.5.2  \n[13] tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n [1] svUnit_1.0.6         tidyselect_1.2.1     farver_2.1.2        \n [4] loo_2.8.0            fastmap_1.2.0        tensorA_0.36.2.1    \n [7] digest_0.6.37        estimability_1.5.1   timechange_0.3.0    \n[10] lifecycle_1.0.4      StanHeaders_2.32.10  processx_3.8.4      \n[13] magrittr_2.0.3       posterior_1.6.0      compiler_4.4.3      \n[16] rlang_1.1.4          tools_4.4.3          utf8_1.2.4          \n[19] yaml_2.3.10          knitr_1.49           labeling_0.4.3      \n[22] bridgesampling_1.1-2 htmlwidgets_1.6.4    pkgbuild_1.4.5      \n[25] curl_6.0.0           plyr_1.8.9           abind_1.4-8         \n[28] withr_3.0.2          numDeriv_2016.8-1.1  grid_4.4.3          \n[31] stats4_4.4.3         fansi_1.0.6          xtable_1.8-4        \n[34] colorspace_2.1-1     inline_0.3.21        emmeans_1.10.5      \n[37] scales_1.3.0         cli_3.6.3            mvtnorm_1.3-2       \n[40] rmarkdown_2.29       generics_0.1.3       RcppParallel_5.1.10 \n[43] rstudioapi_0.17.1    reshape2_1.4.4       tzdb_0.4.0          \n[46] rstan_2.32.6         bayesplot_1.11.1     parallel_4.4.3      \n[49] matrixStats_1.5.0    vctrs_0.6.5          V8_6.0.1            \n[52] Matrix_1.7-2         jsonlite_1.8.9       callr_3.7.6         \n[55] hms_1.1.3            arrayhelpers_1.1-0   ggdist_3.3.2        \n[58] glue_1.8.0           codetools_0.2-20     ps_1.8.1            \n[61] distributional_0.5.0 stringi_1.8.4        gtable_0.3.6        \n[64] QuickJSR_1.5.1       munsell_0.5.1        pillar_1.9.0        \n[67] htmltools_0.5.8.1    Brobdingnag_1.2-9    R6_2.5.1            \n[70] evaluate_1.0.1       lattice_0.22-6       backports_1.5.0     \n[73] rstantools_2.4.0     coda_0.19-4.1        gridExtra_2.3       \n[76] nlme_3.1-167         checkmate_2.3.2      xfun_0.49           \n[79] pkgconfig_2.0.3"
  },
  {
    "objectID": "assignments/bayes_HMC_diagnostics.html",
    "href": "assignments/bayes_HMC_diagnostics.html",
    "title": "bayes_HMC_diagnostics.qmd",
    "section": "",
    "text": "This worksheet helps to give you a better idea about what to do with the trace plots."
  },
  {
    "objectID": "assignments/bayes_HMC_diagnostics.html#packages-and-data",
    "href": "assignments/bayes_HMC_diagnostics.html#packages-and-data",
    "title": "bayes_HMC_diagnostics.qmd",
    "section": "Packages and data",
    "text": "Packages and data\nLoad the primary packages.\n\n\nCode\nlibrary(tidyverse)\nlibrary(faux)\nlibrary(GGally)\nlibrary(brms)\nlibrary(ggmcmc)\nlibrary(bayesplot)\n\n\nThis time we’ll simulate data with the faux package.\n\n\nCode\n# how many cases?\nn &lt;- 100\n\n# population values\nmu    &lt;- 0\nsigma &lt;- 1\nrho   &lt;- .5\n\n# simulate and save\nset.seed(1)\n\nd &lt;- rnorm_multi(\n  n = n,\n  mu = c(mu, mu),\n  sd = c(sigma, sigma), \n  r = rho, \n  varnames = list(\"x\", \"y\")\n)\n\nglimpse(d)\n\n\nRows: 100\nColumns: 2\n$ x &lt;dbl&gt; -0.232341576, 0.137981847, -0.268214782, 1.302539315, 0.612654423, -…\n$ y &lt;dbl&gt; -0.85270825, 0.18009772, -1.17913643, 1.46056809, -0.04193022, 0.173…\n\n\nWe might look at the data with a ggpairs() plot.\n\n\nCode\nd %&gt;% \n  ggpairs(diag = list(continuous = wrap(\"barDiag\", binwidth = 0.25)),\n          upper = list(continuous = wrap(\"cor\", stars = FALSE)))\n\n\n\n\n\n\n\n\n\nCheck the sample statistics.\n\n\nCode\n# univariate\nd %&gt;% \n  pivot_longer(everything()) %&gt;% \n  group_by(name) %&gt;% \n  summarise(m = mean(value),\n            s = sd(value))\n\n\n# A tibble: 2 × 3\n  name       m     s\n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 x     0.113  0.914\n2 y     0.0754 0.913\n\n\nCode\n# bivariate\nd %&gt;% \n  summarise(r = cor(y, x))\n\n\n          r\n1 0.4502206"
  },
  {
    "objectID": "assignments/bayes_HMC_diagnostics.html#base-model",
    "href": "assignments/bayes_HMC_diagnostics.html#base-model",
    "title": "bayes_HMC_diagnostics.qmd",
    "section": "Base model",
    "text": "Base model\nLet’s fit a simple model\n\\[\n\\begin{align}\ny_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i & = \\beta_0 + \\beta_1 x_i \\\\\n\\beta_0 & \\sim \\operatorname{Normal}(0, 1) \\\\\n\\beta_1 & \\sim \\operatorname{Normal}(0, 1) \\\\\n\\sigma & \\sim \\operatorname{Exponential}(1),\n\\end{align}\n\\]\nAs we fit the model with brm(), take the opportunity to consider some of the default settings.\n\n\nCode\nfit13.b &lt;- brm(\n  data = d,\n  family = gaussian,\n  y ~ 1 + x,\n  prior = prior(normal(0, 1), class = Intercept) +\n    prior(normal(0, 1), class = b) +\n    prior(exponential(1), class = sigma),\n  seed = 13,\n  \n  # default settings we've been ignoring up to this point\n  iter = 2000, warmup = 1000, chains = 4, cores = 1\n  # if you have a good computer, maybe try setting cores = 4\n)\n\n\nIf you’d like to use multiple cores, but you’re not sure how many you have, execute parallel::detectCores().\n\nQuestion 1: How many cores do you have?\ni have 10 cores.\nCheck the model summary.\n\n\nCode\nsummary(fit13.b)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: y ~ 1 + x \n   Data: d (Number of observations: 100) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.02      0.08    -0.14     0.18 1.00     3741     3084\nx             0.45      0.09     0.27     0.62 1.00     4125     3260\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.83      0.06     0.72     0.95 1.00     3939     3163\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nLook at the parameter posteriors in a pairs() plot.\n\n\nCode\npairs(fit13.b, \n      off_diag_args = list(size = 1/3, alpha = 1/3))\n\n\n\n\n\n\n\n\n\nThe pairs() plot is a wrapper around the mcmc_pairs() function from bayesplot. By default, half of the chains are depicted in the scatter plots below the diagonal, and the other half are displayed above the diagonal. The basic idea is you want the results form different chains to mirror one another. You can control this behavior with the condition argument.\n\n\nCode\npairs(fit13.b, \n      off_diag_args = list(size = 1/3, alpha = 1/3),\n      # here we put the first chain in above the diagonal,\n      # and we put the second through fourth chains below the diagonal\n      condition = pairs_condition(chains = list(1, 2:4)))\n\n\n\n\n\n\n\n\n\nThis particular arrangement is a little silly, but it should give you a sense of how to control the output. Also, by default the histograms on the diagonal use the draws from all the chains.\nIf you wanted, you could also make a similar kind of plot with ggpairs().\n\n\nCode\nas_draws_df(fit13.b) %&gt;% \n  select(b_Intercept:sigma) %&gt;% \n  ggpairs(diag = list(continuous = wrap(\"barDiag\", bins = 25)),\n          upper = list(continuous = wrap(\"cor\", stars = FALSE)),\n          lower = list(continuous = wrap(\"points\", size = 1/4, alpha = 1/3)))\n\n\n\n\n\n\n\n\n\nNow take a look at the plot() output.\n\n\nCode\nplot(fit13.b, widths = c(1, 2))\n\n\n\n\n\n\n\n\n\nThese trace plots look like a dream. They have the appearance of fuzzy caterpillars, which is why they’re even sometimes called caterpillar plots.\nLet’s work directly with the chains via as_draws_df().\n\n\nCode\nas_draws_df(fit13.b) %&gt;% \n  # notice the 3 meta-data columns at the end\n  glimpse()\n\n\nRows: 4,000\nColumns: 9\n$ b_Intercept &lt;dbl&gt; 0.023961191, 0.008764307, -0.066725637, -0.085990390, 0.01…\n$ b_x         &lt;dbl&gt; 0.5574909, 0.5822172, 0.4748096, 0.4267265, 0.4081808, 0.4…\n$ sigma       &lt;dbl&gt; 0.8455813, 0.8642759, 0.7653377, 0.9147409, 0.8412759, 0.7…\n$ Intercept   &lt;dbl&gt; 0.08707099, 0.07467320, -0.01297564, -0.03768355, 0.063654…\n$ lprior      &lt;dbl&gt; -2.842647, -2.874429, -2.716021, -2.844376, -2.764485, -2.…\n$ lp__        &lt;dbl&gt; -124.8296, -125.3474, -125.0186, -126.0295, -124.1495, -13…\n$ .chain      &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ .iteration  &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n$ .draw       &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n\n\nWe can use those meta-data columns to make our own trace plots with ggplot functions.\n\n\nCode\nas_draws_df(fit13.b) %&gt;% \n  pivot_longer(b_Intercept:sigma) %&gt;% \n  mutate(.chain = factor(.chain),\n         # not needed, but makes for Greek formatted strip labels\n         greek = case_when(\n    name == \"b_Intercept\" ~ \"beta[0]\",\n    name == \"b_x\"         ~ \"beta[1]\",\n    name == \"sigma\"       ~ \"sigma\"\n  )) %&gt;% \n  \n  ggplot(aes(x = .iteration, y = value, color = .chain)) +\n  geom_line(linewidth = 1/3) +\n  scale_color_viridis_d(option = \"B\", end = .9) +\n  ggtitle(\"Hand-made trace plots!\") +\n  facet_wrap(~ greek, labeller = label_parsed, scales = \"free_y\")\n\n\n\n\n\n\n\n\n\nWe might restrict to the first few post-warmup iterations to help give us a better sense of what’s happening.\n\n\nCode\nas_draws_df(fit13.b) %&gt;% \n  filter(.iteration &lt; 21) %&gt;% \n  pivot_longer(b_Intercept:sigma) %&gt;% \n  mutate(.chain = factor(.chain),\n         # not needed, but makes for nice formatting\n         greek = case_when(\n    name == \"b_Intercept\" ~ \"beta[0]\",\n    name == \"b_x\"         ~ \"beta[1]\",\n    name == \"sigma\"       ~ \"sigma\"\n  )) %&gt;% \n  \n  ggplot(aes(x = .iteration, y = value, color = .chain)) +\n  geom_line(linewidth = 1) +\n  scale_color_viridis_d(option = \"B\", end = .9) +\n  ggtitle(\"Hand-made trace plots (zoomed in)\") +\n  facet_wrap(~ greek, labeller = label_parsed, scales = \"free_y\")\n\n\n\n\n\n\n\n\n\nNote that these are all post-warmup draws. The brms package doesn’t make it easy to visualize the warmup draws. But we can do so with a little help from the ggmcmc package’s ggs() function.\n\n\nCode\n# first execute without summarise()\nggs(fit13.b) %&gt;% \n  summarise(min = min(Iteration),\n            max = max(Iteration))\n\n\n# A tibble: 1 × 2\n    min   max\n  &lt;int&gt; &lt;int&gt;\n1     1  2000\n\n\nNote how how the values in the Iteration column range from 1 to 2,000. By brms default, the first 1,000 of those iterations are the warmup’s. Here is how we can use the ggs() output to make trace plots that include the warmup draws.\n\n\nCode\nggs(fit13.b) %&gt;% \n  filter(Parameter != \"lprior\") %&gt;% \n  mutate(Chain = factor(Chain),\n         greek = case_when(\n    Parameter == \"b_Intercept\" ~ \"beta[0]\",\n    Parameter == \"b_x\"         ~ \"beta[1]\",\n    Parameter == \"sigma\"       ~ \"sigma\"\n  )) %&gt;% \n  \n  ggplot(aes(x = Iteration, y = value, color = Chain)) +\n  # this marks off the warmups\n  annotate(geom = \"rect\", \n           xmin = 0, xmax = 1000, ymin = -Inf, ymax = Inf,\n           fill = \"black\", alpha = 1/6, linewidth = 0) +\n  geom_line(linewidth = 1/3) +\n  scale_color_viridis_d(option = \"B\", end = .9) +\n  labs(title = \"More hand-made trace plots\",\n       subtitle = \"warmup/post-warmup by background\") +\n  facet_wrap(~ greek, labeller = label_parsed, scales = \"free_y\")\n\n\n\n\n\n\n\n\n\nLet’s take a closer look at the first few warmup iterations.\n\n\nCode\nggs(fit13.b) %&gt;% \n  filter(Parameter != \"lprior\") %&gt;% \n  mutate(Chain = factor(Chain),\n         greek = case_when(\n    Parameter == \"b_Intercept\" ~ \"beta[0]\",\n    Parameter == \"b_x\"         ~ \"beta[1]\",\n    Parameter == \"sigma\"       ~ \"sigma\"\n  )) %&gt;% \n  \n  ggplot(aes(x = Iteration, y = value, color = Chain)) +\n  annotate(geom = \"rect\", \n           xmin = 0, xmax = 1000, ymin = -Inf, ymax = Inf,\n           fill = \"black\", alpha = 1/6, linewidth = 0) +\n  geom_line(linewidth = 2/3) +\n  scale_color_viridis_d(option = \"B\", end = .9) +\n  coord_cartesian(xlim = c(0, 50)) +\n  labs(title = \"More hand-made trace plots (zoomed in)\",\n       subtitle = \"warmup only\") +\n  facet_wrap(~ greek, labeller = label_parsed, scales = \"free_y\")\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2: Can you use the results here to describe the need for discarding warmup draws?\nYes. The warmup phase is used by the HMC algorithm to tune the sampler, such as adjusting step sizes, adapting mass matrices, and finding regions of high posterior probability. During this phase, the sampler has not yet stabilized and is still “learning” how to explore the posterior efficiently. Discarding the warmup draws ensures that the posterior samples used for inference reflect converged draws that are representative of the true posterior distribution.\nAnother issue is autocorrelation, the degree to which a given HMC draw is correlated with the previous draw(s). We can make a plot of the autocorrelations with the mcmc_acf() function from the bayesplot package.\n\n\nCode\nfit13.b %&gt;% \n  mcmc_acf(pars = vars(b_Intercept, b_x, sigma),\n           lags = 10)  # lags = 20 is the default\n\n\n\n\n\n\n\n\n\nThis is what we like to see: Nice L-shaped autocorrelation plots. Low autocorrelations like this are one of the major achievements of Stan’s implementation of HMC. It’s not uncommon for MCMC via the older Gibbs sampler method to routinely show much higher autocorrelations. You can get a sense of this by comparing the various models in Kruschke’s (2015) textbook, which often uses the Gibbs sampler, versus their brms() analogues in my (2023) ebook translation.\n\n\n\n\n\n\nNote\n\n\n\nMixing describes how efficiently MCMC chains explore the posterior distribution. Good mixing means samples move freely across the parameter space. And high autocorrelation =&gt; poor mixing.\n\n\n\n\nQuestion 3: Why are L-shaped autocorrelation plots are desirable? What would an undesirable autocorrelation plot look like?\nBecause they indicate that the chains are mixing well and that the samples are not highly correlated with each other. An undesirable autocorrelation plot would show high correlations at multiple lags, or stay flat indicating that the samples are not independent and that the chains are not mixing well.\nThose low autocorrelations also have a lot to do with our effective sample size (ESS) estimates. Take another look at the summary() output.\n\n\nCode\nsummary(fit13.b)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: y ~ 1 + x \n   Data: d (Number of observations: 100) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.02      0.08    -0.14     0.18 1.00     3741     3084\nx             0.45      0.09     0.27     0.62 1.00     4125     3260\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.83      0.06     0.72     0.95 1.00     3939     3163\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThere used to be a single ESS column. Starting with version 2.10.0, brms returns two columns: Bulk_ESS and Tail_ESS. These originate from Vehtari et al (2019). From the paper, we read:\n\nWhen reporting quantile estimates or posterior intervals, we strongly suggest assessing the convergence of the chains for these quantiles. In Section 4.3, we show that convergence of Markov chains is not uniform across the parameter space, that is, convergence might be different in the bulk of the distribution (e.g., for the mean or median) than in the tails (e.g., for extreme quantiles). We propose diagnostics and effective sample sizes specifically for extreme quantiles. This is different from the standard ESS estimate (which we refer to as bulk-ESS), which mainly assesses how well the centre of the distribution is resolved. Instead, these “tail-ESS” measures allow the user to estimate the MCSE for interval estimates. (pp. 672-673)\n\nWe generally like the values in both the Bulk_ESS and Tail_ESS columns to be as close to the total number of post-warmup draws as possible, which would be 4,000 for a default brm() model. Sometimes, as in the case of the Bulk_ESS value for our \\(\\beta_1\\) parameter, the HMC chains are so efficient that we can get larger numbers than the actual number of post-warmup draws. This is related to when we have negative autocorrelations (see above).\nHow much is enough, and how low is too low? Yeah, indeed… Higher is generally better, with diminishing returns rolling in somewhere between 1,000 and 10,000. brms will give you a warning message when the ESS estimates get below a couple hundred.\nNow look back at the Rhat column in the summary() output. This is the potential scale reduction factor \\(\\hat R\\). It has its origins in Gelman & Rubin (1992), but the current version used in brms is from Vehtari et al (2019), as cited above. In short, it is something of a ratio of the between-chain variation versus the within-chain variation. This ratio is usually a little above 1, and we want it to be as close to 1 as possible. The Stan team (e.g., https://mc-stan.org/rstan/reference/Rhat.html) recommends against values greater than 1.05. In our case, we’re good to go."
  },
  {
    "objectID": "assignments/bayes_HMC_diagnostics.html#what-bad-chains-look-like..",
    "href": "assignments/bayes_HMC_diagnostics.html#what-bad-chains-look-like..",
    "title": "bayes_HMC_diagnostics.qmd",
    "section": "What bad chains look like..",
    "text": "What bad chains look like..\nNow let’s break the model. This time, we’ll subset the d data to just the first 2 rows, we’ll make the priors very wide on the scale of the data, and we’ll dramatically reduce the warmup period.\n\n\nCode\nfit14.b &lt;- brm(\n  data = d %&gt;% slice(1:2),\n  family = gaussian,\n  y ~ 1 + x,\n  # don't use priors like this for real data analyses\n  prior = prior(normal(0, 100000), class = Intercept) +\n    prior(normal(0, 100000), class = b) +\n    prior(uniform(0, 100000), class = sigma),\n  seed = 14,\n  iter = 1100, warmup = 100, chains = 4, cores = 4\n)\n\n\nCheck the parameter summary.\n\n\nCode\nprint(fit14.b)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: y ~ 1 + x \n   Data: d %&gt;% slice(1:2) (Number of observations: 2) \n  Draws: 4 chains, each with iter = 1100; warmup = 100; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept   478.83   2036.34 -1979.74  6652.81 1.46        8       13\nx         -1228.64   3124.10 -6776.94  5668.31 1.88        6       15\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma  6231.76  11923.16   165.35 42287.98 1.13       22      336\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nNever ignore Warning messages like that.\nThose Rhat, Bulk_ESS, and Tail_ESS look really bad. Also notice how large the posterior means (Estimate) and standard deviations (Est.Error) are. Seems off, eh?\nLet’s investigate further with a pairs() plot.\n\n\nCode\nplot(fit14.b, widths = c(1, 2))\n\n\n\n\n\n\n\n\n\nThis is a full-scale disaster. DO NOT trust model results from chains that look like this.\nIn this case, just giving the model a longer warmup period helped a lot.\n\n\nCode\nfit15.b &lt;- brm(\n  data = d %&gt;% slice(1:2),\n  family = gaussian,\n  y ~ 1 + x,\n  # don't use priors like this in real life\n  prior = prior(normal(0, 100000), class = Intercept) +\n    prior(normal(0, 100000), class = b) +\n    prior(uniform(0, 100000), class = sigma),\n  seed = 14,\n  iter = 2000, warmup = 1000, chains = 4, cores = 4\n)\n\n\n\n\nCode\nplot(fit15.b, widths = c(1, 2))\n\n\n\n\n\n\n\n\n\nWe still have a lot of Warning messages, but things have improved.\nWe can do an even better with default weakly-regularizing priors.\n\n\nCode\nfit16.b &lt;- brm(\n  data = d %&gt;% slice(1:2),\n  family = gaussian,\n  y ~ 1 + x,\n  prior = prior(normal(0, 1), class = Intercept) +\n    prior(normal(0, 1), class = b) +\n    prior(exponential(1), class = sigma),\n  seed = 14,\n  iter = 2000, warmup = 1000, chains = 4, cores = 4\n)\n\n\n\n\nCode\nplot(fit16.b, widths = c(1, 2))\n\n\n\n\n\n\n\n\n\nNow look at the parameter summaries.\n\n\nCode\nprint(fit16.b)\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: y ~ 1 + x \n   Data: d %&gt;% slice(1:2) (Number of observations: 2) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.23      0.49    -1.21     0.83 1.00     1810     1643\nx             0.47      0.99    -1.46     2.38 1.00     1955     1932\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.86      0.59     0.19     2.45 1.00     1065      982\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThose Warning messages still remain, but they’re less dire than before. Also, most of the other diagnostics look better. I still wouldn’t trust this model. It is only based on 2 data points, after all. But look how far we got by paying attention to the diagnostics and picking better priors."
  },
  {
    "objectID": "assignments/bayes_HMC_diagnostics.html#references",
    "href": "assignments/bayes_HMC_diagnostics.html#references",
    "title": "bayes_HMC_diagnostics.qmd",
    "section": "References",
    "text": "References\nGelman, A. and Rubin, D. (1992). Inference from iterative simulation using multiple sequences. Statistical Science, 7(4):457–472. https://dx.doi.org/10.1214/ss/1177011136\nKruschke, J. K. (2015). Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. Academic Press. https://sites.google.com/site/doingbayesiandataanalysis/\nKurz, A. S. (2023). Doing Bayesian data analysis in brms and the tidyverse (Version 1.1.0). https://bookdown.org/content/3686/\nMcElreath, R. (2020). Statistical rethinking: A Bayesian course with examples in R and Stan (Second Edition). CRC Press. https://xcelab.net/rm/statistical-rethinking/\nVehtari, A., Gelman, A., Simpson, D., Carpenter, B., & Bürkner, P.-C. (2019). Rank-normalization, folding, and localization: An improved \\(\\widehat R\\) for assessing convergence of MCMC (with discussion). Bayesian Analysis, 16(2), 667-718. https://doi.org/10.1214/20-BA1221"
  },
  {
    "objectID": "assignments/bayes_HMC_diagnostics.html#session-information",
    "href": "assignments/bayes_HMC_diagnostics.html#session-information",
    "title": "bayes_HMC_diagnostics.qmd",
    "section": "Session information",
    "text": "Session information\n\n\nCode\nsessionInfo()\n\n\nR version 4.4.3 (2025-02-28)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sequoia 15.4\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] bayesplot_1.11.1 ggmcmc_1.5.1.1   brms_2.22.0      Rcpp_1.0.13-1   \n [5] GGally_2.2.1     faux_1.2.2       lubridate_1.9.3  forcats_1.0.0   \n [9] stringr_1.5.1    dplyr_1.1.4      purrr_1.0.2      readr_2.1.5     \n[13] tidyr_1.3.1      tibble_3.2.1     ggplot2_3.5.2    tidyverse_2.0.0 \n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.2.1     viridisLite_0.4.2    farver_2.1.2        \n [4] loo_2.8.0            fastmap_1.2.0        tensorA_0.36.2.1    \n [7] digest_0.6.37        timechange_0.3.0     estimability_1.5.1  \n[10] lifecycle_1.0.4      StanHeaders_2.32.10  processx_3.8.4      \n[13] magrittr_2.0.3       posterior_1.6.0      compiler_4.4.3      \n[16] rlang_1.1.4          tools_4.4.3          utf8_1.2.4          \n[19] yaml_2.3.10          knitr_1.49           labeling_0.4.3      \n[22] bridgesampling_1.1-2 htmlwidgets_1.6.4    pkgbuild_1.4.5      \n[25] curl_6.0.0           plyr_1.8.9           RColorBrewer_1.1-3  \n[28] abind_1.4-8          withr_3.0.2          grid_4.4.3          \n[31] stats4_4.4.3         fansi_1.0.6          xtable_1.8-4        \n[34] colorspace_2.1-1     inline_0.3.21        emmeans_1.10.5      \n[37] scales_1.3.0         cli_3.6.3            mvtnorm_1.3-2       \n[40] rmarkdown_2.29       generics_0.1.3       RcppParallel_5.1.10 \n[43] rstudioapi_0.17.1    reshape2_1.4.4       tzdb_0.4.0          \n[46] rstan_2.32.6         parallel_4.4.3       matrixStats_1.5.0   \n[49] vctrs_0.6.5          V8_6.0.1             Matrix_1.7-2        \n[52] jsonlite_1.8.9       callr_3.7.6          hms_1.1.3           \n[55] glue_1.8.0           ps_1.8.1             ggstats_0.9.0       \n[58] codetools_0.2-20     distributional_0.5.0 stringi_1.8.4       \n[61] gtable_0.3.6         QuickJSR_1.5.1       munsell_0.5.1       \n[64] pillar_1.9.0         htmltools_0.5.8.1    Brobdingnag_1.2-9   \n[67] R6_2.5.1             evaluate_1.0.1       lattice_0.22-6      \n[70] backports_1.5.0      rstantools_2.4.0     coda_0.19-4.1       \n[73] gridExtra_2.3        nlme_3.1-167         checkmate_2.3.2     \n[76] xfun_0.49            pkgconfig_2.0.3"
  },
  {
    "objectID": "assignments/missing_data.html",
    "href": "assignments/missing_data.html",
    "title": "Missing Data",
    "section": "",
    "text": "Code\n#load packages\nlibrary(skimr)\nlibrary(naniar)\nlibrary(tidyverse)\nlibrary(mice)\n\n\nMissing data is a common problem and dealing with it appropriately is extremely important. Ignoring the missing data points or filling them incorrectly may cause the models to work in unexpected ways and cause the predictions and inferences to be biased.\nLe’ts consider built-in dataset ‘airquality’ in R as a sample dataset.\n\n\nCode\n# Load the airquality dataset\ndata(\"airquality\")\n\n\n\nQuestion 1:\n\nExamine this dataset for missing values. While there are many ways to do this, the skim function from the library ‘skimr’ is elegant;\n\n\n\nCode\nskim(airquality)\n\n\n\nData summary\n\n\nName\nairquality\n\n\nNumber of rows\n153\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nOzone\n37\n0.76\n42.13\n32.99\n1.0\n18.00\n31.5\n63.25\n168.0\n▇▃▂▁▁\n\n\nSolar.R\n7\n0.95\n185.93\n90.06\n7.0\n115.75\n205.0\n258.75\n334.0\n▅▃▅▇▅\n\n\nWind\n0\n1.00\n9.96\n3.52\n1.7\n7.40\n9.7\n11.50\n20.7\n▂▇▇▃▁\n\n\nTemp\n0\n1.00\n77.88\n9.47\n56.0\n72.00\n79.0\n85.00\n97.0\n▂▃▇▇▃\n\n\nMonth\n0\n1.00\n6.99\n1.42\n5.0\n6.00\n7.0\n8.00\n9.0\n▇▇▇▇▇\n\n\nDay\n0\n1.00\n15.80\n8.86\n1.0\n8.00\n16.0\n23.00\n31.0\n▇▇▇▇▆\n\n\n\n\n\n\nuse the nanair package to visualize missing values\n\n\n\nCode\nvis_miss(airquality)\n\n\n\n\n\n\n\n\n\n\neven though it’s hard to confirm based on visualizations alone, what do your visualizations lead you to believe about the missing data being MCAR, MAR, or MNAR?\n\nFrom the visualization, it appears that missing data is not completely random (MCAR) as some variables (like Ozone and Solar.R) have more missing values than others. It might be MAR (Missing at Random) if missingness in Ozone is related to other variables like Temp or Wind.\n\nCarry out Little’s statistical test to evaluate MCAR and report results.\n\n\n\nCode\nmcar_test &lt;- mcar_test(airquality)\nprint(mcar_test)\n\n\n# A tibble: 1 × 4\n  statistic    df p.value missing.patterns\n      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;            &lt;int&gt;\n1      35.1    14 0.00142                4\n\n\nLittle’s MCAR test on the airquality dataset yielded a chi-square of 35.11 (df = 14) with a p-value of 0.001418, which is statistically significant. This result means we can reject the null hypothesis that the missing data is Missing Completely At Random (MCAR), suggesting that missingness is likely related to either observed variables (MAR) or the unobserved values themselves (MNAR).\n\nCreating a binary indicator for missingness allows you to test whether the presence of missing data is related to observed data.\n\nFor instance, you can create a dummy variable: 1 = Missing; 0 = Observed.\nNext you can conduct a chi-square test or t-test:\n\nChi-square: Compare proportions of missingness across groups.\nT-test: Compare means of (other) observed variables with missingness indicators.\n\n\n\n\n\nCode\n# Create binary indicator for Ozone missingness\nairquality$miss_ozone &lt;- as.factor(ifelse(is.na(airquality$Ozone), 1, 0))\n\n# Create binary indicator for Solar.R missingness\nairquality$miss_solar &lt;- as.factor(ifelse(is.na(airquality$Solar.R), 1, 0))\n\n# T-test to see if Wind values differ between records with missing vs. non-missing Ozone\nt.test(Wind ~ miss_ozone, data = airquality)\n\n\n\n    Welch Two Sample t-test\n\ndata:  Wind by miss_ozone\nt = -0.60911, df = 63.646, p-value = 0.5446\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -1.6893132  0.8999377\nsample estimates:\nmean in group 0 mean in group 1 \n       9.862069       10.256757 \n\n\nCode\n# T-test to see if Temperature values differ between records with missing vs. non-missing Ozone\nt.test(Temp ~ miss_ozone, data = airquality)\n\n\n\n    Welch Two Sample t-test\n\ndata:  Temp by miss_ozone\nt = -0.026831, df = 60.447, p-value = 0.9787\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -3.643306  3.546847\nsample estimates:\nmean in group 0 mean in group 1 \n       77.87069        77.91892 \n\n\nCode\n# T-test to see if Wind values differ between records with missing vs. non-missing Solar.R\nt.test(Wind ~ miss_solar, data = airquality)\n\n\n\n    Welch Two Sample t-test\n\ndata:  Wind by miss_solar\nt = 0.65629, df = 6.4571, p-value = 0.5343\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -2.674488  4.681338\nsample estimates:\nmean in group 0 mean in group 1 \n       10.00342         9.00000 \n\n\nCode\n# T-test to see if Temperature values differ between records with missing vs. non-missing Solar.R  \nt.test(Temp ~ miss_solar, data = airquality)\n\n\n\n    Welch Two Sample t-test\n\ndata:  Temp by miss_solar\nt = 0.98706, df = 6.2689, p-value = 0.3602\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -7.436381 17.669258\nsample estimates:\nmean in group 0 mean in group 1 \n       78.11644        73.00000 \n\n\nCode\nggplot(airquality, aes(x = miss_ozone, y = Temp)) +\n  geom_boxplot() +\n  labs(title = \"Temperature by Ozone Missingness\",\n       x = \"Ozone Missing (1 = Yes, 0 = No)\",\n       y = \"Temperature\")\n\n\n\n\n\n\n\n\n\nCode\nggplot(airquality, aes(x = miss_ozone, y = Wind)) +\n  geom_boxplot() +\n  labs(title = \"Wind by Ozone Missingness\",\n       x = \"Ozone Missing (1 = Yes, 0 = No)\",\n       y = \"Wind\")\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2:\nCreate new and appropriately named datasets that are based on airquality for each of the following ways of fixing the dataset:\n  - (a) \"listwise deletion\" or \"complete case analysis\" --- where entire records from the analysis are removed if they are missing any data point in one or more variables \n  \n  - (b) Imputation with mean --- involves filling in the missing values with the mean of the available values in the same variable.\n  \n  - (c) Imputation with regression (use mice package)\n  \n  - (d) Imputation with stochastic regression (use mice package)\n\n  - (e) Imputation with multiple induction (use mice package, 5 imputations, and Predictive mean matching method)\n\n\nCode\n# (a) Listwise deletion \nairquality_listwise &lt;- na.omit(airquality)\nprint(paste0(nrow(airquality), \" → \", nrow(airquality_listwise)))\n\n\n[1] \"153 → 111\"\n\n\nCode\n# (b) Imputation with mean\nairquality_mean &lt;- airquality\nairquality_mean$Ozone[is.na(airquality_mean$Ozone)] &lt;- mean(airquality_mean$Ozone, na.rm = TRUE)\nairquality_mean$Solar.R[is.na(airquality_mean$Solar.R)] &lt;- mean(airquality_mean$Solar.R, na.rm = TRUE)\n\n# (c) Imputation with regression using mice\nset.seed(123) \nimp_reg &lt;- mice(airquality, method = \"norm.predict\", m = 1)\n\n\n\n iter imp variable\n  1   1  Ozone  Solar.R\n  2   1  Ozone  Solar.R\n  3   1  Ozone  Solar.R\n  4   1  Ozone  Solar.R\n  5   1  Ozone  Solar.R\n\n\nCode\nairquality_reg &lt;- complete(imp_reg)\n\n# (d) Imputation with stochastic regression using mice\nset.seed(123) \nimp_stoch_reg &lt;- mice(airquality, method = \"norm.nob\", m = 1)\n\n\n\n iter imp variable\n  1   1  Ozone  Solar.R\n  2   1  Ozone  Solar.R\n  3   1  Ozone  Solar.R\n  4   1  Ozone  Solar.R\n  5   1  Ozone  Solar.R\n\n\nCode\nairquality_stoch_reg &lt;- complete(imp_stoch_reg)\n\n# (e) Multiple imputation using Predictive Mean Matching (PMM)\nset.seed(123) \nimp_mi &lt;- mice(airquality, method = \"pmm\", m = 5)\n\n\n\n iter imp variable\n  1   1  Ozone  Solar.R\n  1   2  Ozone  Solar.R\n  1   3  Ozone  Solar.R\n  1   4  Ozone  Solar.R\n  1   5  Ozone  Solar.R\n  2   1  Ozone  Solar.R\n  2   2  Ozone  Solar.R\n  2   3  Ozone  Solar.R\n  2   4  Ozone  Solar.R\n  2   5  Ozone  Solar.R\n  3   1  Ozone  Solar.R\n  3   2  Ozone  Solar.R\n  3   3  Ozone  Solar.R\n  3   4  Ozone  Solar.R\n  3   5  Ozone  Solar.R\n  4   1  Ozone  Solar.R\n  4   2  Ozone  Solar.R\n  4   3  Ozone  Solar.R\n  4   4  Ozone  Solar.R\n  4   5  Ozone  Solar.R\n  5   1  Ozone  Solar.R\n  5   2  Ozone  Solar.R\n  5   3  Ozone  Solar.R\n  5   4  Ozone  Solar.R\n  5   5  Ozone  Solar.R\n\n\nCode\nairquality_mi &lt;- complete(imp_mi, 1)\n\nsummary(imp_mi)\n\n\nClass: mids\nNumber of multiple imputations:  5 \nImputation methods:\n     Ozone    Solar.R       Wind       Temp      Month        Day miss_ozone \n     \"pmm\"      \"pmm\"         \"\"         \"\"         \"\"         \"\"         \"\" \nmiss_solar \n        \"\" \nPredictorMatrix:\n        Ozone Solar.R Wind Temp Month Day miss_ozone miss_solar\nOzone       0       1    1    1     1   1          1          1\nSolar.R     1       0    1    1     1   1          1          1\nWind        1       1    0    1     1   1          1          1\nTemp        1       1    1    0     1   1          1          1\nMonth       1       1    1    1     0   1          1          1\nDay         1       1    1    1     1   0          1          1\nNumber of logged events:  50 \n  it im     dep meth         out\n1  1  1   Ozone  pmm miss_ozone1\n2  1  1 Solar.R  pmm miss_solar1\n3  1  2   Ozone  pmm miss_ozone1\n4  1  2 Solar.R  pmm miss_solar1\n5  1  3   Ozone  pmm miss_ozone1\n6  1  3 Solar.R  pmm miss_solar1\n\n\n\n\nQuestion 3:\nCompare the eventual distribution from these datasets on the variable ’Ozone’against the orgiinal. Below is a template that considers only 2 datasets but please consider all the datasets you generated within a single plot\n\n\nCode\n# ggplot(airquality, aes(x=Ozone, fill=\"Original\")) +\n#   geom_density(alpha=0.5) +\n#   geom_density(data=dataset_listwise_deletion, aes(x=Ozone, fill=\"Listwise Deletion\"), alpha=0.5) +\n#   labs(title=\"Density Plot of Ozone: Original vs. Imputed\")\n\n# Create a combined dataframe for plotting\nozone_original &lt;- airquality %&gt;% \n  select(Ozone) %&gt;% \n  mutate(Method = \"Original\") %&gt;%\n  filter(!is.na(Ozone))\n\nozone_listwise &lt;- airquality_listwise %&gt;% \n  select(Ozone) %&gt;% \n  mutate(Method = \"Listwise Deletion\")\n\nozone_mean &lt;- airquality_mean %&gt;% \n  select(Ozone) %&gt;% \n  mutate(Method = \"Mean Imputation\")\n\nozone_reg &lt;- airquality_reg %&gt;% \n  select(Ozone) %&gt;% \n  mutate(Method = \"Regression Imputation\")\n\nozone_stoch_reg &lt;- airquality_stoch_reg %&gt;% \n  select(Ozone) %&gt;% \n  mutate(Method = \"Stochastic Regression\")\n\nozone_mi &lt;- airquality_mi %&gt;% \n  select(Ozone) %&gt;% \n  mutate(Method = \"Multiple Imputation (PMM)\")\n\n# Combine all datasets\nall_ozone &lt;- bind_rows(ozone_original, ozone_listwise, ozone_mean, ozone_reg, ozone_stoch_reg, ozone_mi)\n\nggplot(all_ozone, aes(x = Ozone, fill = Method)) +\n  geom_density(alpha = 0.5) +\n  labs(title = \"Density Plot of Ozone: Original vs. Imputed Methods\",\n       x = \"Ozone\",\n       y = \"Density\") +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\n\n\nCode\nggplot(all_ozone, aes(x = Method, y = Ozone, fill = Method)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot of Ozone: Original vs. Imputed Methods\",\n       x = \"Method\",\n       y = \"Ozone\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  scale_fill_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\n\n\nWhat do you observe?\n\n\nAnswer:\nThe density plot and boxplot comparisons of the ‘Ozone’ variable across different imputation methods reveal significant differences in how each method handles missing data: - The original data (excluding missing values) shows a natural distribution of ‘Ozone’ values. - Listwise deletion retains the original distribution but reduces the sample size. - Mean imputation creates an artificial peak at the mean value, which distorts the natural distribution and underestimating variance. - Regression imputation preserves the general shape of the distribution but doesn’t capture the full variability of the data. - Stochastic regression imputation adds random noise to the regression predictions, which helps to better represent the variability in the data compared to regular regression imputation. - Multiple imputation with PMM appears to best preserve the original distribution’s shape while filling in missing values, as it samples from observed values that are similar to the predicted values for missing data points.\nThe key takeaway is that different imputation methods can significantly impact the distribution of the data. Methods like mean imputation can distort the distribution, while more sophisticated methods like PMM better preserve the original distribution’s properties. The choice of imputation method should consider how well it preserves the distribution and relationships between variables in the dataset.\n\n\nOf course, each dataset you produced will lead to different modeling results, but we won’t go into that in today’s lab."
  }
]